{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Aprenda sobre a BD A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados de qualidade no Brasil. Para isso, criamos uma ferramenta que te permite acessar recursos importantes de diversos conjuntos de dados p\u00fablicos , como: Tabelas tratadas BD+ : Tabelas completas, j\u00e1 tratadas e prontas para an\u00e1lise, dispon\u00edveis no nosso datalake p\u00fablico. Dados originais : Links com informa\u00e7\u00f5es \u00fateis para explorar mais sobre o conjunto de dados, como a fonte original e outros. Temos um time de Dados e volunt\u00e1rios(as) de todo o Brasil que ajudam a limpar e manter as tabelas tratadas BD+. Saiba como fazer parte Acessando tabelas tratadas BD+ No nosso site voc\u00ea encontra a lista de todas as tabelas tratadas de cada conjunto de dados. Apresentamos tamb\u00e9m informa\u00e7\u00f5es importantes de todas as tabelas, como a lista de colunas, cobertura temporal, periodicidade, entre outras informa\u00e7\u00f5es. Voc\u00ea pode consultar os dados das tabelas via: Download Voc\u00ea pode baixar o arquivo CSV completo da tabela direto no site. Este tipo de Consulta n\u00e3o est\u00e1 dispon\u00edvel para arquivos grandes. BigQuery (SQL) O BigQuery \u00e9 o um servi\u00e7o de banco de dados em nuvem da Google. Direto do navegador, voc\u00ea pode fazer consultas \u00e0s tabelas tratadas com: Rapidez: Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala: O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Economia: Todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consulta aos dados . Aprenda Pacotes Os pacotes da Base dos Dados permitem o acesso ao datalake p\u00fablico direto do seu computador ou ambiente de desenvolvimento. Outra forma de acessar os recursos da BD \u00e9 diretamente pelos endpoints, conforme documentado em BD Open API . Os pacotes atualmente dispon\u00edveis s\u00e3o: Python R Stata CLI (Terminal) Aprenda Dicas para melhor uso dos dados Nosso time de dados trabalha constantemente em desenvolver melhores padr\u00f5es e metodologias para facilitar o processo de an\u00e1lise de dados. Separamos alguns materiais \u00fateis para voc\u00ea entender melhor o que fazemos e como tirar o melhor proveito dos dados: Cruzar dados de diferentes organiza\u00e7\u00f5es de forma r\u00e1pida Entender padr\u00f5es de tabelas, conjuntos e vari\u00e1veis","title":"Aprenda sobre a BD"},{"location":"#aprenda-sobre-a-bd","text":"A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados de qualidade no Brasil. Para isso, criamos uma ferramenta que te permite acessar recursos importantes de diversos conjuntos de dados p\u00fablicos , como: Tabelas tratadas BD+ : Tabelas completas, j\u00e1 tratadas e prontas para an\u00e1lise, dispon\u00edveis no nosso datalake p\u00fablico. Dados originais : Links com informa\u00e7\u00f5es \u00fateis para explorar mais sobre o conjunto de dados, como a fonte original e outros. Temos um time de Dados e volunt\u00e1rios(as) de todo o Brasil que ajudam a limpar e manter as tabelas tratadas BD+. Saiba como fazer parte","title":"Aprenda sobre a BD"},{"location":"#acessando-tabelas-tratadas-bd","text":"No nosso site voc\u00ea encontra a lista de todas as tabelas tratadas de cada conjunto de dados. Apresentamos tamb\u00e9m informa\u00e7\u00f5es importantes de todas as tabelas, como a lista de colunas, cobertura temporal, periodicidade, entre outras informa\u00e7\u00f5es. Voc\u00ea pode consultar os dados das tabelas via:","title":"Acessando tabelas tratadas BD+"},{"location":"#download","text":"Voc\u00ea pode baixar o arquivo CSV completo da tabela direto no site. Este tipo de Consulta n\u00e3o est\u00e1 dispon\u00edvel para arquivos grandes.","title":"Download"},{"location":"#bigquery-sql","text":"O BigQuery \u00e9 o um servi\u00e7o de banco de dados em nuvem da Google. Direto do navegador, voc\u00ea pode fazer consultas \u00e0s tabelas tratadas com: Rapidez: Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala: O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Economia: Todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consulta aos dados . Aprenda","title":"BigQuery (SQL)"},{"location":"#pacotes","text":"Os pacotes da Base dos Dados permitem o acesso ao datalake p\u00fablico direto do seu computador ou ambiente de desenvolvimento. Outra forma de acessar os recursos da BD \u00e9 diretamente pelos endpoints, conforme documentado em BD Open API . Os pacotes atualmente dispon\u00edveis s\u00e3o: Python R Stata CLI (Terminal) Aprenda","title":"Pacotes"},{"location":"#dicas-para-melhor-uso-dos-dados","text":"Nosso time de dados trabalha constantemente em desenvolver melhores padr\u00f5es e metodologias para facilitar o processo de an\u00e1lise de dados. Separamos alguns materiais \u00fateis para voc\u00ea entender melhor o que fazemos e como tirar o melhor proveito dos dados: Cruzar dados de diferentes organiza\u00e7\u00f5es de forma r\u00e1pida Entender padr\u00f5es de tabelas, conjuntos e vari\u00e1veis","title":"Dicas para melhor uso dos dados"},{"location":"access_data_bq/","text":"BigQuery O BigQuery \u00e9 o um servi\u00e7o de banco de dados em nuvem da Google. Voc\u00ea faz consultas ao banco em SQL direto do navegador com: Rapidez : Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala : O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Economia : Todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consulta aos dados . Pronto(a) para come\u00e7ar? Nesta p\u00e1gina voc\u00ea encontra: Primeiros passos Tutoriais Manuais e Cursos de SQL Primeiros passos Antes de come\u00e7ar: Crie o seu projeto no Google Cloud Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso datalake p\u00fablico. Acesse o Google Cloud . Caso for a sua primeira vez, aceite o Termo de Servi\u00e7os. Clique em Create Project/Criar Projeto . Escolha um nome bacana para o projeto. Clique em Create/Criar Por que eu preciso criar um projeto no Google Cloud? A Google fornece 1 TB gratuito por m\u00eas de uso do BigQuery para cada projeto que voc\u00ea possui. Um projeto \u00e9 necess\u00e1rio para ativar os servi\u00e7os do Google Cloud, incluindo a permiss\u00e3o de uso do BigQuery. Pense no projeto como a \"conta\" na qual a Google vai contabilizar o quanto de processamento voc\u00ea j\u00e1 utilizou. N\u00e3o \u00e9 necess\u00e1rio adicionar nenhum cart\u00e3o ou forma de pagamento - O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais aqui . Acessando o datalake da basedosdados O bot\u00e3o abaixo via te direcionar ao nosso projeto no Google BigQuery: Ir para BigQuery Agora voc\u00ea precisa fixar o projeto da BD no seu BigQuery, \u00e9 bem simples, veja: Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o dos dados, datasets (conjuntos de dados) e tables (tabelas), nos quais: Todas as tabelas est\u00e3o organizadas dentro de cojuntos de dados , que representaam sua organiza\u00e7\u00e3o/tema (ex: o conjunto br_ibge_populacao cont\u00e9m uma tabela municipio com a s\u00e9rie hist\u00f3ricac de popula\u00e7\u00e3o a n\u00edvel municipal) Cada tabela pertence a um \u00fanico conjunto de dados (ex: a tabela municipio em br_ibge_populacao \u00e9 diferente de municipio em br_bd_diretorios ) Veja aqui o guia do Google de como funciona a interface do BigQuery . Caso n\u00e3o apare\u00e7am as tabelas na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina. Fa\u00e7a sua primeira consulta! Que tal fazer uma consulta simples? Vamos usar o Editor de Consultas do BigQuery para ver as informa\u00e7\u00f5es sobre munic\u00edpios direto na nossa base de diret\u00f3rios brasileiros. Para isso, copiar e colar o c\u00f3digo abaixo: SELECT * FROM ` basedosdados . br_bd_diretorios_brasil . municipio ` S\u00f3 clicar em Executar e pronto! Dica Clicando no bot\u00e3o \ud83d\udd0d Consultar tabela/Query View , o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em Query Editor/Editor de consultas - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios. Tutoriais Como navegar pelo BigQuery Para entender mais sobre a interface do BigQuery e como explorar os dados, preparamos um texto completo no blog com um exemplo de busca dos dados da RAIS - Minist\u00e9rio da Economia . Cansado(a) da leitura? Temos tamb\u00e9m um v\u00eddeo completo no nosso Youtube . Entenda os dados O BigQuery possui um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais . Conectando com o PowerBI O Power BI \u00e9 uma das tecnologias mais populares para o desenvolvimento de dashboards com dados relacionais. Por isso, preparamos um tutorial para voc\u00ea descobrir como usar os dados do datalake no desenvolvimento dos seus dashboards . Manuais e Cursos de SQL Est\u00e1 come\u00e7ando a aprender sobre SQL para fazer suas consultas? Abaixo colocamos algumas recomenda\u00e7\u00f5es usadas pela nossa equipe tanto no aprendizado quanto no dia-a-dia: Lista de fun\u00e7\u00f5es em SQL da W3 Curso SQL na Codeacademy","title":"BigQuery"},{"location":"access_data_bq/#bigquery","text":"O BigQuery \u00e9 o um servi\u00e7o de banco de dados em nuvem da Google. Voc\u00ea faz consultas ao banco em SQL direto do navegador com: Rapidez : Mesmo queries muito longas demoram apenas minutos para serem processadas. Escala : O BigQuery escala magicamente para hexabytes se necess\u00e1rio. Economia : Todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consulta aos dados . Pronto(a) para come\u00e7ar? Nesta p\u00e1gina voc\u00ea encontra: Primeiros passos Tutoriais Manuais e Cursos de SQL","title":"BigQuery"},{"location":"access_data_bq/#primeiros-passos","text":"","title":"Primeiros passos"},{"location":"access_data_bq/#antes-de-comecar-crie-o-seu-projeto-no-google-cloud","text":"Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso datalake p\u00fablico. Acesse o Google Cloud . Caso for a sua primeira vez, aceite o Termo de Servi\u00e7os. Clique em Create Project/Criar Projeto . Escolha um nome bacana para o projeto. Clique em Create/Criar Por que eu preciso criar um projeto no Google Cloud? A Google fornece 1 TB gratuito por m\u00eas de uso do BigQuery para cada projeto que voc\u00ea possui. Um projeto \u00e9 necess\u00e1rio para ativar os servi\u00e7os do Google Cloud, incluindo a permiss\u00e3o de uso do BigQuery. Pense no projeto como a \"conta\" na qual a Google vai contabilizar o quanto de processamento voc\u00ea j\u00e1 utilizou. N\u00e3o \u00e9 necess\u00e1rio adicionar nenhum cart\u00e3o ou forma de pagamento - O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais aqui .","title":"Antes de come\u00e7ar: Crie o seu projeto no Google Cloud"},{"location":"access_data_bq/#acessando-o-datalake-da-basedosdados","text":"O bot\u00e3o abaixo via te direcionar ao nosso projeto no Google BigQuery: Ir para BigQuery Agora voc\u00ea precisa fixar o projeto da BD no seu BigQuery, \u00e9 bem simples, veja: Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o dos dados, datasets (conjuntos de dados) e tables (tabelas), nos quais: Todas as tabelas est\u00e3o organizadas dentro de cojuntos de dados , que representaam sua organiza\u00e7\u00e3o/tema (ex: o conjunto br_ibge_populacao cont\u00e9m uma tabela municipio com a s\u00e9rie hist\u00f3ricac de popula\u00e7\u00e3o a n\u00edvel municipal) Cada tabela pertence a um \u00fanico conjunto de dados (ex: a tabela municipio em br_ibge_populacao \u00e9 diferente de municipio em br_bd_diretorios ) Veja aqui o guia do Google de como funciona a interface do BigQuery . Caso n\u00e3o apare\u00e7am as tabelas na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina.","title":"Acessando o datalake da basedosdados"},{"location":"access_data_bq/#faca-sua-primeira-consulta","text":"Que tal fazer uma consulta simples? Vamos usar o Editor de Consultas do BigQuery para ver as informa\u00e7\u00f5es sobre munic\u00edpios direto na nossa base de diret\u00f3rios brasileiros. Para isso, copiar e colar o c\u00f3digo abaixo: SELECT * FROM ` basedosdados . br_bd_diretorios_brasil . municipio ` S\u00f3 clicar em Executar e pronto! Dica Clicando no bot\u00e3o \ud83d\udd0d Consultar tabela/Query View , o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em Query Editor/Editor de consultas - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios.","title":"Fa\u00e7a sua primeira consulta!"},{"location":"access_data_bq/#tutoriais","text":"","title":"Tutoriais"},{"location":"access_data_bq/#como-navegar-pelo-bigquery","text":"Para entender mais sobre a interface do BigQuery e como explorar os dados, preparamos um texto completo no blog com um exemplo de busca dos dados da RAIS - Minist\u00e9rio da Economia . Cansado(a) da leitura? Temos tamb\u00e9m um v\u00eddeo completo no nosso Youtube .","title":"Como navegar pelo BigQuery"},{"location":"access_data_bq/#entenda-os-dados","text":"O BigQuery possui um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais .","title":"Entenda os dados"},{"location":"access_data_bq/#conectando-com-o-powerbi","text":"O Power BI \u00e9 uma das tecnologias mais populares para o desenvolvimento de dashboards com dados relacionais. Por isso, preparamos um tutorial para voc\u00ea descobrir como usar os dados do datalake no desenvolvimento dos seus dashboards .","title":"Conectando com o PowerBI"},{"location":"access_data_bq/#manuais-e-cursos-de-sql","text":"Est\u00e1 come\u00e7ando a aprender sobre SQL para fazer suas consultas? Abaixo colocamos algumas recomenda\u00e7\u00f5es usadas pela nossa equipe tanto no aprendizado quanto no dia-a-dia: Lista de fun\u00e7\u00f5es em SQL da W3 Curso SQL na Codeacademy","title":"Manuais e Cursos de SQL"},{"location":"access_data_packages/","text":"Pacotes Os pacotes da Base dos Dados permitem o acesso ao datalake p\u00fablico direto do seu computador ou ambiente de desenvolvimento. Atualmente dispon\u00edveis em: Python R Stata CLI (terminal) Pronto(a) para come\u00e7ar? Nesta p\u00e1gina voc\u00ea encontra: Primeiros passos Tutoriais Manuais de refer\u00eancia Primeiros passos Antes de come\u00e7ar: Crie o seu projeto no Google Cloud Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso datalake p\u00fablico. Acesse o Google Cloud . Caso for a sua primeira vez, aceite o Termo de Servi\u00e7os. Clique em Create Project/Criar Projeto . Escolha um nome bacana para o projeto. Clique em Create/Criar Por que eu preciso criar um projeto no Google Cloud? A Google fornece 1 TB gratuito por m\u00eas de uso do BigQuery para cada projeto que voc\u00ea possui. Um projeto \u00e9 necess\u00e1rio para ativar os servi\u00e7os do Google Cloud, incluindo a permiss\u00e3o de uso do BigQuery. Pense no projeto como a \"conta\" na qual a Google vai contabilizar o quanto de processamento voc\u00ea j\u00e1 utilizou. N\u00e3o \u00e9 necess\u00e1rio adicionar nenhum cart\u00e3o ou forma de pagamento - O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais aqui . Instalando o pacote Para instala\u00e7\u00e3o do pacote em Python e linha de comando, voc\u00ea pode usar o pip direto do seu terminal. Em R, basta instalar diretamente no RStudio ou editor de sua prefer\u00eancia. Python/CLI pip install basedosdados R install.packages ( \"basedosdados\" ) Stata Requerimentos: Garantir que seu Stata seja a vers\u00e3o 16+ Garantir que o Python esteja instalado no seu computador. Com os requerimentos satisfeitos, rodar os comandos abaixo: net install basedosdados, from( \"https://raw.githubusercontent.com/basedosdados/mais/master/stata-package\" ) Configurando o pacote Uma vez com seu projeto, voc\u00ea precisa configurar o pacote para usar o ID desse projeto nas consultas ao datalake . Para isso, voc\u00ea deve usar o project_id que a Google fornece para voc\u00ea assim que o projeto \u00e9 criado. Python/CLI N\u00e3o \u00e9 necess\u00e1rio configurar o projeto de antem\u00e3o. Assim que voc\u00ea roda a 1\u00aa consulta, o pacote ir\u00e1 indicar os passos para configura\u00e7\u00e3o. R Uma vez com o project_id , voc\u00ea deve passar essa informa\u00e7\u00e3o para o pacote usando a fun\u00e7\u00e3o set_billing_id . set_billing_id ( \"<YOUR_PROJECT_ID>\" ) Stata \u00c9 necess\u00e1rio especificar o project_id a cada vez que usar o pacote. Fa\u00e7a sua primeira consulta Um exemplo simples para come\u00e7ar a explorar o datalake \u00e9 puxar informa\u00e7\u00f5es cadastrais de munic\u00edpios direto na nossa base de Diret\u00f3rios Brasileiros (tabela municipio ) . Para isso, vamos usar a fun\u00e7\u00e3o download , baixando os dados direto para nossa m\u00e1quina. Python import basedosdados as bd bd . download ( savepath = \"<PATH>\" , dataset_id = \"br-bd-diretorios-brasil\" , table_id = \"municipio\" ) Para entender mais sobre a fun\u00e7\u00e3o download , leia o manual de refer\u00eancia . R library ( \"basedosdados\" ) query <- \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\" dir <- tempdir () data <- download ( query , \"<PATH>\" ) Para entender mais sobre a fun\u00e7\u00e3o download , leia o manual de refer\u00eancia . Stata bd_read_sql, /// path( \"<PATH>\" ) /// query ( \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\") /// billing_project_id(\"<PROJECT_ID>\") CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query 'SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`' Para entender mais sobre a fun\u00e7\u00e3o download , leia o manual de refer\u00eancia . Tutoriais Como usar os pacotes Preparamos tutoriais apresentando as principais fun\u00e7\u00f5es de cada pacote para voc\u00ea come\u00e7ar a us\u00e1-los. Python Blog: Introdu\u00e7\u00e3o ao pacote Python Introdu\u00e7\u00e3o ao pacote Python (cont.) V\u00eddeos: Workshop: Aplica\u00e7\u00f5es em Python R Blog: Introdu\u00e7\u00e3o ao pacote R Explorando o Censo Escolar An\u00e1lise: O Brasil nas Olimp\u00edadas V\u00eddeos: Workshop: Aprenda a acessar dados p\u00fablicos em R Stata Documenta\u00e7\u00e3o: GitHub Manuais de refer\u00eancia (API) Python R Stata CLI","title":"Pacotes"},{"location":"access_data_packages/#pacotes","text":"Os pacotes da Base dos Dados permitem o acesso ao datalake p\u00fablico direto do seu computador ou ambiente de desenvolvimento. Atualmente dispon\u00edveis em: Python R Stata CLI (terminal) Pronto(a) para come\u00e7ar? Nesta p\u00e1gina voc\u00ea encontra: Primeiros passos Tutoriais Manuais de refer\u00eancia","title":"Pacotes"},{"location":"access_data_packages/#primeiros-passos","text":"","title":"Primeiros passos"},{"location":"access_data_packages/#antes-de-comecar-crie-o-seu-projeto-no-google-cloud","text":"Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso datalake p\u00fablico. Acesse o Google Cloud . Caso for a sua primeira vez, aceite o Termo de Servi\u00e7os. Clique em Create Project/Criar Projeto . Escolha um nome bacana para o projeto. Clique em Create/Criar Por que eu preciso criar um projeto no Google Cloud? A Google fornece 1 TB gratuito por m\u00eas de uso do BigQuery para cada projeto que voc\u00ea possui. Um projeto \u00e9 necess\u00e1rio para ativar os servi\u00e7os do Google Cloud, incluindo a permiss\u00e3o de uso do BigQuery. Pense no projeto como a \"conta\" na qual a Google vai contabilizar o quanto de processamento voc\u00ea j\u00e1 utilizou. N\u00e3o \u00e9 necess\u00e1rio adicionar nenhum cart\u00e3o ou forma de pagamento - O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais aqui .","title":"Antes de come\u00e7ar: Crie o seu projeto no Google Cloud"},{"location":"access_data_packages/#instalando-o-pacote","text":"Para instala\u00e7\u00e3o do pacote em Python e linha de comando, voc\u00ea pode usar o pip direto do seu terminal. Em R, basta instalar diretamente no RStudio ou editor de sua prefer\u00eancia. Python/CLI pip install basedosdados R install.packages ( \"basedosdados\" ) Stata Requerimentos: Garantir que seu Stata seja a vers\u00e3o 16+ Garantir que o Python esteja instalado no seu computador. Com os requerimentos satisfeitos, rodar os comandos abaixo: net install basedosdados, from( \"https://raw.githubusercontent.com/basedosdados/mais/master/stata-package\" )","title":"Instalando o pacote"},{"location":"access_data_packages/#configurando-o-pacote","text":"Uma vez com seu projeto, voc\u00ea precisa configurar o pacote para usar o ID desse projeto nas consultas ao datalake . Para isso, voc\u00ea deve usar o project_id que a Google fornece para voc\u00ea assim que o projeto \u00e9 criado. Python/CLI N\u00e3o \u00e9 necess\u00e1rio configurar o projeto de antem\u00e3o. Assim que voc\u00ea roda a 1\u00aa consulta, o pacote ir\u00e1 indicar os passos para configura\u00e7\u00e3o. R Uma vez com o project_id , voc\u00ea deve passar essa informa\u00e7\u00e3o para o pacote usando a fun\u00e7\u00e3o set_billing_id . set_billing_id ( \"<YOUR_PROJECT_ID>\" ) Stata \u00c9 necess\u00e1rio especificar o project_id a cada vez que usar o pacote.","title":"Configurando o pacote"},{"location":"access_data_packages/#faca-sua-primeira-consulta","text":"Um exemplo simples para come\u00e7ar a explorar o datalake \u00e9 puxar informa\u00e7\u00f5es cadastrais de munic\u00edpios direto na nossa base de Diret\u00f3rios Brasileiros (tabela municipio ) . Para isso, vamos usar a fun\u00e7\u00e3o download , baixando os dados direto para nossa m\u00e1quina. Python import basedosdados as bd bd . download ( savepath = \"<PATH>\" , dataset_id = \"br-bd-diretorios-brasil\" , table_id = \"municipio\" ) Para entender mais sobre a fun\u00e7\u00e3o download , leia o manual de refer\u00eancia . R library ( \"basedosdados\" ) query <- \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\" dir <- tempdir () data <- download ( query , \"<PATH>\" ) Para entender mais sobre a fun\u00e7\u00e3o download , leia o manual de refer\u00eancia . Stata bd_read_sql, /// path( \"<PATH>\" ) /// query ( \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\") /// billing_project_id(\"<PROJECT_ID>\") CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query 'SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`' Para entender mais sobre a fun\u00e7\u00e3o download , leia o manual de refer\u00eancia .","title":"Fa\u00e7a sua primeira consulta"},{"location":"access_data_packages/#tutoriais","text":"","title":"Tutoriais"},{"location":"access_data_packages/#como-usar-os-pacotes","text":"Preparamos tutoriais apresentando as principais fun\u00e7\u00f5es de cada pacote para voc\u00ea come\u00e7ar a us\u00e1-los. Python Blog: Introdu\u00e7\u00e3o ao pacote Python Introdu\u00e7\u00e3o ao pacote Python (cont.) V\u00eddeos: Workshop: Aplica\u00e7\u00f5es em Python R Blog: Introdu\u00e7\u00e3o ao pacote R Explorando o Censo Escolar An\u00e1lise: O Brasil nas Olimp\u00edadas V\u00eddeos: Workshop: Aprenda a acessar dados p\u00fablicos em R Stata Documenta\u00e7\u00e3o: GitHub","title":"Como usar os pacotes"},{"location":"access_data_packages/#manuais-de-referencia-api","text":"Python R Stata CLI","title":"Manuais de refer\u00eancia (API)"},{"location":"api_reference_cli/","text":"Linha de comando (CLI) config CLI config commands. Usage: config [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False init Initialize configuration Usage: config init [OPTIONS] Options: Name Type Description Default --overwrite boolean Wheteher to overwrite current config False --help boolean Show this message and exit. False refresh_template Overwrite current templates Usage: config refresh_template [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False dataset Command to manage datasets. Usage: dataset [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False create Create dataset on BigQuery Usage: dataset create [OPTIONS] DATASET_ID Options: Name Type Description Default --mode , -m text What datasets to create [prod staging --if_exists text [raise update --dataset_is_public boolean Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True --location text Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None --help boolean Show this message and exit. False delete Delete dataset Usage: dataset delete [OPTIONS] DATASET_ID Options: Name Type Description Default --mode , -m text What datasets to create [prod staging --help boolean Show this message and exit. False init Initialize metadata files of dataset Usage: dataset init [OPTIONS] DATASET_ID Options: Name Type Description Default --replace boolean Whether to replace current metadata files False --help boolean Show this message and exit. False publicize Make a dataset public Usage: dataset publicize [OPTIONS] DATASET_ID Options: Name Type Description Default --dataset_is_public boolean Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True --help boolean Show this message and exit. False update Update dataset on BigQuery Usage: dataset update [OPTIONS] DATASET_ID Options: Name Type Description Default --mode , -m text What datasets to create [prod staging --help boolean Show this message and exit. False download Downloads data do SAVEPATH. SAVEPATH must point to a .csv file. Example: basedosdados download data.csv --query=\"select * from basedosdados.br_ibge_pib.municipio limit 10\" \\ --billing_project_id=basedosdados-dev Usage: download [OPTIONS] SAVEPATH Options: Name Type Description Default --query text A SQL Standard query to download data from BigQuery None --dataset_id text Dataset_id, enter with table_id to download table None --table_id text Table_id, enter with dataset_id to download table None --query_project_id text Which project the table lives. You can change this you want to query different projects. None --billing_project_id text Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None --limit text Number of rows returned None --help boolean Show this message and exit. False get Get commands. Usage: get [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False dataset_description Get the full description for given dataset Usage: get dataset_description [OPTIONS] DATASET_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --help boolean Show this message and exit. False table_columns Get fields names,types and description for columns at given table Usage: get table_columns [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --help boolean Show this message and exit. False table_description Get the full description for given table Usage: get table_description [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --help boolean Show this message and exit. False list CLI list commands. Usage: list [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False dataset_tables List tables available at given dataset Usage: list dataset_tables [OPTIONS] DATASET_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --filter_by text Filter your search, must be a string None --with_description boolean [bool]Fetch short description for each table False --help boolean Show this message and exit. False datasets List datasets available at given project_id Usage: list datasets [OPTIONS] Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --filter_by text Filter your search, must be a string None --with_description boolean [bool]Fetch short description for each dataset False --help boolean Show this message and exit. False metadata CLI metadata commands. Usage: metadata [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False create Creates new metadata config file Usage: metadata create [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --if_exists text [raise replace --columns text Data columns. Example: --columns=col1,col2 [] --partition_columns text Columns that partition the data. Example: --partition_columns=col1,col2 [] --force_columns boolean Overwrite columns with local columns. False --table_only boolean Force the creation of table_config.yaml file only if dataset_config.yaml doesn't exist. True --help boolean Show this message and exit. False is_updated Check if user's local metadata is updated Usage: metadata is_updated [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --help boolean Show this message and exit. False publish Publish user's local metadata Usage: metadata publish [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --all boolean Force the publishment of metadata specified in both dataset_config.yaml and table_config.yaml at once. False --if_exists text Define what to do in case metadata already exists in CKAN. raise --update_locally boolean Update local metadata with the new CKAN metadata on publish. False --help boolean Show this message and exit. False validate Validate user's local metadata Usage: metadata validate [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --help boolean Show this message and exit. False reauth Reauthorize credentials. Usage: reauth [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False storage Commands for Google Cloud Storage. Usage: storage [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False copy_table Copy table to your bucket Usage: storage copy_table [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --source_bucket_name text N/A basedosdados --dst_bucket_name text Bucket where data will be copied to, defaults to your bucket None --mode , -m text which bucket folder to get the table [raw staging --help boolean Show this message and exit. False delete_table Delete table from bucket Usage: storage delete_table [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --mode , -m text Where to delete the file from [raw staging --bucket_name text Bucket from which to delete data, you can change it to delete from a bucket other than yours None --not_found_ok boolean what to do if table not found False --help boolean Show this message and exit. False download Download file from bucket Usage: storage download [OPTIONS] DATASET_ID TABLE_ID SAVEPATH Options: Name Type Description Default --filename , -f text filename to download single file. If * downloads all files from bucket folder * --mode , -m text Where to download data from [raw staging --partitions text Data partition as value=key/value2=key2 None --if_not_exists text [raise pass] if file file not found at bucket folder --help boolean Show this message and exit. False init Create bucket and initial folders Usage: storage init [OPTIONS] Options: Name Type Description Default --bucket_name text Bucket name basedosdados --replace boolean Whether to replace current bucket files False --very-sure / --not-sure boolean Are you sure that you want to replace current bucket files? False --help boolean Show this message and exit. False upload Upload file to bucket Usage: storage upload [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: Name Type Description Default --mode , -m text Where to save the file [raw staging --partitions text Data partition as value=key/value2=key2 None --if_exists text [raise replace --chunk_size text The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None --help boolean Show this message and exit. False table Command to manage tables. Usage: table [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False append Append new data to existing table Usage: table append [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: Name Type Description Default --partitions text Data partition as value=key/value2=key2 None --if_exists text [raise replace --chunk_size text The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None --help boolean Show this message and exit. False create Create stagging table in BigQuery Usage: table create [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --path , -p path Path of data folder or file. None --if_table_exists text [raise replace --force_dataset boolean Whether to automatically create the dataset folders and in BigQuery True --if_storage_data_exists text [raise replace --if_table_config_exists text [raise replace --source_format text Data source format. Only 'csv' is supported. Defaults to 'csv'. csv --columns_config_url_or_path text Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None --dataset_is_public boolean Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True --location text Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None --chunk_size text The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None --help boolean Show this message and exit. False delete Delete BigQuery table Usage: table delete [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --mode text Which table to delete [prod staging] --help boolean Show this message and exit. False init Create metadata files Usage: table init [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --data_sample_path path Sample data used to pre-fill metadata None --if_folder_exists text [raise replace --if_table_config_exists text [raise replace --source_format text Data source format. Only 'csv' is supported. Defaults to 'csv'. csv --columns_config_url_or_path text google sheets URL. Must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: 'coluna' and column description: 'descricao'. None --help boolean Show this message and exit. False publish Publish staging table to prod Usage: table publish [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --if_exists text [raise replace] actions if table exists --help boolean Show this message and exit. False update Update tables in BigQuery Usage: table update [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --mode text Choose a table from a dataset to update [prod staging --help boolean Show this message and exit. False update_columns Update columns fields in tables_config.yaml Usage: table update_columns [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --columns_config_url_or_path text Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format <dataset_id>.<table_id>:<column_name> - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Args: columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. | None | | --help | boolean | Show this message and exit. | False |","title":"CLI"},{"location":"api_reference_cli/#linha-de-comando-cli","text":"","title":"Linha de comando (CLI)"},{"location":"api_reference_cli/#config","text":"CLI config commands. Usage: config [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"config"},{"location":"api_reference_cli/#init","text":"Initialize configuration Usage: config init [OPTIONS] Options: Name Type Description Default --overwrite boolean Wheteher to overwrite current config False --help boolean Show this message and exit. False","title":"init"},{"location":"api_reference_cli/#refresh_template","text":"Overwrite current templates Usage: config refresh_template [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"refresh_template"},{"location":"api_reference_cli/#dataset","text":"Command to manage datasets. Usage: dataset [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"dataset"},{"location":"api_reference_cli/#create","text":"Create dataset on BigQuery Usage: dataset create [OPTIONS] DATASET_ID Options: Name Type Description Default --mode , -m text What datasets to create [prod staging --if_exists text [raise update --dataset_is_public boolean Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True --location text Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None --help boolean Show this message and exit. False","title":"create"},{"location":"api_reference_cli/#delete","text":"Delete dataset Usage: dataset delete [OPTIONS] DATASET_ID Options: Name Type Description Default --mode , -m text What datasets to create [prod staging --help boolean Show this message and exit. False","title":"delete"},{"location":"api_reference_cli/#init_1","text":"Initialize metadata files of dataset Usage: dataset init [OPTIONS] DATASET_ID Options: Name Type Description Default --replace boolean Whether to replace current metadata files False --help boolean Show this message and exit. False","title":"init"},{"location":"api_reference_cli/#publicize","text":"Make a dataset public Usage: dataset publicize [OPTIONS] DATASET_ID Options: Name Type Description Default --dataset_is_public boolean Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True --help boolean Show this message and exit. False","title":"publicize"},{"location":"api_reference_cli/#update","text":"Update dataset on BigQuery Usage: dataset update [OPTIONS] DATASET_ID Options: Name Type Description Default --mode , -m text What datasets to create [prod staging --help boolean Show this message and exit. False","title":"update"},{"location":"api_reference_cli/#download","text":"Downloads data do SAVEPATH. SAVEPATH must point to a .csv file. Example: basedosdados download data.csv --query=\"select * from basedosdados.br_ibge_pib.municipio limit 10\" \\ --billing_project_id=basedosdados-dev Usage: download [OPTIONS] SAVEPATH Options: Name Type Description Default --query text A SQL Standard query to download data from BigQuery None --dataset_id text Dataset_id, enter with table_id to download table None --table_id text Table_id, enter with dataset_id to download table None --query_project_id text Which project the table lives. You can change this you want to query different projects. None --billing_project_id text Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None --limit text Number of rows returned None --help boolean Show this message and exit. False","title":"download"},{"location":"api_reference_cli/#get","text":"Get commands. Usage: get [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"get"},{"location":"api_reference_cli/#dataset_description","text":"Get the full description for given dataset Usage: get dataset_description [OPTIONS] DATASET_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --help boolean Show this message and exit. False","title":"dataset_description"},{"location":"api_reference_cli/#table_columns","text":"Get fields names,types and description for columns at given table Usage: get table_columns [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --help boolean Show this message and exit. False","title":"table_columns"},{"location":"api_reference_cli/#table_description","text":"Get the full description for given table Usage: get table_description [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --help boolean Show this message and exit. False","title":"table_description"},{"location":"api_reference_cli/#list","text":"CLI list commands. Usage: list [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"list"},{"location":"api_reference_cli/#dataset_tables","text":"List tables available at given dataset Usage: list dataset_tables [OPTIONS] DATASET_ID Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --filter_by text Filter your search, must be a string None --with_description boolean [bool]Fetch short description for each table False --help boolean Show this message and exit. False","title":"dataset_tables"},{"location":"api_reference_cli/#datasets","text":"List datasets available at given project_id Usage: list datasets [OPTIONS] Options: Name Type Description Default --project_id text The project which will be queried. You should have list/read permissions basedosdados --filter_by text Filter your search, must be a string None --with_description boolean [bool]Fetch short description for each dataset False --help boolean Show this message and exit. False","title":"datasets"},{"location":"api_reference_cli/#metadata","text":"CLI metadata commands. Usage: metadata [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"metadata"},{"location":"api_reference_cli/#create_1","text":"Creates new metadata config file Usage: metadata create [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --if_exists text [raise replace --columns text Data columns. Example: --columns=col1,col2 [] --partition_columns text Columns that partition the data. Example: --partition_columns=col1,col2 [] --force_columns boolean Overwrite columns with local columns. False --table_only boolean Force the creation of table_config.yaml file only if dataset_config.yaml doesn't exist. True --help boolean Show this message and exit. False","title":"create"},{"location":"api_reference_cli/#is_updated","text":"Check if user's local metadata is updated Usage: metadata is_updated [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"is_updated"},{"location":"api_reference_cli/#publish","text":"Publish user's local metadata Usage: metadata publish [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --all boolean Force the publishment of metadata specified in both dataset_config.yaml and table_config.yaml at once. False --if_exists text Define what to do in case metadata already exists in CKAN. raise --update_locally boolean Update local metadata with the new CKAN metadata on publish. False --help boolean Show this message and exit. False","title":"publish"},{"location":"api_reference_cli/#validate","text":"Validate user's local metadata Usage: metadata validate [OPTIONS] DATASET_ID [TABLE_ID] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"validate"},{"location":"api_reference_cli/#reauth","text":"Reauthorize credentials. Usage: reauth [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"reauth"},{"location":"api_reference_cli/#storage","text":"Commands for Google Cloud Storage. Usage: storage [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"storage"},{"location":"api_reference_cli/#copy_table","text":"Copy table to your bucket Usage: storage copy_table [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --source_bucket_name text N/A basedosdados --dst_bucket_name text Bucket where data will be copied to, defaults to your bucket None --mode , -m text which bucket folder to get the table [raw staging --help boolean Show this message and exit. False","title":"copy_table"},{"location":"api_reference_cli/#delete_table","text":"Delete table from bucket Usage: storage delete_table [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --mode , -m text Where to delete the file from [raw staging --bucket_name text Bucket from which to delete data, you can change it to delete from a bucket other than yours None --not_found_ok boolean what to do if table not found False --help boolean Show this message and exit. False","title":"delete_table"},{"location":"api_reference_cli/#download_1","text":"Download file from bucket Usage: storage download [OPTIONS] DATASET_ID TABLE_ID SAVEPATH Options: Name Type Description Default --filename , -f text filename to download single file. If * downloads all files from bucket folder * --mode , -m text Where to download data from [raw staging --partitions text Data partition as value=key/value2=key2 None --if_not_exists text [raise pass] if file file not found at bucket folder --help boolean Show this message and exit. False","title":"download"},{"location":"api_reference_cli/#init_2","text":"Create bucket and initial folders Usage: storage init [OPTIONS] Options: Name Type Description Default --bucket_name text Bucket name basedosdados --replace boolean Whether to replace current bucket files False --very-sure / --not-sure boolean Are you sure that you want to replace current bucket files? False --help boolean Show this message and exit. False","title":"init"},{"location":"api_reference_cli/#upload","text":"Upload file to bucket Usage: storage upload [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: Name Type Description Default --mode , -m text Where to save the file [raw staging --partitions text Data partition as value=key/value2=key2 None --if_exists text [raise replace --chunk_size text The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None --help boolean Show this message and exit. False","title":"upload"},{"location":"api_reference_cli/#table","text":"Command to manage tables. Usage: table [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --help boolean Show this message and exit. False","title":"table"},{"location":"api_reference_cli/#append","text":"Append new data to existing table Usage: table append [OPTIONS] DATASET_ID TABLE_ID FILEPATH Options: Name Type Description Default --partitions text Data partition as value=key/value2=key2 None --if_exists text [raise replace --chunk_size text The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None --help boolean Show this message and exit. False","title":"append"},{"location":"api_reference_cli/#create_2","text":"Create stagging table in BigQuery Usage: table create [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --path , -p path Path of data folder or file. None --if_table_exists text [raise replace --force_dataset boolean Whether to automatically create the dataset folders and in BigQuery True --if_storage_data_exists text [raise replace --if_table_config_exists text [raise replace --source_format text Data source format. Only 'csv' is supported. Defaults to 'csv'. csv --columns_config_url_or_path text Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None --dataset_is_public boolean Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True --location text Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None --chunk_size text The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None --help boolean Show this message and exit. False","title":"create"},{"location":"api_reference_cli/#delete_1","text":"Delete BigQuery table Usage: table delete [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --mode text Which table to delete [prod staging] --help boolean Show this message and exit. False","title":"delete"},{"location":"api_reference_cli/#init_3","text":"Create metadata files Usage: table init [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --data_sample_path path Sample data used to pre-fill metadata None --if_folder_exists text [raise replace --if_table_config_exists text [raise replace --source_format text Data source format. Only 'csv' is supported. Defaults to 'csv'. csv --columns_config_url_or_path text google sheets URL. Must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . The sheet must contain the column name: 'coluna' and column description: 'descricao'. None --help boolean Show this message and exit. False","title":"init"},{"location":"api_reference_cli/#publish_1","text":"Publish staging table to prod Usage: table publish [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --if_exists text [raise replace] actions if table exists --help boolean Show this message and exit. False","title":"publish"},{"location":"api_reference_cli/#update_1","text":"Update tables in BigQuery Usage: table update [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --mode text Choose a table from a dataset to update [prod staging --help boolean Show this message and exit. False","title":"update"},{"location":"api_reference_cli/#update_columns","text":"Update columns fields in tables_config.yaml Usage: table update_columns [OPTIONS] DATASET_ID TABLE_ID Options: Name Type Description Default --columns_config_url_or_path text Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format <dataset_id>.<table_id>:<column_name> - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Args: columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. | None | | --help | boolean | Show this message and exit. | False |","title":"update_columns"},{"location":"api_reference_python/","text":"Python Esta API \u00e9 composta por fun\u00e7\u00f5es com 2 tipos de funcionalidade: M\u00f3dulos para requisi\u00e7\u00e3o de dados : para aquele(as) que desejam somente consultar os dados e metadados do nosso projeto (ou qualquer outro projeto no Google Cloud). Classes para gerenciamento de dados no Google Cloud: para aqueles(as) que desejam subir dados no nosso projeto (ou qualquer outro projeto no Google Cloud, seguindo a nossa metodologia e infraestrutura). Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas M\u00f3dulos (Requisi\u00e7\u00e3o de dados) Functions for managing downloads download ( savepath , query = None , dataset_id = None , table_id = None , billing_project_id = None , query_project_id = 'basedosdados' , limit = None , from_file = False , reauth = False , compression = 'GZIP' ) Download table or query result from basedosdados BigQuery (or other). Using a query : download('select * from basedosdados.br_suporte.diretorio_municipios limit 10') Using dataset_id & table_id : download(dataset_id='br_suporte', table_id='diretorio_municipios') You can also add arguments to modify save parameters: download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|') Parameters: Name Type Description Default savepath str, pathlib.PosixPath savepath must be a file path. Only supports .csv . required query str Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. None dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' limit int Optional Number of rows. None from_file boolean Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ False reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False compression str Optional. Compression type. Only GZIP is available for now. 'GZIP' Exceptions: Type Description Exception If either table_id, dataset_id or query are empty. Source code in basedosdados/download/download.py def download ( savepath , query = None , dataset_id = None , table_id = None , billing_project_id = None , query_project_id = \"basedosdados\" , limit = None , from_file = False , reauth = False , compression = \"GZIP\" , ): \"\"\"Download table or query result from basedosdados BigQuery (or other). * Using a **query**: `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')` * Using **dataset_id & table_id**: `download(dataset_id='br_suporte', table_id='diretorio_municipios')` You can also add arguments to modify save parameters: `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')` Args: savepath (str, pathlib.PosixPath): savepath must be a file path. Only supports `.csv`. query (str): Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. limit (int): Optional Number of rows. from_file (boolean): Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. compression (str): Optional. Compression type. Only `GZIP` is available for now. Raises: Exception: If either table_id, dataset_id or query are empty. \"\"\" billing_project_id , from_file = _set_config_variables ( billing_project_id = billing_project_id , from_file = from_file ) if ( query is None ) and (( table_id is None ) or ( dataset_id is None )): raise BaseDosDadosException ( \"Either table_id, dataset_id or query should be filled.\" ) client = google_client ( billing_project_id , from_file , reauth ) # makes sure that savepath is a filepath and not a folder savepath = _sets_savepath ( savepath ) # if query is not defined (so it won't be overwritten) and if # table is a view or external or if limit is specified, # convert it to a query. if not query and ( not _is_table ( client , dataset_id , table_id , query_project_id ) or limit ): query = f \"\"\" SELECT * FROM { query_project_id } . { dataset_id } . { table_id } \"\"\" if limit is not None : query += f \" limit { limit } \" if query : # sql queries produces anonymous tables, whose names # can be found within `job._properties` job = client [ \"bigquery\" ] . query ( query ) # views may take longer: wait for job to finish. _wait_for ( job ) dest_table = job . _properties [ \"configuration\" ][ \"query\" ][ \"destinationTable\" ] project_id = dest_table [ \"projectId\" ] dataset_id = dest_table [ \"datasetId\" ] table_id = dest_table [ \"tableId\" ] _direct_download ( client , dataset_id , table_id , savepath , project_id , compression ) read_sql ( query , billing_project_id = None , from_file = False , reauth = False , use_bqstorage_api = False ) Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Parameters: Name Type Description Default query sql Valid SQL Standard Query to basedosdados required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None from_file boolean Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ False reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False use_bqstorage_api boolean Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_sql ( query , billing_project_id = None , from_file = False , reauth = False , use_bqstorage_api = False , ): \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Args: query (sql): Valid SQL Standard Query to basedosdados billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard from_file (boolean): Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. use_bqstorage_api (boolean): Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. Returns: pd.DataFrame: Query result \"\"\" billing_project_id , from_file = _set_config_variables ( billing_project_id = billing_project_id , from_file = from_file ) try : # Set a two hours timeout bigquery_storage_v1 . client . BigQueryReadClient . read_rows = partialmethod ( bigquery_storage_v1 . client . BigQueryReadClient . read_rows , timeout = 3600 * 2 , ) return pandas_gbq . read_gbq ( query , credentials = credentials ( from_file = from_file , reauth = reauth ), project_id = billing_project_id , use_bqstorage_api = use_bqstorage_api , ) except GenericGBQException as e : if \"Reason: 403\" in str ( e ): raise BaseDosDadosAccessDeniedException from e if re . match ( \"Reason: 400 POST .* [Pp]roject[ ]*I[Dd]\" , str ( e )): raise BaseDosDadosInvalidProjectIDException from e raise except PyDataCredentialsError as e : raise BaseDosDadosAuthorizationException from e except ( OSError , ValueError ) as e : no_billing_id = \"Could not determine project ID\" in str ( e ) no_billing_id |= \"reading from stdin while output is captured\" in str ( e ) if no_billing_id : raise BaseDosDadosNoBillingProjectIDException from e raise read_table ( dataset_id , table_id , billing_project_id = None , query_project_id = 'basedosdados' , limit = None , from_file = False , reauth = False , use_bqstorage_api = False ) Load data from BigQuery using dataset_id and table_id. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' limit int Optional. Number of rows to read from table. None from_file boolean Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ False reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False use_bqstorage_api boolean Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_table ( dataset_id , table_id , billing_project_id = None , query_project_id = \"basedosdados\" , limit = None , from_file = False , reauth = False , use_bqstorage_api = False , ): \"\"\"Load data from BigQuery using dataset_id and table_id. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. limit (int): Optional. Number of rows to read from table. from_file (boolean): Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. use_bqstorage_api (boolean): Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. Returns: pd.DataFrame: Query result \"\"\" billing_project_id , from_file = _set_config_variables ( billing_project_id = billing_project_id , from_file = from_file ) if ( dataset_id is not None ) and ( table_id is not None ): query = f \"\"\" SELECT * FROM ` { query_project_id } . { dataset_id } . { table_id } `\"\"\" if limit is not None : query += f \" LIMIT { limit } \" else : raise BaseDosDadosException ( \"Both table_id and dataset_id should be filled.\" ) return read_sql ( query , billing_project_id = billing_project_id , from_file = from_file , reauth = reauth , use_bqstorage_api = use_bqstorage_api , ) Classes (Gerenciamento de dados) Class for managing the files in cloud storage. Storage ( Base ) Manage files on Google Cloud Storage. Source code in basedosdados/upload/storage.py class Storage ( Base ): \"\"\" Manage files on Google Cloud Storage. \"\"\" def __init__ ( self , dataset_id = None , table_id = None , ** kwargs ): super () . __init__ ( ** kwargs ) self . bucket = self . client [ \"storage_staging\" ] . bucket ( self . bucket_name ) self . dataset_id = dataset_id . replace ( \"-\" , \"_\" ) self . table_id = table_id . replace ( \"-\" , \"_\" ) @staticmethod def _resolve_partitions ( partitions ): if isinstance ( partitions , dict ): return \"/\" . join ( f \" { k } = { v } \" for k , v in partitions . items ()) + \"/\" if isinstance ( partitions , str ): if partitions . endswith ( \"/\" ): partitions = partitions [: - 1 ] # If there is no partition if len ( partitions ) == 0 : return \"\" # It should fail if there is folder which is not a partition try : # check if it fits rule { b . split ( \"=\" )[ 0 ]: b . split ( \"=\" )[ 1 ] for b in partitions . split ( \"/\" )} except IndexError as e : raise Exception ( f \"The path { partitions } is not a valid partition\" ) from e return partitions + \"/\" raise Exception ( f \"Partitions format or type not accepted: { partitions } \" ) def _build_blob_name ( self , filename , mode , partitions = None ): ''' Builds the blob name. ''' # table folder blob_name = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # add partition folder if partitions is not None : blob_name += self . _resolve_partitions ( partitions ) # add file name blob_name += filename return blob_name def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" ) def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , chunk_size = None , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 6 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix in [ \".csv\" , \".parquet\" , \"parquet.gzip\" ] ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name , chunk_size = chunk_size ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists == \"pass\" : pass else : raise BaseDosDadosException ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"If you are using Storage.upload then set if_exists to \" \"'replace' to overwrite data \\n \" \"If you are using Table.create then set if_storage_data_exists \" \"to 'replace' to overwrite data.\" ) logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filepath . name , mode = mode , object = \"File\" , action = \"uploaded\" , ) def download ( self , filename = \"*\" , savepath = \"\" , partitions = None , mode = \"raw\" , if_not_exists = \"raise\" , ): \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy on save, There are 5 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables You can also use the `partitions` argument to choose files from a partition Args: filename (str): Optional Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\". savepath (str): Where you want to save the data on your computer. Must be a path to a directory. partitions (str, dict): Optional If downloading a single file, use this to specify the partition path from which to download. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` mode (str): Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture] if_not_exists (str): Optional. What to do if data not found. * 'raise' : Raises FileNotFoundError. * 'pass' : Do nothing and exit the function Raises: FileNotFoundError: If the given path `<mode>/<dataset_id>/<table_id>/<partitions>/<filename>` could not be found or there are no files to download. \"\"\" # Prefix to locate files within the bucket prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # Add specific partition to search prefix if partitions : prefix += self . _resolve_partitions ( partitions ) # if no filename is passed, list all blobs within a given table if filename != \"*\" : prefix += filename blob_list = list ( self . bucket . list_blobs ( prefix = prefix )) # if there are no blobs matching the search raise FileNotFoundError or return if not blob_list : if if_not_exists == \"raise\" : raise FileNotFoundError ( f \"Could not locate files at { prefix } \" ) return # download all blobs matching the search to given savepath for blob in tqdm ( blob_list , desc = \"Download Blob\" ): # parse blob.name and get the csv file name csv_name = blob . name . split ( \"/\" )[ - 1 ] # build folder path replicating storage hierarchy blob_folder = blob . name . replace ( csv_name , \"\" ) # replicate folder hierarchy ( Path ( savepath ) / blob_folder ) . mkdir ( parents = True , exist_ok = True ) # download blob to savepath blob . download_to_filename ( filename = f \" { savepath } / { blob . name } \" ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"File\" , action = \"downloaded\" , ) def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists () or not blob . exists () and not not_found_ok : blob . delete () else : return logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filename , mode = mode , object = \"File\" , action = \"deleted\" , ) def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. Args: mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = list ( self . client [ \"storage_staging\" ] . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) ) else : table_blobs = list ( self . bucket . list_blobs ( prefix = prefix )) if not table_blobs : if not_found_ok : return raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) # Divides table_blobs list for maximum batch request size table_blobs_chunks = [ table_blobs [ i : i + 999 ] for i in range ( 0 , len ( table_blobs ), 999 ) ] for i , source_table in enumerate ( tqdm ( table_blobs_chunks , desc = \"Delete Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : blob . delete () break except Exception : print ( f \"Delete Table Chunk { i } | Attempt { counter } : delete operation starts again in 5 seconds...\" , ) time . sleep ( 5 ) counter += 1 traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name (str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = list ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" ) ) if not source_table_ref : raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) # Divides source_table_ref list for maximum batch request size source_table_ref_chunks = [ source_table_ref [ i : i + 999 ] for i in range ( 0 , len ( source_table_ref ), 999 ) ] for i , source_table in enumerate ( tqdm ( source_table_ref_chunks , desc = \"Copy Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket , ) break except Exception : print ( f \"Copy Table Chunk { i } | Attempt { counter } : copy operation starts again in 5 seconds...\" , ) counter += 1 time . sleep ( 5 ) traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"copied\" , ) copy_table ( self , source_bucket_name = 'basedosdados' , destination_bucket_name = None , mode = 'staging' ) Copies table from a source bucket to your bucket, sends request in batches. Parameters: Name Type Description Default source_bucket_name str The bucket name from which to copy data. You can change it to copy from other external bucket. 'basedosdados' destination_bucket_name str Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) None mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". 'staging' Source code in basedosdados/upload/storage.py def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name (str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = list ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" ) ) if not source_table_ref : raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) # Divides source_table_ref list for maximum batch request size source_table_ref_chunks = [ source_table_ref [ i : i + 999 ] for i in range ( 0 , len ( source_table_ref ), 999 ) ] for i , source_table in enumerate ( tqdm ( source_table_ref_chunks , desc = \"Copy Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket , ) break except Exception : print ( f \"Copy Table Chunk { i } | Attempt { counter } : copy operation starts again in 5 seconds...\" , ) counter += 1 time . sleep ( 5 ) traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"copied\" , ) delete_file ( self , filename , mode , partitions = None , not_found_ok = False ) Deletes file from path <bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename> . Parameters: Name Type Description Default filename str Name of the file to be deleted required mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] required partitions str, pathlib.PosixPath, or dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None not_found_ok bool Optional. What to do if file not found False Source code in basedosdados/upload/storage.py def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists () or not blob . exists () and not not_found_ok : blob . delete () else : return logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filename , mode = mode , object = \"File\" , action = \"deleted\" , ) delete_table ( self , mode = 'staging' , bucket_name = None , not_found_ok = False ) Deletes a table from storage, sends request in batches. Parameters: Name Type Description Default mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". 'staging' bucket_name str The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) None not_found_ok bool Optional. What to do if table not found False Source code in basedosdados/upload/storage.py def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. Args: mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = list ( self . client [ \"storage_staging\" ] . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) ) else : table_blobs = list ( self . bucket . list_blobs ( prefix = prefix )) if not table_blobs : if not_found_ok : return raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) # Divides table_blobs list for maximum batch request size table_blobs_chunks = [ table_blobs [ i : i + 999 ] for i in range ( 0 , len ( table_blobs ), 999 ) ] for i , source_table in enumerate ( tqdm ( table_blobs_chunks , desc = \"Delete Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : blob . delete () break except Exception : print ( f \"Delete Table Chunk { i } | Attempt { counter } : delete operation starts again in 5 seconds...\" , ) time . sleep ( 5 ) counter += 1 traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) download ( self , filename = '*' , savepath = '' , partitions = None , mode = 'raw' , if_not_exists = 'raise' ) Download files from Google Storage from path mode / dataset_id / table_id / partitions / filename and replicate folder hierarchy on save, There are 5 modes: * raw : should contain raw files from datasource * staging : should contain pre-treated files ready to upload to BiqQuery * header : should contain the header of the tables * auxiliary_files : should contain auxiliary files from eache table * architecture : should contain the architecture sheet of the tables You can also use the partitions argument to choose files from a partition Parameters: Name Type Description Default filename str Optional Specify which file to download. If \" \" , downloads all files within the bucket folder. Defaults to \" \". '*' savepath str Where you want to save the data on your computer. Must be a path to a directory. '' partitions str, dict Optional If downloading a single file, use this to specify the partition path from which to download. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None mode str Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture] 'raw' if_not_exists str Optional. What to do if data not found. 'raise' : Raises FileNotFoundError. 'pass' : Do nothing and exit the function 'raise' Exceptions: Type Description FileNotFoundError If the given path <mode>/<dataset_id>/<table_id>/<partitions>/<filename> could not be found or there are no files to download. Source code in basedosdados/upload/storage.py def download ( self , filename = \"*\" , savepath = \"\" , partitions = None , mode = \"raw\" , if_not_exists = \"raise\" , ): \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy on save, There are 5 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables You can also use the `partitions` argument to choose files from a partition Args: filename (str): Optional Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\". savepath (str): Where you want to save the data on your computer. Must be a path to a directory. partitions (str, dict): Optional If downloading a single file, use this to specify the partition path from which to download. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` mode (str): Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture] if_not_exists (str): Optional. What to do if data not found. * 'raise' : Raises FileNotFoundError. * 'pass' : Do nothing and exit the function Raises: FileNotFoundError: If the given path `<mode>/<dataset_id>/<table_id>/<partitions>/<filename>` could not be found or there are no files to download. \"\"\" # Prefix to locate files within the bucket prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # Add specific partition to search prefix if partitions : prefix += self . _resolve_partitions ( partitions ) # if no filename is passed, list all blobs within a given table if filename != \"*\" : prefix += filename blob_list = list ( self . bucket . list_blobs ( prefix = prefix )) # if there are no blobs matching the search raise FileNotFoundError or return if not blob_list : if if_not_exists == \"raise\" : raise FileNotFoundError ( f \"Could not locate files at { prefix } \" ) return # download all blobs matching the search to given savepath for blob in tqdm ( blob_list , desc = \"Download Blob\" ): # parse blob.name and get the csv file name csv_name = blob . name . split ( \"/\" )[ - 1 ] # build folder path replicating storage hierarchy blob_folder = blob . name . replace ( csv_name , \"\" ) # replicate folder hierarchy ( Path ( savepath ) / blob_folder ) . mkdir ( parents = True , exist_ok = True ) # download blob to savepath blob . download_to_filename ( filename = f \" { savepath } / { blob . name } \" ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"File\" , action = \"downloaded\" , ) init ( self , replace = False , very_sure = False ) Initializes bucket and folders. Folder should be: raw : that contains really raw data staging : preprocessed data ready to upload to BigQuery Parameters: Name Type Description Default replace bool Optional. Whether to replace if bucket already exists False very_sure bool Optional. Are you aware that everything is going to be erased if you replace the bucket? False Exceptions: Type Description Warning very_sure argument is still False. Source code in basedosdados/upload/storage.py def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" ) upload ( self , path , mode = 'all' , partitions = None , if_exists = 'raise' , chunk_size = None , ** upload_args ) Upload to storage at <bucket_name>/<mode>/<dataset_id>/<table_id> . You can: Add a single file setting path = <file_path> . Add a folder with multiple files setting path = <folder_path> . The folder should just contain the files and no folders. Add partitioned files setting path = <folder_path> . This folder must follow the hive partitioning scheme i.e. <table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv (ex: mytable/country=brasil/year=2020/mypart.csv ). Remember all files must follow a single schema. Otherwise, things might fail in the future. There are 6 modes: raw : should contain raw files from datasource staging : should contain pre-treated files ready to upload to BiqQuery header : should contain the header of the tables auxiliary_files : should contain auxiliary files from eache table architecture : should contain the architecture sheet of the tables all : if no treatment is needed, use all . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file or folder that you want to upload to storage required mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] 'all' partitions str, pathlib.PosixPath, or dict Optional. If adding a single file , use this to add it to a specific partition. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str Optional. What to do if data exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' chunk_size int Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. None upload_args Extra arguments accepted by google.cloud.storage.blob.Blob.upload_from_file {} Source code in basedosdados/upload/storage.py def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , chunk_size = None , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 6 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix in [ \".csv\" , \".parquet\" , \"parquet.gzip\" ] ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name , chunk_size = chunk_size ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists == \"pass\" : pass else : raise BaseDosDadosException ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"If you are using Storage.upload then set if_exists to \" \"'replace' to overwrite data \\n \" \"If you are using Table.create then set if_storage_data_exists \" \"to 'replace' to overwrite data.\" ) logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filepath . name , mode = mode , object = \"File\" , action = \"uploaded\" , ) Module for manage dataset to the server. Dataset ( Base ) Manage datasets in BigQuery. Source code in basedosdados/upload/dataset.py class Dataset ( Base ): \"\"\" Manage datasets in BigQuery. \"\"\" def __init__ ( self , dataset_id , ** kwargs ): super () . __init__ ( ** kwargs ) self . dataset_id = dataset_id . replace ( \"-\" , \"_\" ) self . dataset_folder = Path ( self . metadata_path / self . dataset_id ) self . metadata = Metadata ( self . dataset_id , ** kwargs ) @property def dataset_config ( self ): \"\"\" Dataset config file. \"\"\" return self . _load_yaml ( self . metadata_path / self . dataset_id / \"dataset_config.yaml\" ) def _loop_modes ( self , mode = \"all\" ): \"\"\" Loop modes. \"\"\" mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] dataset_tag = lambda m : f \"_ { m } \" if m == \"staging\" else \"\" return ( { \"client\" : self . client [ f \"bigquery_ { m } \" ], \"id\" : f \" { self . client [ f 'bigquery_ { m } ' ] . project } . { self . dataset_id }{ dataset_tag ( m ) } \" , } for m in mode ) @staticmethod def _setup_dataset_object ( dataset_id , location = None ): \"\"\" Setup dataset object. \"\"\" dataset = bigquery . Dataset ( dataset_id ) ## TODO: not being used since 1.6.0 - need to redo the description tha goes to bigquery dataset . description = \"Para saber mais acesse https://basedosdados.org/\" # dataset.description = self._render_template( # Path(\"dataset/dataset_description.txt\"), self.dataset_config # ) dataset . location = location return dataset def _write_readme_file ( self ): \"\"\" Write README.md file. \"\"\" readme_content = ( f \"Como capturar os dados de { self . dataset_id } ? \\n\\n Para cap\" f \"turar esses dados, basta verificar o link dos dados orig\" f \"inais indicado em dataset_config.yaml no item website. \\n \" f \" \\n Caso tenha sido utilizado algum c\u00f3digo de captura ou t\" f \"ratamento, estes estar\u00e3o contidos em code/. Se o dado pu\" f \"blicado for em sua vers\u00e3o bruta, n\u00e3o existir\u00e1 a pasta co\" f \"de/. \\n\\n Os dados publicados est\u00e3o dispon\u00edveis em: https:\" f \"//basedosdados.org/dataset/ { self . dataset_id . replace ( '_' , '-' ) } \" ) readme_path = Path ( self . metadata_path / self . dataset_id / \"README.md\" ) with open ( readme_path , \"w\" , encoding = \"utf-8\" ) as readmefile : readmefile . write ( readme_content ) def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError as e : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) from e # create dataset_config.yaml with metadata self . metadata . create ( if_exists = \"replace\" ) # create README.md file self . _write_readme_file () # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self def publicize ( self , mode = \"all\" , dataset_is_public = True ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries # TODO https://github.com/basedosdados/mais/pull/1020 # TODO if staging dataset is private, the prod view can't acess it: if dataset_is_public and \"staging\" not in dataset.dataset_id: if dataset_is_public : if \"staging\" not in dataset . dataset_id : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) else : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ]) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"publicized\" , ) def create ( self , mode = \"all\" , if_exists = \"raise\" , dataset_is_public = True , location = None ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflict exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ], location = location ) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"created\" , ) except Conflict as e : if if_exists == \"pass\" : return raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) from e # Make prod dataset public self . publicize ( dataset_is_public = dataset_is_public ) def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"deleted\" , ) def update ( self , mode = \"all\" , location = None ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ], location = location , ), fields = [ \"description\" ], ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"updated\" , ) dataset_config property readonly Dataset config file. create ( self , mode = 'all' , if_exists = 'raise' , dataset_is_public = True , location = None ) Creates BigQuery datasets given dataset_id . It can create two datasets: <dataset_id> (mode = 'prod') <dataset_id>_staging (mode = 'staging') If mode is all, it creates both. Parameters: Name Type Description Default mode str Optional. Which dataset to create [prod|staging|all]. 'all' if_exists str Optional. What to do if dataset exists raise : Raises Conflict exception replace : Drop all tables and replace dataset update : Update dataset description pass : Do nothing 'raise' dataset_is_public bool Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True location str Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None Exceptions: Type Description Warning Dataset already exists and if_exists is set to raise Source code in basedosdados/upload/dataset.py def create ( self , mode = \"all\" , if_exists = \"raise\" , dataset_is_public = True , location = None ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflict exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ], location = location ) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"created\" , ) except Conflict as e : if if_exists == \"pass\" : return raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) from e # Make prod dataset public self . publicize ( dataset_is_public = dataset_is_public ) delete ( self , mode = 'all' ) Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Parameters: Name Type Description Default mode str Optional. Which dataset to delete [prod|staging|all] 'all' Source code in basedosdados/upload/dataset.py def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"deleted\" , ) init ( self , replace = False ) Initialize dataset folder at metadata_path at metadata_path/<dataset_id> . The folder should contain: dataset_config.yaml README.md Parameters: Name Type Description Default replace str Optional. Whether to replace existing folder. False Exceptions: Type Description FileExistsError If dataset folder already exists and replace is False Source code in basedosdados/upload/dataset.py def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError as e : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) from e # create dataset_config.yaml with metadata self . metadata . create ( if_exists = \"replace\" ) # create README.md file self . _write_readme_file () # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self publicize ( self , mode = 'all' , dataset_is_public = True ) Changes IAM configuration to turn BigQuery dataset public. Parameters: Name Type Description Default mode bool Which dataset to create [prod|staging|all]. 'all' dataset_is_public bool Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True Source code in basedosdados/upload/dataset.py def publicize ( self , mode = \"all\" , dataset_is_public = True ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries # TODO https://github.com/basedosdados/mais/pull/1020 # TODO if staging dataset is private, the prod view can't acess it: if dataset_is_public and \"staging\" not in dataset.dataset_id: if dataset_is_public : if \"staging\" not in dataset . dataset_id : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) else : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ]) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"publicized\" , ) update ( self , mode = 'all' , location = None ) Update dataset description. Toogle mode to choose which dataset to update. Parameters: Name Type Description Default mode str Optional. Which dataset to update [prod|staging|all] 'all' location str Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None Source code in basedosdados/upload/dataset.py def update ( self , mode = \"all\" , location = None ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ], location = location , ), fields = [ \"description\" ], ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"updated\" , ) Class for manage tables in Storage and Big Query Table ( Base ) Manage tables in Google Cloud Storage and BigQuery. Source code in basedosdados/upload/table.py class Table ( Base ): \"\"\" Manage tables in Google Cloud Storage and BigQuery. \"\"\" def __init__ ( self , dataset_id , table_id , ** kwargs ): super () . __init__ ( ** kwargs ) self . table_id = table_id . replace ( \"-\" , \"_\" ) self . dataset_id = dataset_id . replace ( \"-\" , \"_\" ) self . dataset_folder = Path ( self . metadata_path / self . dataset_id ) self . table_folder = self . dataset_folder / table_id self . table_full_name = dict ( prod = f \" { self . client [ 'bigquery_prod' ] . project } . { self . dataset_id } . { self . table_id } \" , staging = f \" { self . client [ 'bigquery_staging' ] . project } . { self . dataset_id } _staging. { self . table_id } \" , ) self . table_full_name . update ( dict ( all = deepcopy ( self . table_full_name ))) self . metadata = Metadata ( self . dataset_id , self . table_id , ** kwargs ) @property def table_config ( self ): \"\"\" Load table_config.yaml \"\"\" return self . _load_yaml ( self . table_folder / \"table_config.yaml\" ) def _get_table_obj ( self , mode ): \"\"\" Get table object from BigQuery \"\"\" return self . client [ f \"bigquery_ { mode } \" ] . get_table ( self . table_full_name [ mode ]) def _is_partitioned ( self ): \"\"\" Check if table is partitioned \"\"\" ## check if the table are partitioned, need the split because of a change in the type of partitions in pydantic partitions = self . table_config [ \"partitions\" ] if partitions is None or len ( partitions ) == 0 : return False if isinstance ( partitions , list ): # check if any None inside list. # False if it is the case Ex: [None, 'partition'] # True otherwise Ex: ['partition1', 'partition2'] return all ( item is not None for item in partitions ) raise ValueError ( \"Partitions must be a list or None\" ) def _load_schema ( self , mode = \"staging\" ): \"\"\"Load schema from table_config.yaml Args: mode (bool): Which dataset to create [prod|staging]. \"\"\" self . _check_mode ( mode ) json_path = self . table_folder / f \"schema- { mode } .json\" columns = self . table_config [ \"columns\" ] if mode == \"staging\" : new_columns = [] for c in columns : # case is_in_staging are None then must be True is_in_staging = ( True if c . get ( \"is_in_staging\" ) is None else c [ \"is_in_staging\" ] ) # append columns declared in table_config.yaml to schema only if is_in_staging: True if is_in_staging and not c . get ( \"is_partition\" ): c [ \"type\" ] = \"STRING\" new_columns . append ( c ) del columns columns = new_columns elif mode == \"prod\" : schema = self . _get_table_obj ( mode ) . schema # get field names for fields at schema and at table_config.yaml column_names = [ c [ \"name\" ] for c in columns ] schema_names = [ s . name for s in schema ] # check if there are mismatched fields not_in_columns = [ name for name in schema_names if name not in column_names ] not_in_schema = [ name for name in column_names if name not in schema_names ] # raise if field is not in table_config if not_in_columns : raise BaseDosDadosException ( \"Column {error_columns} was not found in table_config.yaml. Are you sure that \" \"all your column names between table_config.yaml, publish.sql and \" \" {project_id} . {dataset_id} . {table_id} are the same?\" . format ( error_columns = not_in_columns , project_id = self . table_config [ \"project_id_prod\" ], dataset_id = self . table_config [ \"dataset_id\" ], table_id = self . table_config [ \"table_id\" ], ) ) # raise if field is not in schema if not_in_schema : raise BaseDosDadosException ( \"Column {error_columns} was not found in publish.sql. Are you sure that \" \"all your column names between table_config.yaml, publish.sql and \" \" {project_id} . {dataset_id} . {table_id} are the same?\" . format ( error_columns = not_in_schema , project_id = self . table_config [ \"project_id_prod\" ], dataset_id = self . table_config [ \"dataset_id\" ], table_id = self . table_config [ \"table_id\" ], ) ) # if field is in schema, get field_type and field_mode for c in columns : for s in schema : if c [ \"name\" ] == s . name : c [ \"type\" ] = s . field_type c [ \"mode\" ] = s . mode break ## force utf-8, write schema_{mode}.json json . dump ( columns , ( json_path ) . open ( \"w\" , encoding = \"utf-8\" )) # load new created schema return self . client [ f \"bigquery_ { mode } \" ] . schema_from_json ( str ( json_path )) def _make_publish_sql ( self ): \"\"\"Create publish.sql with columns and bigquery_type\"\"\" ### publish.sql header and instructions publish_txt = \"\"\" /* Query para publicar a tabela. Esse \u00e9 o lugar para: - modificar nomes, ordem e tipos de colunas - dar join com outras tabelas - criar colunas extras (e.g. logs, propor\u00e7\u00f5es, etc.) Qualquer coluna definida aqui deve tamb\u00e9m existir em `table_config.yaml`. # Al\u00e9m disso, sinta-se \u00e0 vontade para alterar alguns nomes obscuros # para algo um pouco mais expl\u00edcito. TIPOS: - Para modificar tipos de colunas, basta substituir STRING por outro tipo v\u00e1lido. - Exemplo: `SAFE_CAST(column_name AS NUMERIC) column_name` - Mais detalhes: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types */ \"\"\" # remove triple quotes extra space publish_txt = inspect . cleandoc ( publish_txt ) publish_txt = textwrap . dedent ( publish_txt ) # add create table statement project_id_prod = self . client [ \"bigquery_prod\" ] . project publish_txt += f \" \\n\\n CREATE VIEW { project_id_prod } . { self . dataset_id } . { self . table_id } AS \\n SELECT \\n \" # sort columns by is_partition, partitions_columns come first if self . _is_partitioned (): columns = sorted ( self . table_config [ \"columns\" ], key = lambda k : ( k [ \"is_partition\" ] is not None , k [ \"is_partition\" ]), reverse = True , ) else : columns = self . table_config [ \"columns\" ] # add columns in publish.sql for col in columns : name = col [ \"name\" ] bigquery_type = ( \"STRING\" if col [ \"bigquery_type\" ] is None else col [ \"bigquery_type\" ] . upper () ) publish_txt += f \"SAFE_CAST( { name } AS { bigquery_type } ) { name } , \\n \" ## remove last comma publish_txt = publish_txt [: - 2 ] + \" \\n \" # add from statement project_id_staging = self . client [ \"bigquery_staging\" ] . project publish_txt += ( f \"FROM { project_id_staging } . { self . dataset_id } _staging. { self . table_id } AS t\" ) # save publish.sql in table_folder ( self . table_folder / \"publish.sql\" ) . open ( \"w\" , encoding = \"utf-8\" ) . write ( publish_txt ) def _make_template ( self , columns , partition_columns , if_table_config_exists ): # create table_config.yaml with metadata self . metadata . create ( if_exists = if_table_config_exists , columns = partition_columns + columns , partition_columns = partition_columns , table_only = False , ) self . _make_publish_sql () @staticmethod def _sheet_to_df ( columns_config_url_or_path ): \"\"\" Convert sheet to dataframe \"\"\" url = columns_config_url_or_path . replace ( \"edit#gid=\" , \"export?format=csv&gid=\" ) try : return pd . read_csv ( StringIO ( requests . get ( url , timeout = 10 ) . content . decode ( \"utf-8\" ))) except Exception as e : raise BaseDosDadosException ( \"Check if your google sheet Share are: Anyone on the internet with this link can view\" ) from e def table_exists ( self , mode ): \"\"\"Check if table exists in BigQuery. Args: mode (str): Which dataset to check [prod|staging]. \"\"\" try : ref = self . _get_table_obj ( mode = mode ) except google . api_core . exceptions . NotFound : ref = None return bool ( ref ) def update_columns ( self , columns_config_url_or_path = None ): \"\"\" Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format <dataset_id>.<table_id>:<column_name> - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Args: columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. \"\"\" ruamel = ryaml . YAML () ruamel . preserve_quotes = True ruamel . indent ( mapping = 4 , sequence = 6 , offset = 4 ) table_config_yaml = ruamel . load ( ( self . table_folder / \"table_config.yaml\" ) . open ( encoding = \"utf-8\" ) ) if \"https://docs.google.com/spreadsheets/d/\" in columns_config_url_or_path : if ( \"edit#gid=\" not in columns_config_url_or_path or \"https://docs.google.com/spreadsheets/d/\" not in columns_config_url_or_path or not columns_config_url_or_path . split ( \"=\" )[ 1 ] . isdigit () ): raise BaseDosDadosException ( \"The Google sheet url not in correct format.\" \"The url must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>\" ) df = self . _sheet_to_df ( columns_config_url_or_path ) else : file_type = columns_config_url_or_path . split ( \".\" )[ - 1 ] if file_type == \"csv\" : df = pd . read_csv ( columns_config_url_or_path , encoding = \"utf-8\" ) elif file_type in [ \"xls\" , \"xlsx\" , \"xlsm\" , \"xlsb\" , \"odf\" , \"ods\" , \"odt\" ]: df = pd . read_excel ( columns_config_url_or_path ) else : raise BaseDosDadosException ( \"File not suported. Only csv, xls, xlsx, xlsm, xlsb, odf, ods, odt are supported.\" ) df = df . fillna ( \"NULL\" ) required_columns = [ \"name\" , \"bigquery_type\" , \"description\" , \"temporal_coverage\" , \"covered_by_dictionary\" , \"directory_column\" , \"measurement_unit\" , \"has_sensitive_data\" , \"observations\" , ] not_found_columns = required_columns . copy () for sheet_column in df . columns . tolist (): for required_column in required_columns : if sheet_column == required_column : not_found_columns . remove ( required_column ) if not_found_columns : raise BaseDosDadosException ( f \"The following required columns are not found: { ', ' . join ( not_found_columns ) } .\" ) columns_parameters = zip ( * [ df [ required_column ] . tolist () for required_column in required_columns ] ) for ( name , bigquery_type , description , temporal_coverage , covered_by_dictionary , directory_column , measurement_unit , has_sensitive_data , observations , ) in columns_parameters : for col in table_config_yaml [ \"columns\" ]: if col [ \"name\" ] == name : col [ \"bigquery_type\" ] = ( col [ \"bigquery_type\" ] if bigquery_type == \"NULL\" else bigquery_type . lower () ) col [ \"description\" ] = ( col [ \"description\" ] if description == \"NULL\" else description ) col [ \"temporal_coverage\" ] = ( col [ \"temporal_coverage\" ] if temporal_coverage == \"NULL\" else [ temporal_coverage ] ) col [ \"covered_by_dictionary\" ] = ( \"no\" if covered_by_dictionary == \"NULL\" else covered_by_dictionary ) dataset = directory_column . split ( \".\" )[ 0 ] col [ \"directory_column\" ][ \"dataset_id\" ] = ( col [ \"directory_column\" ][ \"dataset_id\" ] if dataset == \"NULL\" else dataset ) table = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ 0 ] col [ \"directory_column\" ][ \"table_id\" ] = ( col [ \"directory_column\" ][ \"table_id\" ] if table == \"NULL\" else table ) column = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ - 1 ] col [ \"directory_column\" ][ \"column_name\" ] = ( col [ \"directory_column\" ][ \"column_name\" ] if column == \"NULL\" else column ) col [ \"measurement_unit\" ] = ( col [ \"measurement_unit\" ] if measurement_unit == \"NULL\" else measurement_unit ) col [ \"has_sensitive_data\" ] = ( \"no\" if has_sensitive_data == \"NULL\" else has_sensitive_data ) col [ \"observations\" ] = ( col [ \"observations\" ] if observations == \"NULL\" else observations ) with open ( self . table_folder / \"table_config.yaml\" , \"w\" , encoding = \"utf-8\" ) as f : ruamel . dump ( table_config_yaml , f ) # regenerate publish.sql self . _make_publish_sql () def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV, Apache Avro and Apache Parquet. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing if_table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError as e : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) from e if if_folder_exists == \"pass\" : return self if not data_sample_path and if_table_config_exists != \"pass\" : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) partition_columns = [] if isinstance ( data_sample_path , ( str , Path , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == f \". { source_format } \" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] columns = Datatype ( self , source_format ) . header ( data_sample_path ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : # Check if config files exists before passing if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass # Raise if no sample to determine columns elif not data_sample_path : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) else : self . _make_template ( columns , partition_columns , if_table_config_exists ) elif if_table_config_exists == \"raise\" : # Check if config files already exist if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) # if config files don't exist, create them self . _make_template ( columns , partition_columns , if_table_config_exists ) else : # Raise: without a path to data sample, should not replace config files with empty template self . _make_template ( columns , partition_columns , if_table_config_exists ) if columns_config_url_or_path is not None : self . update_columns ( columns_config_url_or_path ) return self def create ( self , path = None , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , dataset_is_public = True , location = None , chunk_size = None , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV - Apache Avro - Apache Parquet Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>` - for instance, `year=2012/country=BR`. The partition is automatcally detected by searching for `partitions` on the `table_config.yaml`. Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if path is None : # Look if table data already exists at Storage data = self . client [ \"storage_staging\" ] . list_blobs ( self . bucket_name , prefix = f \"staging/ { self . dataset_id } / { self . table_id } \" ) # Raise: Cannot create table without external data if not data : raise BaseDosDadosException ( \"You must provide a path for uploading data\" ) # Add data to storage if isinstance ( path , ( str , Path , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists , chunk_size = chunk_size , ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" , location = location , dataset_is_public = dataset_is_public ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , columns_config_url_or_path = columns_config_url_or_path , source_format = source_format , ) table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = Datatype ( self , source_format , \"staging\" , partitioned = self . _is_partitioned () ) . external_config # Lookup if table alreay exists table_ref = None try : table_ref = self . client [ \"bigquery_staging\" ] . get_table ( self . table_full_name [ \"staging\" ] ) except google . api_core . exceptions . NotFound : pass if isinstance ( table_ref , google . cloud . bigquery . table . Table ): if if_table_exists == \"pass\" : return None if if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"created\" , ) return None def update ( self , mode = \"all\" ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue # if m == \"staging\": table . description = self . _render_template ( Path ( \"table/table_description.txt\" ), self . table_config ) # save table description with open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , encoding = \"utf-8\" , ) as f : f . write ( table . description ) # when mode is staging the table schema already exists table . schema = self . _load_schema ( m ) fields = [ \"description\" , \"schema\" ] if m == \"prod\" else [ \"description\" ] self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = fields ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"updated\" , ) def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" , encoding = \"utf-8\" ) . read () ) . result () self . update () logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"published\" , ) def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) def append ( self , filepath , partitions = None , if_exists = \"replace\" , chunk_size = None , ** upload_args , ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if not self . table_exists ( \"staging\" ): raise BaseDosDadosException ( \"You cannot append to a table that does not exist\" ) Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = partitions , if_exists = if_exists , chunk_size = chunk_size , ** upload_args , ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"appended\" , ) table_config property readonly Load table_config.yaml append ( self , filepath , partitions = None , if_exists = 'replace' , chunk_size = None , ** upload_args ) Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Parameters: Name Type Description Default filepath str or pathlib.PosixPath Where to find the file that you want to upload to create a table with required partitions str, pathlib.PosixPath, dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str 0ptional. What to do if data with same name exists in storage 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'replace' chunk_size int Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. None Source code in basedosdados/upload/table.py def append ( self , filepath , partitions = None , if_exists = \"replace\" , chunk_size = None , ** upload_args , ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if not self . table_exists ( \"staging\" ): raise BaseDosDadosException ( \"You cannot append to a table that does not exist\" ) Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = partitions , if_exists = if_exists , chunk_size = chunk_size , ** upload_args , ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"appended\" , ) create ( self , path = None , force_dataset = True , if_table_exists = 'raise' , if_storage_data_exists = 'raise' , if_table_config_exists = 'raise' , source_format = 'csv' , columns_config_url_or_path = None , dataset_is_public = True , location = None , chunk_size = None ) Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at <dataset_id>_staging.<table_id> in BigQuery. It looks for data saved in Storage at <bucket_name>/staging/<dataset_id>/<table_id>/* and builds the table. It currently supports the types: Comma Delimited CSV Apache Avro Apache Parquet Data can also be partitioned following the hive partitioning scheme <key1>=<value1>/<key2>=<value2> - for instance, year=2012/country=BR . The partition is automatcally detected by searching for partitions on the table_config.yaml . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file that you want to upload to create a table with None job_config_params dict Optional. Job configuration params from bigquery required if_table_exists str Optional What to do if table exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' force_dataset bool Creates <dataset_id> folder and BigQuery Dataset if it doesn't exists. True if_table_config_exists str Optional. What to do if config files already exist 'raise': Raises FileExistError 'replace': Replace with blank template 'pass'; Do nothing 'raise' if_storage_data_exists str Optional. What to do if data already exists on your bucket: 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. 'csv' columns_config_url_or_path str Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None dataset_is_public bool Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True location str Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None chunk_size int Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. None Source code in basedosdados/upload/table.py def create ( self , path = None , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , dataset_is_public = True , location = None , chunk_size = None , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV - Apache Avro - Apache Parquet Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>` - for instance, `year=2012/country=BR`. The partition is automatcally detected by searching for `partitions` on the `table_config.yaml`. Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if path is None : # Look if table data already exists at Storage data = self . client [ \"storage_staging\" ] . list_blobs ( self . bucket_name , prefix = f \"staging/ { self . dataset_id } / { self . table_id } \" ) # Raise: Cannot create table without external data if not data : raise BaseDosDadosException ( \"You must provide a path for uploading data\" ) # Add data to storage if isinstance ( path , ( str , Path , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists , chunk_size = chunk_size , ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" , location = location , dataset_is_public = dataset_is_public ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , columns_config_url_or_path = columns_config_url_or_path , source_format = source_format , ) table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = Datatype ( self , source_format , \"staging\" , partitioned = self . _is_partitioned () ) . external_config # Lookup if table alreay exists table_ref = None try : table_ref = self . client [ \"bigquery_staging\" ] . get_table ( self . table_full_name [ \"staging\" ] ) except google . api_core . exceptions . NotFound : pass if isinstance ( table_ref , google . cloud . bigquery . table . Table ): if if_table_exists == \"pass\" : return None if if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"created\" , ) return None delete ( self , mode ) Deletes table in BigQuery. Parameters: Name Type Description Default mode str Table of which table to delete [prod|staging] required Source code in basedosdados/upload/table.py def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) init ( self , data_sample_path = None , if_folder_exists = 'raise' , if_table_config_exists = 'raise' , source_format = 'csv' , columns_config_url_or_path = None ) Initialize table folder at metadata_path at metadata_path/<dataset_id>/<table_id> . The folder should contain: table_config.yaml publish.sql You can also point to a sample of the data to auto complete columns names. Parameters: Name Type Description Default data_sample_path str, pathlib.PosixPath Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV, Apache Avro and Apache Parquet. None if_folder_exists str Optional. What to do if table folder exists 'raise' : Raises FileExistsError 'replace' : Replace folder 'pass' : Do nothing 'raise' if_table_config_exists str Optional What to do if table_config.yaml and publish.sql exists 'raise' : Raises FileExistsError 'replace' : Replace files with blank template 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. 'csv' columns_config_url_or_path str Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None Exceptions: Type Description FileExistsError If folder exists and replace is False. NotImplementedError If data sample is not in supported type or format. Source code in basedosdados/upload/table.py def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV, Apache Avro and Apache Parquet. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing if_table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError as e : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) from e if if_folder_exists == \"pass\" : return self if not data_sample_path and if_table_config_exists != \"pass\" : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) partition_columns = [] if isinstance ( data_sample_path , ( str , Path , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == f \". { source_format } \" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] columns = Datatype ( self , source_format ) . header ( data_sample_path ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : # Check if config files exists before passing if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass # Raise if no sample to determine columns elif not data_sample_path : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) else : self . _make_template ( columns , partition_columns , if_table_config_exists ) elif if_table_config_exists == \"raise\" : # Check if config files already exist if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) # if config files don't exist, create them self . _make_template ( columns , partition_columns , if_table_config_exists ) else : # Raise: without a path to data sample, should not replace config files with empty template self . _make_template ( columns , partition_columns , if_table_config_exists ) if columns_config_url_or_path is not None : self . update_columns ( columns_config_url_or_path ) return self publish ( self , if_exists = 'raise' ) Creates BigQuery table at production dataset. Table should be located at <dataset_id>.<table_id> . It creates a view that uses the query from <metadata_path>/<dataset_id>/<table_id>/publish.sql . Make sure that all columns from the query also exists at <metadata_path>/<dataset_id>/<table_id>/table_config.sql , including the partitions. Parameters: Name Type Description Default if_exists str Optional. What to do if table exists. 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Check if all required fields are filled Source code in basedosdados/upload/table.py def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" , encoding = \"utf-8\" ) . read () ) . result () self . update () logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"published\" , ) table_exists ( self , mode ) Check if table exists in BigQuery. Parameters: Name Type Description Default mode str Which dataset to check [prod|staging]. required Source code in basedosdados/upload/table.py def table_exists ( self , mode ): \"\"\"Check if table exists in BigQuery. Args: mode (str): Which dataset to check [prod|staging]. \"\"\" try : ref = self . _get_table_obj ( mode = mode ) except google . api_core . exceptions . NotFound : ref = None return bool ( ref ) update ( self , mode = 'all' ) Updates BigQuery schema and description. Parameters: Name Type Description Default mode str Optional. Table of which table to update [prod|staging|all] 'all' not_found_ok bool Optional. What to do if table is not found required Source code in basedosdados/upload/table.py def update ( self , mode = \"all\" ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue # if m == \"staging\": table . description = self . _render_template ( Path ( \"table/table_description.txt\" ), self . table_config ) # save table description with open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , encoding = \"utf-8\" , ) as f : f . write ( table . description ) # when mode is staging the table schema already exists table . schema = self . _load_schema ( m ) fields = [ \"description\" , \"schema\" ] if m == \"prod\" else [ \"description\" ] self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = fields ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"updated\" , ) update_columns ( self , columns_config_url_or_path = None ) Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format . : - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Parameters: Name Type Description Default columns_config_url_or_path str Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None Source code in basedosdados/upload/table.py def update_columns ( self , columns_config_url_or_path = None ): \"\"\" Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format <dataset_id>.<table_id>:<column_name> - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Args: columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. \"\"\" ruamel = ryaml . YAML () ruamel . preserve_quotes = True ruamel . indent ( mapping = 4 , sequence = 6 , offset = 4 ) table_config_yaml = ruamel . load ( ( self . table_folder / \"table_config.yaml\" ) . open ( encoding = \"utf-8\" ) ) if \"https://docs.google.com/spreadsheets/d/\" in columns_config_url_or_path : if ( \"edit#gid=\" not in columns_config_url_or_path or \"https://docs.google.com/spreadsheets/d/\" not in columns_config_url_or_path or not columns_config_url_or_path . split ( \"=\" )[ 1 ] . isdigit () ): raise BaseDosDadosException ( \"The Google sheet url not in correct format.\" \"The url must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>\" ) df = self . _sheet_to_df ( columns_config_url_or_path ) else : file_type = columns_config_url_or_path . split ( \".\" )[ - 1 ] if file_type == \"csv\" : df = pd . read_csv ( columns_config_url_or_path , encoding = \"utf-8\" ) elif file_type in [ \"xls\" , \"xlsx\" , \"xlsm\" , \"xlsb\" , \"odf\" , \"ods\" , \"odt\" ]: df = pd . read_excel ( columns_config_url_or_path ) else : raise BaseDosDadosException ( \"File not suported. Only csv, xls, xlsx, xlsm, xlsb, odf, ods, odt are supported.\" ) df = df . fillna ( \"NULL\" ) required_columns = [ \"name\" , \"bigquery_type\" , \"description\" , \"temporal_coverage\" , \"covered_by_dictionary\" , \"directory_column\" , \"measurement_unit\" , \"has_sensitive_data\" , \"observations\" , ] not_found_columns = required_columns . copy () for sheet_column in df . columns . tolist (): for required_column in required_columns : if sheet_column == required_column : not_found_columns . remove ( required_column ) if not_found_columns : raise BaseDosDadosException ( f \"The following required columns are not found: { ', ' . join ( not_found_columns ) } .\" ) columns_parameters = zip ( * [ df [ required_column ] . tolist () for required_column in required_columns ] ) for ( name , bigquery_type , description , temporal_coverage , covered_by_dictionary , directory_column , measurement_unit , has_sensitive_data , observations , ) in columns_parameters : for col in table_config_yaml [ \"columns\" ]: if col [ \"name\" ] == name : col [ \"bigquery_type\" ] = ( col [ \"bigquery_type\" ] if bigquery_type == \"NULL\" else bigquery_type . lower () ) col [ \"description\" ] = ( col [ \"description\" ] if description == \"NULL\" else description ) col [ \"temporal_coverage\" ] = ( col [ \"temporal_coverage\" ] if temporal_coverage == \"NULL\" else [ temporal_coverage ] ) col [ \"covered_by_dictionary\" ] = ( \"no\" if covered_by_dictionary == \"NULL\" else covered_by_dictionary ) dataset = directory_column . split ( \".\" )[ 0 ] col [ \"directory_column\" ][ \"dataset_id\" ] = ( col [ \"directory_column\" ][ \"dataset_id\" ] if dataset == \"NULL\" else dataset ) table = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ 0 ] col [ \"directory_column\" ][ \"table_id\" ] = ( col [ \"directory_column\" ][ \"table_id\" ] if table == \"NULL\" else table ) column = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ - 1 ] col [ \"directory_column\" ][ \"column_name\" ] = ( col [ \"directory_column\" ][ \"column_name\" ] if column == \"NULL\" else column ) col [ \"measurement_unit\" ] = ( col [ \"measurement_unit\" ] if measurement_unit == \"NULL\" else measurement_unit ) col [ \"has_sensitive_data\" ] = ( \"no\" if has_sensitive_data == \"NULL\" else has_sensitive_data ) col [ \"observations\" ] = ( col [ \"observations\" ] if observations == \"NULL\" else observations ) with open ( self . table_folder / \"table_config.yaml\" , \"w\" , encoding = \"utf-8\" ) as f : ruamel . dump ( table_config_yaml , f ) # regenerate publish.sql self . _make_publish_sql ()","title":"Python"},{"location":"api_reference_python/#python","text":"Esta API \u00e9 composta por fun\u00e7\u00f5es com 2 tipos de funcionalidade: M\u00f3dulos para requisi\u00e7\u00e3o de dados : para aquele(as) que desejam somente consultar os dados e metadados do nosso projeto (ou qualquer outro projeto no Google Cloud). Classes para gerenciamento de dados no Google Cloud: para aqueles(as) que desejam subir dados no nosso projeto (ou qualquer outro projeto no Google Cloud, seguindo a nossa metodologia e infraestrutura). Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"Python"},{"location":"api_reference_python/#modulos-requisicao-de-dados","text":"Functions for managing downloads","title":"M\u00f3dulos (Requisi\u00e7\u00e3o de dados)"},{"location":"api_reference_python/#basedosdados.download.download.download","text":"Download table or query result from basedosdados BigQuery (or other). Using a query : download('select * from basedosdados.br_suporte.diretorio_municipios limit 10') Using dataset_id & table_id : download(dataset_id='br_suporte', table_id='diretorio_municipios') You can also add arguments to modify save parameters: download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|') Parameters: Name Type Description Default savepath str, pathlib.PosixPath savepath must be a file path. Only supports .csv . required query str Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. None dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. None table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. None billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' limit int Optional Number of rows. None from_file boolean Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ False reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False compression str Optional. Compression type. Only GZIP is available for now. 'GZIP' Exceptions: Type Description Exception If either table_id, dataset_id or query are empty. Source code in basedosdados/download/download.py def download ( savepath , query = None , dataset_id = None , table_id = None , billing_project_id = None , query_project_id = \"basedosdados\" , limit = None , from_file = False , reauth = False , compression = \"GZIP\" , ): \"\"\"Download table or query result from basedosdados BigQuery (or other). * Using a **query**: `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')` * Using **dataset_id & table_id**: `download(dataset_id='br_suporte', table_id='diretorio_municipios')` You can also add arguments to modify save parameters: `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')` Args: savepath (str, pathlib.PosixPath): savepath must be a file path. Only supports `.csv`. query (str): Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required. dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. limit (int): Optional Number of rows. from_file (boolean): Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. compression (str): Optional. Compression type. Only `GZIP` is available for now. Raises: Exception: If either table_id, dataset_id or query are empty. \"\"\" billing_project_id , from_file = _set_config_variables ( billing_project_id = billing_project_id , from_file = from_file ) if ( query is None ) and (( table_id is None ) or ( dataset_id is None )): raise BaseDosDadosException ( \"Either table_id, dataset_id or query should be filled.\" ) client = google_client ( billing_project_id , from_file , reauth ) # makes sure that savepath is a filepath and not a folder savepath = _sets_savepath ( savepath ) # if query is not defined (so it won't be overwritten) and if # table is a view or external or if limit is specified, # convert it to a query. if not query and ( not _is_table ( client , dataset_id , table_id , query_project_id ) or limit ): query = f \"\"\" SELECT * FROM { query_project_id } . { dataset_id } . { table_id } \"\"\" if limit is not None : query += f \" limit { limit } \" if query : # sql queries produces anonymous tables, whose names # can be found within `job._properties` job = client [ \"bigquery\" ] . query ( query ) # views may take longer: wait for job to finish. _wait_for ( job ) dest_table = job . _properties [ \"configuration\" ][ \"query\" ][ \"destinationTable\" ] project_id = dest_table [ \"projectId\" ] dataset_id = dest_table [ \"datasetId\" ] table_id = dest_table [ \"tableId\" ] _direct_download ( client , dataset_id , table_id , savepath , project_id , compression )","title":"download()"},{"location":"api_reference_python/#basedosdados.download.download.read_sql","text":"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Parameters: Name Type Description Default query sql Valid SQL Standard Query to basedosdados required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None from_file boolean Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ False reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False use_bqstorage_api boolean Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_sql ( query , billing_project_id = None , from_file = False , reauth = False , use_bqstorage_api = False , ): \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq Args: query (sql): Valid SQL Standard Query to basedosdados billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard from_file (boolean): Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. use_bqstorage_api (boolean): Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. Returns: pd.DataFrame: Query result \"\"\" billing_project_id , from_file = _set_config_variables ( billing_project_id = billing_project_id , from_file = from_file ) try : # Set a two hours timeout bigquery_storage_v1 . client . BigQueryReadClient . read_rows = partialmethod ( bigquery_storage_v1 . client . BigQueryReadClient . read_rows , timeout = 3600 * 2 , ) return pandas_gbq . read_gbq ( query , credentials = credentials ( from_file = from_file , reauth = reauth ), project_id = billing_project_id , use_bqstorage_api = use_bqstorage_api , ) except GenericGBQException as e : if \"Reason: 403\" in str ( e ): raise BaseDosDadosAccessDeniedException from e if re . match ( \"Reason: 400 POST .* [Pp]roject[ ]*I[Dd]\" , str ( e )): raise BaseDosDadosInvalidProjectIDException from e raise except PyDataCredentialsError as e : raise BaseDosDadosAuthorizationException from e except ( OSError , ValueError ) as e : no_billing_id = \"Could not determine project ID\" in str ( e ) no_billing_id |= \"reading from stdin while output is captured\" in str ( e ) if no_billing_id : raise BaseDosDadosNoBillingProjectIDException from e raise","title":"read_sql()"},{"location":"api_reference_python/#basedosdados.download.download.read_table","text":"Load data from BigQuery using dataset_id and table_id. Parameters: Name Type Description Default dataset_id str Optional. Dataset id available in basedosdados. It should always come with table_id. required table_id str Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. required billing_project_id str Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard None query_project_id str Optional. Which project the table lives. You can change this you want to query different projects. 'basedosdados' limit int Optional. Number of rows to read from table. None from_file boolean Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ False reauth boolean Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. False use_bqstorage_api boolean Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. False Returns: Type Description pd.DataFrame Query result Source code in basedosdados/download/download.py def read_table ( dataset_id , table_id , billing_project_id = None , query_project_id = \"basedosdados\" , limit = None , from_file = False , reauth = False , use_bqstorage_api = False , ): \"\"\"Load data from BigQuery using dataset_id and table_id. Args: dataset_id (str): Optional. Dataset id available in basedosdados. It should always come with table_id. table_id (str): Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id. billing_project_id (str): Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard query_project_id (str): Optional. Which project the table lives. You can change this you want to query different projects. limit (int): Optional. Number of rows to read from table. from_file (boolean): Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/ reauth (boolean): Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations. use_bqstorage_api (boolean): Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to. Returns: pd.DataFrame: Query result \"\"\" billing_project_id , from_file = _set_config_variables ( billing_project_id = billing_project_id , from_file = from_file ) if ( dataset_id is not None ) and ( table_id is not None ): query = f \"\"\" SELECT * FROM ` { query_project_id } . { dataset_id } . { table_id } `\"\"\" if limit is not None : query += f \" LIMIT { limit } \" else : raise BaseDosDadosException ( \"Both table_id and dataset_id should be filled.\" ) return read_sql ( query , billing_project_id = billing_project_id , from_file = from_file , reauth = reauth , use_bqstorage_api = use_bqstorage_api , )","title":"read_table()"},{"location":"api_reference_python/#classes-gerenciamento-de-dados","text":"Class for managing the files in cloud storage.","title":"Classes (Gerenciamento de dados)"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage","text":"Manage files on Google Cloud Storage. Source code in basedosdados/upload/storage.py class Storage ( Base ): \"\"\" Manage files on Google Cloud Storage. \"\"\" def __init__ ( self , dataset_id = None , table_id = None , ** kwargs ): super () . __init__ ( ** kwargs ) self . bucket = self . client [ \"storage_staging\" ] . bucket ( self . bucket_name ) self . dataset_id = dataset_id . replace ( \"-\" , \"_\" ) self . table_id = table_id . replace ( \"-\" , \"_\" ) @staticmethod def _resolve_partitions ( partitions ): if isinstance ( partitions , dict ): return \"/\" . join ( f \" { k } = { v } \" for k , v in partitions . items ()) + \"/\" if isinstance ( partitions , str ): if partitions . endswith ( \"/\" ): partitions = partitions [: - 1 ] # If there is no partition if len ( partitions ) == 0 : return \"\" # It should fail if there is folder which is not a partition try : # check if it fits rule { b . split ( \"=\" )[ 0 ]: b . split ( \"=\" )[ 1 ] for b in partitions . split ( \"/\" )} except IndexError as e : raise Exception ( f \"The path { partitions } is not a valid partition\" ) from e return partitions + \"/\" raise Exception ( f \"Partitions format or type not accepted: { partitions } \" ) def _build_blob_name ( self , filename , mode , partitions = None ): ''' Builds the blob name. ''' # table folder blob_name = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # add partition folder if partitions is not None : blob_name += self . _resolve_partitions ( partitions ) # add file name blob_name += filename return blob_name def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" ) def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , chunk_size = None , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 6 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix in [ \".csv\" , \".parquet\" , \"parquet.gzip\" ] ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name , chunk_size = chunk_size ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists == \"pass\" : pass else : raise BaseDosDadosException ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"If you are using Storage.upload then set if_exists to \" \"'replace' to overwrite data \\n \" \"If you are using Table.create then set if_storage_data_exists \" \"to 'replace' to overwrite data.\" ) logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filepath . name , mode = mode , object = \"File\" , action = \"uploaded\" , ) def download ( self , filename = \"*\" , savepath = \"\" , partitions = None , mode = \"raw\" , if_not_exists = \"raise\" , ): \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy on save, There are 5 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables You can also use the `partitions` argument to choose files from a partition Args: filename (str): Optional Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\". savepath (str): Where you want to save the data on your computer. Must be a path to a directory. partitions (str, dict): Optional If downloading a single file, use this to specify the partition path from which to download. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` mode (str): Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture] if_not_exists (str): Optional. What to do if data not found. * 'raise' : Raises FileNotFoundError. * 'pass' : Do nothing and exit the function Raises: FileNotFoundError: If the given path `<mode>/<dataset_id>/<table_id>/<partitions>/<filename>` could not be found or there are no files to download. \"\"\" # Prefix to locate files within the bucket prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # Add specific partition to search prefix if partitions : prefix += self . _resolve_partitions ( partitions ) # if no filename is passed, list all blobs within a given table if filename != \"*\" : prefix += filename blob_list = list ( self . bucket . list_blobs ( prefix = prefix )) # if there are no blobs matching the search raise FileNotFoundError or return if not blob_list : if if_not_exists == \"raise\" : raise FileNotFoundError ( f \"Could not locate files at { prefix } \" ) return # download all blobs matching the search to given savepath for blob in tqdm ( blob_list , desc = \"Download Blob\" ): # parse blob.name and get the csv file name csv_name = blob . name . split ( \"/\" )[ - 1 ] # build folder path replicating storage hierarchy blob_folder = blob . name . replace ( csv_name , \"\" ) # replicate folder hierarchy ( Path ( savepath ) / blob_folder ) . mkdir ( parents = True , exist_ok = True ) # download blob to savepath blob . download_to_filename ( filename = f \" { savepath } / { blob . name } \" ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"File\" , action = \"downloaded\" , ) def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists () or not blob . exists () and not not_found_ok : blob . delete () else : return logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filename , mode = mode , object = \"File\" , action = \"deleted\" , ) def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. Args: mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = list ( self . client [ \"storage_staging\" ] . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) ) else : table_blobs = list ( self . bucket . list_blobs ( prefix = prefix )) if not table_blobs : if not_found_ok : return raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) # Divides table_blobs list for maximum batch request size table_blobs_chunks = [ table_blobs [ i : i + 999 ] for i in range ( 0 , len ( table_blobs ), 999 ) ] for i , source_table in enumerate ( tqdm ( table_blobs_chunks , desc = \"Delete Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : blob . delete () break except Exception : print ( f \"Delete Table Chunk { i } | Attempt { counter } : delete operation starts again in 5 seconds...\" , ) time . sleep ( 5 ) counter += 1 traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name (str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = list ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" ) ) if not source_table_ref : raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) # Divides source_table_ref list for maximum batch request size source_table_ref_chunks = [ source_table_ref [ i : i + 999 ] for i in range ( 0 , len ( source_table_ref ), 999 ) ] for i , source_table in enumerate ( tqdm ( source_table_ref_chunks , desc = \"Copy Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket , ) break except Exception : print ( f \"Copy Table Chunk { i } | Attempt { counter } : copy operation starts again in 5 seconds...\" , ) counter += 1 time . sleep ( 5 ) traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"copied\" , )","title":"Storage"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.copy_table","text":"Copies table from a source bucket to your bucket, sends request in batches. Parameters: Name Type Description Default source_bucket_name str The bucket name from which to copy data. You can change it to copy from other external bucket. 'basedosdados' destination_bucket_name str Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) None mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". 'staging' Source code in basedosdados/upload/storage.py def copy_table ( self , source_bucket_name = \"basedosdados\" , destination_bucket_name = None , mode = \"staging\" , ): \"\"\"Copies table from a source bucket to your bucket, sends request in batches. Args: source_bucket_name (str): The bucket name from which to copy data. You can change it to copy from other external bucket. destination_bucket_name (str): Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property) mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". \"\"\" source_table_ref = list ( self . client [ \"storage_staging\" ] . bucket ( source_bucket_name ) . list_blobs ( prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" ) ) if not source_table_ref : raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) if destination_bucket_name is None : destination_bucket = self . bucket else : destination_bucket = self . client [ \"storage_staging\" ] . bucket ( destination_bucket_name ) # Divides source_table_ref list for maximum batch request size source_table_ref_chunks = [ source_table_ref [ i : i + 999 ] for i in range ( 0 , len ( source_table_ref ), 999 ) ] for i , source_table in enumerate ( tqdm ( source_table_ref_chunks , desc = \"Copy Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : self . bucket . copy_blob ( blob , destination_bucket = destination_bucket , ) break except Exception : print ( f \"Copy Table Chunk { i } | Attempt { counter } : copy operation starts again in 5 seconds...\" , ) counter += 1 time . sleep ( 5 ) traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"copied\" , )","title":"copy_table()"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.delete_file","text":"Deletes file from path <bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename> . Parameters: Name Type Description Default filename str Name of the file to be deleted required mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] required partitions str, pathlib.PosixPath, or dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None not_found_ok bool Optional. What to do if file not found False Source code in basedosdados/upload/storage.py def delete_file ( self , filename , mode , partitions = None , not_found_ok = False ): \"\"\"Deletes file from path `<bucket_name>/<mode>/<dataset_id>/<table_id>/<partitions>/<filename>`. Args: filename (str): Name of the file to be deleted mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` not_found_ok (bool): Optional. What to do if file not found \"\"\" self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : blob = self . bucket . blob ( self . _build_blob_name ( filename , m , partitions )) if blob . exists () or not blob . exists () and not not_found_ok : blob . delete () else : return logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filename , mode = mode , object = \"File\" , action = \"deleted\" , )","title":"delete_file()"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.delete_table","text":"Deletes a table from storage, sends request in batches. Parameters: Name Type Description Default mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". 'staging' bucket_name str The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) None not_found_ok bool Optional. What to do if table not found False Source code in basedosdados/upload/storage.py def delete_table ( self , mode = \"staging\" , bucket_name = None , not_found_ok = False ): \"\"\"Deletes a table from storage, sends request in batches. Args: mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\". bucket_name (str): The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property) not_found_ok (bool): Optional. What to do if table not found \"\"\" prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" if bucket_name is not None : table_blobs = list ( self . client [ \"storage_staging\" ] . bucket ( f \" { bucket_name } \" ) . list_blobs ( prefix = prefix ) ) else : table_blobs = list ( self . bucket . list_blobs ( prefix = prefix )) if not table_blobs : if not_found_ok : return raise FileNotFoundError ( f \"Could not find the requested table { self . dataset_id } . { self . table_id } \" ) # Divides table_blobs list for maximum batch request size table_blobs_chunks = [ table_blobs [ i : i + 999 ] for i in range ( 0 , len ( table_blobs ), 999 ) ] for i , source_table in enumerate ( tqdm ( table_blobs_chunks , desc = \"Delete Table Chunk\" ) ): counter = 0 while counter < 10 : try : with self . client [ \"storage_staging\" ] . batch (): for blob in source_table : blob . delete () break except Exception : print ( f \"Delete Table Chunk { i } | Attempt { counter } : delete operation starts again in 5 seconds...\" , ) time . sleep ( 5 ) counter += 1 traceback . print_exc ( file = sys . stderr ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , )","title":"delete_table()"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.download","text":"Download files from Google Storage from path mode / dataset_id / table_id / partitions / filename and replicate folder hierarchy on save, There are 5 modes: * raw : should contain raw files from datasource * staging : should contain pre-treated files ready to upload to BiqQuery * header : should contain the header of the tables * auxiliary_files : should contain auxiliary files from eache table * architecture : should contain the architecture sheet of the tables You can also use the partitions argument to choose files from a partition Parameters: Name Type Description Default filename str Optional Specify which file to download. If \" \" , downloads all files within the bucket folder. Defaults to \" \". '*' savepath str Where you want to save the data on your computer. Must be a path to a directory. '' partitions str, dict Optional If downloading a single file, use this to specify the partition path from which to download. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None mode str Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture] 'raw' if_not_exists str Optional. What to do if data not found. 'raise' : Raises FileNotFoundError. 'pass' : Do nothing and exit the function 'raise' Exceptions: Type Description FileNotFoundError If the given path <mode>/<dataset_id>/<table_id>/<partitions>/<filename> could not be found or there are no files to download. Source code in basedosdados/upload/storage.py def download ( self , filename = \"*\" , savepath = \"\" , partitions = None , mode = \"raw\" , if_not_exists = \"raise\" , ): \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy on save, There are 5 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables You can also use the `partitions` argument to choose files from a partition Args: filename (str): Optional Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\". savepath (str): Where you want to save the data on your computer. Must be a path to a directory. partitions (str, dict): Optional If downloading a single file, use this to specify the partition path from which to download. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` mode (str): Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture] if_not_exists (str): Optional. What to do if data not found. * 'raise' : Raises FileNotFoundError. * 'pass' : Do nothing and exit the function Raises: FileNotFoundError: If the given path `<mode>/<dataset_id>/<table_id>/<partitions>/<filename>` could not be found or there are no files to download. \"\"\" # Prefix to locate files within the bucket prefix = f \" { mode } / { self . dataset_id } / { self . table_id } /\" # Add specific partition to search prefix if partitions : prefix += self . _resolve_partitions ( partitions ) # if no filename is passed, list all blobs within a given table if filename != \"*\" : prefix += filename blob_list = list ( self . bucket . list_blobs ( prefix = prefix )) # if there are no blobs matching the search raise FileNotFoundError or return if not blob_list : if if_not_exists == \"raise\" : raise FileNotFoundError ( f \"Could not locate files at { prefix } \" ) return # download all blobs matching the search to given savepath for blob in tqdm ( blob_list , desc = \"Download Blob\" ): # parse blob.name and get the csv file name csv_name = blob . name . split ( \"/\" )[ - 1 ] # build folder path replicating storage hierarchy blob_folder = blob . name . replace ( csv_name , \"\" ) # replicate folder hierarchy ( Path ( savepath ) / blob_folder ) . mkdir ( parents = True , exist_ok = True ) # download blob to savepath blob . download_to_filename ( filename = f \" { savepath } / { blob . name } \" ) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"File\" , action = \"downloaded\" , )","title":"download()"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.init","text":"Initializes bucket and folders. Folder should be: raw : that contains really raw data staging : preprocessed data ready to upload to BigQuery Parameters: Name Type Description Default replace bool Optional. Whether to replace if bucket already exists False very_sure bool Optional. Are you aware that everything is going to be erased if you replace the bucket? False Exceptions: Type Description Warning very_sure argument is still False. Source code in basedosdados/upload/storage.py def init ( self , replace = False , very_sure = False ): \"\"\"Initializes bucket and folders. Folder should be: * `raw` : that contains really raw data * `staging` : preprocessed data ready to upload to BigQuery Args: replace (bool): Optional. Whether to replace if bucket already exists very_sure (bool): Optional. Are you aware that everything is going to be erased if you replace the bucket? Raises: Warning: very_sure argument is still False. \"\"\" if replace : if not very_sure : raise Warning ( \" \\n ********************************************************\" \" \\n You are trying to replace all the data that you have \" f \"in bucket { self . bucket_name } . \\n Are you sure? \\n \" \"If yes, add the flag --very_sure \\n \" \"********************************************************\" ) self . bucket . delete ( force = True ) self . client [ \"storage_staging\" ] . create_bucket ( self . bucket ) for folder in [ \"staging/\" , \"raw/\" ]: self . bucket . blob ( folder ) . upload_from_string ( \"\" )","title":"init()"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.upload","text":"Upload to storage at <bucket_name>/<mode>/<dataset_id>/<table_id> . You can: Add a single file setting path = <file_path> . Add a folder with multiple files setting path = <folder_path> . The folder should just contain the files and no folders. Add partitioned files setting path = <folder_path> . This folder must follow the hive partitioning scheme i.e. <table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv (ex: mytable/country=brasil/year=2020/mypart.csv ). Remember all files must follow a single schema. Otherwise, things might fail in the future. There are 6 modes: raw : should contain raw files from datasource staging : should contain pre-treated files ready to upload to BiqQuery header : should contain the header of the tables auxiliary_files : should contain auxiliary files from eache table architecture : should contain the architecture sheet of the tables all : if no treatment is needed, use all . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file or folder that you want to upload to storage required mode str Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] 'all' partitions str, pathlib.PosixPath, or dict Optional. If adding a single file , use this to add it to a specific partition. str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str Optional. What to do if data exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' chunk_size int Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. None upload_args Extra arguments accepted by google.cloud.storage.blob.Blob.upload_from_file {} Source code in basedosdados/upload/storage.py def upload ( self , path , mode = \"all\" , partitions = None , if_exists = \"raise\" , chunk_size = None , ** upload_args , ): \"\"\"Upload to storage at `<bucket_name>/<mode>/<dataset_id>/<table_id>`. You can: * Add a single **file** setting `path = <file_path>`. * Add a **folder** with multiple files setting `path = <folder_path>`. *The folder should just contain the files and no folders.* * Add **partitioned files** setting `path = <folder_path>`. This folder must follow the hive partitioning scheme i.e. `<table_id>/<key>=<value>/<key2>=<value2>/<partition>.csv` (ex: `mytable/country=brasil/year=2020/mypart.csv`). *Remember all files must follow a single schema.* Otherwise, things might fail in the future. There are 6 modes: * `raw` : should contain raw files from datasource * `staging` : should contain pre-treated files ready to upload to BiqQuery * `header`: should contain the header of the tables * `auxiliary_files`: should contain auxiliary files from eache table * `architecture`: should contain the architecture sheet of the tables * `all`: if no treatment is needed, use `all`. Args: path (str or pathlib.PosixPath): Where to find the file or folder that you want to upload to storage mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all] partitions (str, pathlib.PosixPath, or dict): Optional. *If adding a single file*, use this to add it to a specific partition. * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): Optional. What to do if data exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. upload_args (): Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename) \"\"\" if ( self . dataset_id is None ) or ( self . table_id is None ): raise Exception ( \"You need to pass dataset_id and table_id\" ) path = Path ( path ) if path . is_dir (): paths = [ f for f in path . glob ( \"**/*\" ) if f . is_file () and f . suffix in [ \".csv\" , \".parquet\" , \"parquet.gzip\" ] ] parts = [ ( filepath . as_posix () . replace ( path . as_posix () + \"/\" , \"\" ) . replace ( str ( filepath . name ), \"\" ) ) for filepath in paths ] else : paths = [ path ] parts = [ partitions or None ] self . _check_mode ( mode ) mode = ( [ \"raw\" , \"staging\" , \"header\" , \"auxiliary_files\" , \"architecture\" ] if mode == \"all\" else [ mode ] ) for m in mode : for filepath , part in tqdm ( list ( zip ( paths , parts )), desc = \"Uploading files\" ): blob_name = self . _build_blob_name ( filepath . name , m , part ) blob = self . bucket . blob ( blob_name , chunk_size = chunk_size ) if not blob . exists () or if_exists == \"replace\" : upload_args [ \"timeout\" ] = upload_args . get ( \"timeout\" , None ) blob . upload_from_filename ( str ( filepath ), ** upload_args ) elif if_exists == \"pass\" : pass else : raise BaseDosDadosException ( f \"Data already exists at { self . bucket_name } / { blob_name } . \" \"If you are using Storage.upload then set if_exists to \" \"'replace' to overwrite data \\n \" \"If you are using Table.create then set if_storage_data_exists \" \"to 'replace' to overwrite data.\" ) logger . success ( \" {object} {filename} _ {mode} was {action} !\" , filename = filepath . name , mode = mode , object = \"File\" , action = \"uploaded\" , ) Module for manage dataset to the server.","title":"upload()"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset","text":"Manage datasets in BigQuery. Source code in basedosdados/upload/dataset.py class Dataset ( Base ): \"\"\" Manage datasets in BigQuery. \"\"\" def __init__ ( self , dataset_id , ** kwargs ): super () . __init__ ( ** kwargs ) self . dataset_id = dataset_id . replace ( \"-\" , \"_\" ) self . dataset_folder = Path ( self . metadata_path / self . dataset_id ) self . metadata = Metadata ( self . dataset_id , ** kwargs ) @property def dataset_config ( self ): \"\"\" Dataset config file. \"\"\" return self . _load_yaml ( self . metadata_path / self . dataset_id / \"dataset_config.yaml\" ) def _loop_modes ( self , mode = \"all\" ): \"\"\" Loop modes. \"\"\" mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] dataset_tag = lambda m : f \"_ { m } \" if m == \"staging\" else \"\" return ( { \"client\" : self . client [ f \"bigquery_ { m } \" ], \"id\" : f \" { self . client [ f 'bigquery_ { m } ' ] . project } . { self . dataset_id }{ dataset_tag ( m ) } \" , } for m in mode ) @staticmethod def _setup_dataset_object ( dataset_id , location = None ): \"\"\" Setup dataset object. \"\"\" dataset = bigquery . Dataset ( dataset_id ) ## TODO: not being used since 1.6.0 - need to redo the description tha goes to bigquery dataset . description = \"Para saber mais acesse https://basedosdados.org/\" # dataset.description = self._render_template( # Path(\"dataset/dataset_description.txt\"), self.dataset_config # ) dataset . location = location return dataset def _write_readme_file ( self ): \"\"\" Write README.md file. \"\"\" readme_content = ( f \"Como capturar os dados de { self . dataset_id } ? \\n\\n Para cap\" f \"turar esses dados, basta verificar o link dos dados orig\" f \"inais indicado em dataset_config.yaml no item website. \\n \" f \" \\n Caso tenha sido utilizado algum c\u00f3digo de captura ou t\" f \"ratamento, estes estar\u00e3o contidos em code/. Se o dado pu\" f \"blicado for em sua vers\u00e3o bruta, n\u00e3o existir\u00e1 a pasta co\" f \"de/. \\n\\n Os dados publicados est\u00e3o dispon\u00edveis em: https:\" f \"//basedosdados.org/dataset/ { self . dataset_id . replace ( '_' , '-' ) } \" ) readme_path = Path ( self . metadata_path / self . dataset_id / \"README.md\" ) with open ( readme_path , \"w\" , encoding = \"utf-8\" ) as readmefile : readmefile . write ( readme_content ) def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError as e : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) from e # create dataset_config.yaml with metadata self . metadata . create ( if_exists = \"replace\" ) # create README.md file self . _write_readme_file () # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self def publicize ( self , mode = \"all\" , dataset_is_public = True ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries # TODO https://github.com/basedosdados/mais/pull/1020 # TODO if staging dataset is private, the prod view can't acess it: if dataset_is_public and \"staging\" not in dataset.dataset_id: if dataset_is_public : if \"staging\" not in dataset . dataset_id : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) else : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ]) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"publicized\" , ) def create ( self , mode = \"all\" , if_exists = \"raise\" , dataset_is_public = True , location = None ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflict exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ], location = location ) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"created\" , ) except Conflict as e : if if_exists == \"pass\" : return raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) from e # Make prod dataset public self . publicize ( dataset_is_public = dataset_is_public ) def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"deleted\" , ) def update ( self , mode = \"all\" , location = None ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ], location = location , ), fields = [ \"description\" ], ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"updated\" , )","title":"Dataset"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.dataset_config","text":"Dataset config file.","title":"dataset_config"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.create","text":"Creates BigQuery datasets given dataset_id . It can create two datasets: <dataset_id> (mode = 'prod') <dataset_id>_staging (mode = 'staging') If mode is all, it creates both. Parameters: Name Type Description Default mode str Optional. Which dataset to create [prod|staging|all]. 'all' if_exists str Optional. What to do if dataset exists raise : Raises Conflict exception replace : Drop all tables and replace dataset update : Update dataset description pass : Do nothing 'raise' dataset_is_public bool Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True location str Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None Exceptions: Type Description Warning Dataset already exists and if_exists is set to raise Source code in basedosdados/upload/dataset.py def create ( self , mode = \"all\" , if_exists = \"raise\" , dataset_is_public = True , location = None ): \"\"\"Creates BigQuery datasets given `dataset_id`. It can create two datasets: * `<dataset_id>` (mode = 'prod') * `<dataset_id>_staging` (mode = 'staging') If `mode` is all, it creates both. Args: mode (str): Optional. Which dataset to create [prod|staging|all]. if_exists (str): Optional. What to do if dataset exists * raise : Raises Conflict exception * replace : Drop all tables and replace dataset * update : Update dataset description * pass : Do nothing dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations Raises: Warning: Dataset already exists and if_exists is set to `raise` \"\"\" if if_exists == \"replace\" : self . delete ( mode ) elif if_exists == \"update\" : self . update () return # Set dataset_id to the ID of the dataset to create. for m in self . _loop_modes ( mode ): # Construct a full Dataset object to send to the API. dataset_obj = self . _setup_dataset_object ( m [ \"id\" ], location = location ) # Send the dataset to the API for creation, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. try : m [ \"client\" ] . create_dataset ( dataset_obj ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"created\" , ) except Conflict as e : if if_exists == \"pass\" : return raise Conflict ( f \"Dataset { self . dataset_id } already exists\" ) from e # Make prod dataset public self . publicize ( dataset_is_public = dataset_is_public )","title":"create()"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.delete","text":"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Parameters: Name Type Description Default mode str Optional. Which dataset to delete [prod|staging|all] 'all' Source code in basedosdados/upload/dataset.py def delete ( self , mode = \"all\" ): \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete. Args: mode (str): Optional. Which dataset to delete [prod|staging|all] \"\"\" for m in self . _loop_modes ( mode ): m [ \"client\" ] . delete_dataset ( m [ \"id\" ], delete_contents = True , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"deleted\" , )","title":"delete()"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.init","text":"Initialize dataset folder at metadata_path at metadata_path/<dataset_id> . The folder should contain: dataset_config.yaml README.md Parameters: Name Type Description Default replace str Optional. Whether to replace existing folder. False Exceptions: Type Description FileExistsError If dataset folder already exists and replace is False Source code in basedosdados/upload/dataset.py def init ( self , replace = False ): \"\"\"Initialize dataset folder at metadata_path at `metadata_path/<dataset_id>`. The folder should contain: * `dataset_config.yaml` * `README.md` Args: replace (str): Optional. Whether to replace existing folder. Raises: FileExistsError: If dataset folder already exists and replace is False \"\"\" # Create dataset folder try : self . dataset_folder . mkdir ( exist_ok = replace , parents = True ) except FileExistsError as e : raise FileExistsError ( f \"Dataset { str ( self . dataset_folder . stem ) } folder does not exists. \" \"Set replace=True to replace current files.\" ) from e # create dataset_config.yaml with metadata self . metadata . create ( if_exists = \"replace\" ) # create README.md file self . _write_readme_file () # Add code folder ( self . dataset_folder / \"code\" ) . mkdir ( exist_ok = replace , parents = True ) return self","title":"init()"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.publicize","text":"Changes IAM configuration to turn BigQuery dataset public. Parameters: Name Type Description Default mode bool Which dataset to create [prod|staging|all]. 'all' dataset_is_public bool Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True Source code in basedosdados/upload/dataset.py def publicize ( self , mode = \"all\" , dataset_is_public = True ): \"\"\"Changes IAM configuration to turn BigQuery dataset public. Args: mode (bool): Which dataset to create [prod|staging|all]. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. \"\"\" for m in self . _loop_modes ( mode ): dataset = m [ \"client\" ] . get_dataset ( m [ \"id\" ]) entries = dataset . access_entries # TODO https://github.com/basedosdados/mais/pull/1020 # TODO if staging dataset is private, the prod view can't acess it: if dataset_is_public and \"staging\" not in dataset.dataset_id: if dataset_is_public : if \"staging\" not in dataset . dataset_id : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.metadataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), bigquery . AccessEntry ( role = \"roles/bigquery.user\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) else : entries . extend ( [ bigquery . AccessEntry ( role = \"roles/bigquery.dataViewer\" , entity_type = \"iamMember\" , entity_id = \"allUsers\" , ), ] ) dataset . access_entries = entries m [ \"client\" ] . update_dataset ( dataset , [ \"access_entries\" ]) logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"publicized\" , )","title":"publicize()"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.update","text":"Update dataset description. Toogle mode to choose which dataset to update. Parameters: Name Type Description Default mode str Optional. Which dataset to update [prod|staging|all] 'all' location str Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None Source code in basedosdados/upload/dataset.py def update ( self , mode = \"all\" , location = None ): \"\"\"Update dataset description. Toogle mode to choose which dataset to update. Args: mode (str): Optional. Which dataset to update [prod|staging|all] location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations \"\"\" for m in self . _loop_modes ( mode ): # Send the dataset to the API to update, with an explicit timeout. # Raises google.api_core.exceptions.Conflict if the Dataset already # exists within the project. m [ \"client\" ] . update_dataset ( self . _setup_dataset_object ( m [ \"id\" ], location = location , ), fields = [ \"description\" ], ) # Make an API request. logger . success ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . dataset_id , mode = mode , object = \"Dataset\" , action = \"updated\" , ) Class for manage tables in Storage and Big Query","title":"update()"},{"location":"api_reference_python/#basedosdados.upload.table.Table","text":"Manage tables in Google Cloud Storage and BigQuery. Source code in basedosdados/upload/table.py class Table ( Base ): \"\"\" Manage tables in Google Cloud Storage and BigQuery. \"\"\" def __init__ ( self , dataset_id , table_id , ** kwargs ): super () . __init__ ( ** kwargs ) self . table_id = table_id . replace ( \"-\" , \"_\" ) self . dataset_id = dataset_id . replace ( \"-\" , \"_\" ) self . dataset_folder = Path ( self . metadata_path / self . dataset_id ) self . table_folder = self . dataset_folder / table_id self . table_full_name = dict ( prod = f \" { self . client [ 'bigquery_prod' ] . project } . { self . dataset_id } . { self . table_id } \" , staging = f \" { self . client [ 'bigquery_staging' ] . project } . { self . dataset_id } _staging. { self . table_id } \" , ) self . table_full_name . update ( dict ( all = deepcopy ( self . table_full_name ))) self . metadata = Metadata ( self . dataset_id , self . table_id , ** kwargs ) @property def table_config ( self ): \"\"\" Load table_config.yaml \"\"\" return self . _load_yaml ( self . table_folder / \"table_config.yaml\" ) def _get_table_obj ( self , mode ): \"\"\" Get table object from BigQuery \"\"\" return self . client [ f \"bigquery_ { mode } \" ] . get_table ( self . table_full_name [ mode ]) def _is_partitioned ( self ): \"\"\" Check if table is partitioned \"\"\" ## check if the table are partitioned, need the split because of a change in the type of partitions in pydantic partitions = self . table_config [ \"partitions\" ] if partitions is None or len ( partitions ) == 0 : return False if isinstance ( partitions , list ): # check if any None inside list. # False if it is the case Ex: [None, 'partition'] # True otherwise Ex: ['partition1', 'partition2'] return all ( item is not None for item in partitions ) raise ValueError ( \"Partitions must be a list or None\" ) def _load_schema ( self , mode = \"staging\" ): \"\"\"Load schema from table_config.yaml Args: mode (bool): Which dataset to create [prod|staging]. \"\"\" self . _check_mode ( mode ) json_path = self . table_folder / f \"schema- { mode } .json\" columns = self . table_config [ \"columns\" ] if mode == \"staging\" : new_columns = [] for c in columns : # case is_in_staging are None then must be True is_in_staging = ( True if c . get ( \"is_in_staging\" ) is None else c [ \"is_in_staging\" ] ) # append columns declared in table_config.yaml to schema only if is_in_staging: True if is_in_staging and not c . get ( \"is_partition\" ): c [ \"type\" ] = \"STRING\" new_columns . append ( c ) del columns columns = new_columns elif mode == \"prod\" : schema = self . _get_table_obj ( mode ) . schema # get field names for fields at schema and at table_config.yaml column_names = [ c [ \"name\" ] for c in columns ] schema_names = [ s . name for s in schema ] # check if there are mismatched fields not_in_columns = [ name for name in schema_names if name not in column_names ] not_in_schema = [ name for name in column_names if name not in schema_names ] # raise if field is not in table_config if not_in_columns : raise BaseDosDadosException ( \"Column {error_columns} was not found in table_config.yaml. Are you sure that \" \"all your column names between table_config.yaml, publish.sql and \" \" {project_id} . {dataset_id} . {table_id} are the same?\" . format ( error_columns = not_in_columns , project_id = self . table_config [ \"project_id_prod\" ], dataset_id = self . table_config [ \"dataset_id\" ], table_id = self . table_config [ \"table_id\" ], ) ) # raise if field is not in schema if not_in_schema : raise BaseDosDadosException ( \"Column {error_columns} was not found in publish.sql. Are you sure that \" \"all your column names between table_config.yaml, publish.sql and \" \" {project_id} . {dataset_id} . {table_id} are the same?\" . format ( error_columns = not_in_schema , project_id = self . table_config [ \"project_id_prod\" ], dataset_id = self . table_config [ \"dataset_id\" ], table_id = self . table_config [ \"table_id\" ], ) ) # if field is in schema, get field_type and field_mode for c in columns : for s in schema : if c [ \"name\" ] == s . name : c [ \"type\" ] = s . field_type c [ \"mode\" ] = s . mode break ## force utf-8, write schema_{mode}.json json . dump ( columns , ( json_path ) . open ( \"w\" , encoding = \"utf-8\" )) # load new created schema return self . client [ f \"bigquery_ { mode } \" ] . schema_from_json ( str ( json_path )) def _make_publish_sql ( self ): \"\"\"Create publish.sql with columns and bigquery_type\"\"\" ### publish.sql header and instructions publish_txt = \"\"\" /* Query para publicar a tabela. Esse \u00e9 o lugar para: - modificar nomes, ordem e tipos de colunas - dar join com outras tabelas - criar colunas extras (e.g. logs, propor\u00e7\u00f5es, etc.) Qualquer coluna definida aqui deve tamb\u00e9m existir em `table_config.yaml`. # Al\u00e9m disso, sinta-se \u00e0 vontade para alterar alguns nomes obscuros # para algo um pouco mais expl\u00edcito. TIPOS: - Para modificar tipos de colunas, basta substituir STRING por outro tipo v\u00e1lido. - Exemplo: `SAFE_CAST(column_name AS NUMERIC) column_name` - Mais detalhes: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types */ \"\"\" # remove triple quotes extra space publish_txt = inspect . cleandoc ( publish_txt ) publish_txt = textwrap . dedent ( publish_txt ) # add create table statement project_id_prod = self . client [ \"bigquery_prod\" ] . project publish_txt += f \" \\n\\n CREATE VIEW { project_id_prod } . { self . dataset_id } . { self . table_id } AS \\n SELECT \\n \" # sort columns by is_partition, partitions_columns come first if self . _is_partitioned (): columns = sorted ( self . table_config [ \"columns\" ], key = lambda k : ( k [ \"is_partition\" ] is not None , k [ \"is_partition\" ]), reverse = True , ) else : columns = self . table_config [ \"columns\" ] # add columns in publish.sql for col in columns : name = col [ \"name\" ] bigquery_type = ( \"STRING\" if col [ \"bigquery_type\" ] is None else col [ \"bigquery_type\" ] . upper () ) publish_txt += f \"SAFE_CAST( { name } AS { bigquery_type } ) { name } , \\n \" ## remove last comma publish_txt = publish_txt [: - 2 ] + \" \\n \" # add from statement project_id_staging = self . client [ \"bigquery_staging\" ] . project publish_txt += ( f \"FROM { project_id_staging } . { self . dataset_id } _staging. { self . table_id } AS t\" ) # save publish.sql in table_folder ( self . table_folder / \"publish.sql\" ) . open ( \"w\" , encoding = \"utf-8\" ) . write ( publish_txt ) def _make_template ( self , columns , partition_columns , if_table_config_exists ): # create table_config.yaml with metadata self . metadata . create ( if_exists = if_table_config_exists , columns = partition_columns + columns , partition_columns = partition_columns , table_only = False , ) self . _make_publish_sql () @staticmethod def _sheet_to_df ( columns_config_url_or_path ): \"\"\" Convert sheet to dataframe \"\"\" url = columns_config_url_or_path . replace ( \"edit#gid=\" , \"export?format=csv&gid=\" ) try : return pd . read_csv ( StringIO ( requests . get ( url , timeout = 10 ) . content . decode ( \"utf-8\" ))) except Exception as e : raise BaseDosDadosException ( \"Check if your google sheet Share are: Anyone on the internet with this link can view\" ) from e def table_exists ( self , mode ): \"\"\"Check if table exists in BigQuery. Args: mode (str): Which dataset to check [prod|staging]. \"\"\" try : ref = self . _get_table_obj ( mode = mode ) except google . api_core . exceptions . NotFound : ref = None return bool ( ref ) def update_columns ( self , columns_config_url_or_path = None ): \"\"\" Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format <dataset_id>.<table_id>:<column_name> - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Args: columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. \"\"\" ruamel = ryaml . YAML () ruamel . preserve_quotes = True ruamel . indent ( mapping = 4 , sequence = 6 , offset = 4 ) table_config_yaml = ruamel . load ( ( self . table_folder / \"table_config.yaml\" ) . open ( encoding = \"utf-8\" ) ) if \"https://docs.google.com/spreadsheets/d/\" in columns_config_url_or_path : if ( \"edit#gid=\" not in columns_config_url_or_path or \"https://docs.google.com/spreadsheets/d/\" not in columns_config_url_or_path or not columns_config_url_or_path . split ( \"=\" )[ 1 ] . isdigit () ): raise BaseDosDadosException ( \"The Google sheet url not in correct format.\" \"The url must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>\" ) df = self . _sheet_to_df ( columns_config_url_or_path ) else : file_type = columns_config_url_or_path . split ( \".\" )[ - 1 ] if file_type == \"csv\" : df = pd . read_csv ( columns_config_url_or_path , encoding = \"utf-8\" ) elif file_type in [ \"xls\" , \"xlsx\" , \"xlsm\" , \"xlsb\" , \"odf\" , \"ods\" , \"odt\" ]: df = pd . read_excel ( columns_config_url_or_path ) else : raise BaseDosDadosException ( \"File not suported. Only csv, xls, xlsx, xlsm, xlsb, odf, ods, odt are supported.\" ) df = df . fillna ( \"NULL\" ) required_columns = [ \"name\" , \"bigquery_type\" , \"description\" , \"temporal_coverage\" , \"covered_by_dictionary\" , \"directory_column\" , \"measurement_unit\" , \"has_sensitive_data\" , \"observations\" , ] not_found_columns = required_columns . copy () for sheet_column in df . columns . tolist (): for required_column in required_columns : if sheet_column == required_column : not_found_columns . remove ( required_column ) if not_found_columns : raise BaseDosDadosException ( f \"The following required columns are not found: { ', ' . join ( not_found_columns ) } .\" ) columns_parameters = zip ( * [ df [ required_column ] . tolist () for required_column in required_columns ] ) for ( name , bigquery_type , description , temporal_coverage , covered_by_dictionary , directory_column , measurement_unit , has_sensitive_data , observations , ) in columns_parameters : for col in table_config_yaml [ \"columns\" ]: if col [ \"name\" ] == name : col [ \"bigquery_type\" ] = ( col [ \"bigquery_type\" ] if bigquery_type == \"NULL\" else bigquery_type . lower () ) col [ \"description\" ] = ( col [ \"description\" ] if description == \"NULL\" else description ) col [ \"temporal_coverage\" ] = ( col [ \"temporal_coverage\" ] if temporal_coverage == \"NULL\" else [ temporal_coverage ] ) col [ \"covered_by_dictionary\" ] = ( \"no\" if covered_by_dictionary == \"NULL\" else covered_by_dictionary ) dataset = directory_column . split ( \".\" )[ 0 ] col [ \"directory_column\" ][ \"dataset_id\" ] = ( col [ \"directory_column\" ][ \"dataset_id\" ] if dataset == \"NULL\" else dataset ) table = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ 0 ] col [ \"directory_column\" ][ \"table_id\" ] = ( col [ \"directory_column\" ][ \"table_id\" ] if table == \"NULL\" else table ) column = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ - 1 ] col [ \"directory_column\" ][ \"column_name\" ] = ( col [ \"directory_column\" ][ \"column_name\" ] if column == \"NULL\" else column ) col [ \"measurement_unit\" ] = ( col [ \"measurement_unit\" ] if measurement_unit == \"NULL\" else measurement_unit ) col [ \"has_sensitive_data\" ] = ( \"no\" if has_sensitive_data == \"NULL\" else has_sensitive_data ) col [ \"observations\" ] = ( col [ \"observations\" ] if observations == \"NULL\" else observations ) with open ( self . table_folder / \"table_config.yaml\" , \"w\" , encoding = \"utf-8\" ) as f : ruamel . dump ( table_config_yaml , f ) # regenerate publish.sql self . _make_publish_sql () def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV, Apache Avro and Apache Parquet. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing if_table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError as e : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) from e if if_folder_exists == \"pass\" : return self if not data_sample_path and if_table_config_exists != \"pass\" : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) partition_columns = [] if isinstance ( data_sample_path , ( str , Path , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == f \". { source_format } \" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] columns = Datatype ( self , source_format ) . header ( data_sample_path ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : # Check if config files exists before passing if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass # Raise if no sample to determine columns elif not data_sample_path : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) else : self . _make_template ( columns , partition_columns , if_table_config_exists ) elif if_table_config_exists == \"raise\" : # Check if config files already exist if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) # if config files don't exist, create them self . _make_template ( columns , partition_columns , if_table_config_exists ) else : # Raise: without a path to data sample, should not replace config files with empty template self . _make_template ( columns , partition_columns , if_table_config_exists ) if columns_config_url_or_path is not None : self . update_columns ( columns_config_url_or_path ) return self def create ( self , path = None , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , dataset_is_public = True , location = None , chunk_size = None , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV - Apache Avro - Apache Parquet Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>` - for instance, `year=2012/country=BR`. The partition is automatcally detected by searching for `partitions` on the `table_config.yaml`. Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if path is None : # Look if table data already exists at Storage data = self . client [ \"storage_staging\" ] . list_blobs ( self . bucket_name , prefix = f \"staging/ { self . dataset_id } / { self . table_id } \" ) # Raise: Cannot create table without external data if not data : raise BaseDosDadosException ( \"You must provide a path for uploading data\" ) # Add data to storage if isinstance ( path , ( str , Path , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists , chunk_size = chunk_size , ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" , location = location , dataset_is_public = dataset_is_public ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , columns_config_url_or_path = columns_config_url_or_path , source_format = source_format , ) table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = Datatype ( self , source_format , \"staging\" , partitioned = self . _is_partitioned () ) . external_config # Lookup if table alreay exists table_ref = None try : table_ref = self . client [ \"bigquery_staging\" ] . get_table ( self . table_full_name [ \"staging\" ] ) except google . api_core . exceptions . NotFound : pass if isinstance ( table_ref , google . cloud . bigquery . table . Table ): if if_table_exists == \"pass\" : return None if if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"created\" , ) return None def update ( self , mode = \"all\" ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue # if m == \"staging\": table . description = self . _render_template ( Path ( \"table/table_description.txt\" ), self . table_config ) # save table description with open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , encoding = \"utf-8\" , ) as f : f . write ( table . description ) # when mode is staging the table schema already exists table . schema = self . _load_schema ( m ) fields = [ \"description\" , \"schema\" ] if m == \"prod\" else [ \"description\" ] self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = fields ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"updated\" , ) def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" , encoding = \"utf-8\" ) . read () ) . result () self . update () logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"published\" , ) def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) def append ( self , filepath , partitions = None , if_exists = \"replace\" , chunk_size = None , ** upload_args , ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if not self . table_exists ( \"staging\" ): raise BaseDosDadosException ( \"You cannot append to a table that does not exist\" ) Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = partitions , if_exists = if_exists , chunk_size = chunk_size , ** upload_args , ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"appended\" , )","title":"Table"},{"location":"api_reference_python/#basedosdados.upload.table.Table.table_config","text":"Load table_config.yaml","title":"table_config"},{"location":"api_reference_python/#basedosdados.upload.table.Table.append","text":"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Parameters: Name Type Description Default filepath str or pathlib.PosixPath Where to find the file that you want to upload to create a table with required partitions str, pathlib.PosixPath, dict Optional. Hive structured partition as a string or dict str : <key>=<value>/<key2>=<value2> dict: dict(key=value, key2=value2) None if_exists str 0ptional. What to do if data with same name exists in storage 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'replace' chunk_size int Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. None Source code in basedosdados/upload/table.py def append ( self , filepath , partitions = None , if_exists = \"replace\" , chunk_size = None , ** upload_args , ): \"\"\"Appends new data to existing BigQuery table. As long as the data has the same schema. It appends the data in the filepath to the existing table. Args: filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with partitions (str, pathlib.PosixPath, dict): Optional. Hive structured partition as a string or dict * str : `<key>=<value>/<key2>=<value2>` * dict: `dict(key=value, key2=value2)` if_exists (str): 0ptional. What to do if data with same name exists in storage * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if not self . table_exists ( \"staging\" ): raise BaseDosDadosException ( \"You cannot append to a table that does not exist\" ) Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( filepath , mode = \"staging\" , partitions = partitions , if_exists = if_exists , chunk_size = chunk_size , ** upload_args , ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"appended\" , )","title":"append()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.create","text":"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at <dataset_id>_staging.<table_id> in BigQuery. It looks for data saved in Storage at <bucket_name>/staging/<dataset_id>/<table_id>/* and builds the table. It currently supports the types: Comma Delimited CSV Apache Avro Apache Parquet Data can also be partitioned following the hive partitioning scheme <key1>=<value1>/<key2>=<value2> - for instance, year=2012/country=BR . The partition is automatcally detected by searching for partitions on the table_config.yaml . Parameters: Name Type Description Default path str or pathlib.PosixPath Where to find the file that you want to upload to create a table with None job_config_params dict Optional. Job configuration params from bigquery required if_table_exists str Optional What to do if table exists 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' force_dataset bool Creates <dataset_id> folder and BigQuery Dataset if it doesn't exists. True if_table_config_exists str Optional. What to do if config files already exist 'raise': Raises FileExistError 'replace': Replace with blank template 'pass'; Do nothing 'raise' if_storage_data_exists str Optional. What to do if data already exists on your bucket: 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. 'csv' columns_config_url_or_path str Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None dataset_is_public bool Control if prod dataset is public or not. By default staging datasets like dataset_id_staging are not public. True location str Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations None chunk_size int Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. None Source code in basedosdados/upload/table.py def create ( self , path = None , force_dataset = True , if_table_exists = \"raise\" , if_storage_data_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , dataset_is_public = True , location = None , chunk_size = None , ): \"\"\"Creates BigQuery table at staging dataset. If you add a path, it automatically saves the data in the storage, creates a datasets folder and BigQuery location, besides creating the table and its configuration files. The new table should be located at `<dataset_id>_staging.<table_id>` in BigQuery. It looks for data saved in Storage at `<bucket_name>/staging/<dataset_id>/<table_id>/*` and builds the table. It currently supports the types: - Comma Delimited CSV - Apache Avro - Apache Parquet Data can also be partitioned following the hive partitioning scheme `<key1>=<value1>/<key2>=<value2>` - for instance, `year=2012/country=BR`. The partition is automatcally detected by searching for `partitions` on the `table_config.yaml`. Args: path (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with job_config_params (dict): Optional. Job configuration params from bigquery if_table_exists (str): Optional What to do if table exists * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing force_dataset (bool): Creates `<dataset_id>` folder and BigQuery Dataset if it doesn't exists. if_table_config_exists (str): Optional. What to do if config files already exist * 'raise': Raises FileExistError * 'replace': Replace with blank template * 'pass'; Do nothing if_storage_data_exists (str): Optional. What to do if data already exists on your bucket: * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public. location (str): Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations chunk_size (int): Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used. \"\"\" if path is None : # Look if table data already exists at Storage data = self . client [ \"storage_staging\" ] . list_blobs ( self . bucket_name , prefix = f \"staging/ { self . dataset_id } / { self . table_id } \" ) # Raise: Cannot create table without external data if not data : raise BaseDosDadosException ( \"You must provide a path for uploading data\" ) # Add data to storage if isinstance ( path , ( str , Path , ), ): Storage ( self . dataset_id , self . table_id , ** self . main_vars ) . upload ( path , mode = \"staging\" , if_exists = if_storage_data_exists , chunk_size = chunk_size , ) # Create Dataset if it doesn't exist if force_dataset : dataset_obj = Dataset ( self . dataset_id , ** self . main_vars ) try : dataset_obj . init () except FileExistsError : pass dataset_obj . create ( if_exists = \"pass\" , location = location , dataset_is_public = dataset_is_public ) self . init ( data_sample_path = path , if_folder_exists = \"replace\" , if_table_config_exists = if_table_config_exists , columns_config_url_or_path = columns_config_url_or_path , source_format = source_format , ) table = bigquery . Table ( self . table_full_name [ \"staging\" ]) table . external_data_configuration = Datatype ( self , source_format , \"staging\" , partitioned = self . _is_partitioned () ) . external_config # Lookup if table alreay exists table_ref = None try : table_ref = self . client [ \"bigquery_staging\" ] . get_table ( self . table_full_name [ \"staging\" ] ) except google . api_core . exceptions . NotFound : pass if isinstance ( table_ref , google . cloud . bigquery . table . Table ): if if_table_exists == \"pass\" : return None if if_table_exists == \"raise\" : raise FileExistsError ( \"Table already exists, choose replace if you want to overwrite it\" ) if if_table_exists == \"replace\" : self . delete ( mode = \"staging\" ) self . client [ \"bigquery_staging\" ] . create_table ( table ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"created\" , ) return None","title":"create()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.delete","text":"Deletes table in BigQuery. Parameters: Name Type Description Default mode str Table of which table to delete [prod|staging] required Source code in basedosdados/upload/table.py def delete ( self , mode ): \"\"\"Deletes table in BigQuery. Args: mode (str): Table of which table to delete [prod|staging] \"\"\" self . _check_mode ( mode ) if mode == \"all\" : for m , n in self . table_full_name [ mode ] . items (): self . client [ f \"bigquery_ { m } \" ] . delete_table ( n , not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , ) else : self . client [ f \"bigquery_ { mode } \" ] . delete_table ( self . table_full_name [ mode ], not_found_ok = True ) logger . info ( \" {object} {object_id} _ {mode} was {action} !\" , object_id = self . table_id , mode = mode , object = \"Table\" , action = \"deleted\" , )","title":"delete()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.init","text":"Initialize table folder at metadata_path at metadata_path/<dataset_id>/<table_id> . The folder should contain: table_config.yaml publish.sql You can also point to a sample of the data to auto complete columns names. Parameters: Name Type Description Default data_sample_path str, pathlib.PosixPath Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV, Apache Avro and Apache Parquet. None if_folder_exists str Optional. What to do if table folder exists 'raise' : Raises FileExistsError 'replace' : Replace folder 'pass' : Do nothing 'raise' if_table_config_exists str Optional What to do if table_config.yaml and publish.sql exists 'raise' : Raises FileExistsError 'replace' : Replace files with blank template 'pass' : Do nothing 'raise' source_format str Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. 'csv' columns_config_url_or_path str Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None Exceptions: Type Description FileExistsError If folder exists and replace is False. NotImplementedError If data sample is not in supported type or format. Source code in basedosdados/upload/table.py def init ( self , data_sample_path = None , if_folder_exists = \"raise\" , if_table_config_exists = \"raise\" , source_format = \"csv\" , columns_config_url_or_path = None , ): \"\"\"Initialize table folder at metadata_path at `metadata_path/<dataset_id>/<table_id>`. The folder should contain: * `table_config.yaml` * `publish.sql` You can also point to a sample of the data to auto complete columns names. Args: data_sample_path (str, pathlib.PosixPath): Optional. Data sample path to auto complete columns names It supports Comma Delimited CSV, Apache Avro and Apache Parquet. if_folder_exists (str): Optional. What to do if table folder exists * 'raise' : Raises FileExistsError * 'replace' : Replace folder * 'pass' : Do nothing if_table_config_exists (str): Optional What to do if table_config.yaml and publish.sql exists * 'raise' : Raises FileExistsError * 'replace' : Replace files with blank template * 'pass' : Do nothing source_format (str): Optional Data source format. Only 'csv', 'avro' and 'parquet' are supported. Defaults to 'csv'. columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. Raises: FileExistsError: If folder exists and replace is False. NotImplementedError: If data sample is not in supported type or format. \"\"\" if not self . dataset_folder . exists (): raise FileExistsError ( f \"Dataset folder { self . dataset_folder } folder does not exists. \" \"Create a dataset before adding tables.\" ) try : self . table_folder . mkdir ( exist_ok = ( if_folder_exists == \"replace\" )) except FileExistsError as e : if if_folder_exists == \"raise\" : raise FileExistsError ( f \"Table folder already exists for { self . table_id } . \" ) from e if if_folder_exists == \"pass\" : return self if not data_sample_path and if_table_config_exists != \"pass\" : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) partition_columns = [] if isinstance ( data_sample_path , ( str , Path , ), ): # Check if partitioned and get data sample and partition columns data_sample_path = Path ( data_sample_path ) if data_sample_path . is_dir (): data_sample_path = [ f for f in data_sample_path . glob ( \"**/*\" ) if f . is_file () and f . suffix == f \". { source_format } \" ][ 0 ] partition_columns = [ k . split ( \"=\" )[ 0 ] for k in data_sample_path . as_posix () . split ( \"/\" ) if \"=\" in k ] columns = Datatype ( self , source_format ) . header ( data_sample_path ) else : columns = [ \"column_name\" ] if if_table_config_exists == \"pass\" : # Check if config files exists before passing if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): pass # Raise if no sample to determine columns elif not data_sample_path : raise BaseDosDadosException ( \"You must provide a path to correctly create config files\" ) else : self . _make_template ( columns , partition_columns , if_table_config_exists ) elif if_table_config_exists == \"raise\" : # Check if config files already exist if ( Path ( self . table_folder / \"table_config.yaml\" ) . is_file () and Path ( self . table_folder / \"publish.sql\" ) . is_file () ): raise FileExistsError ( f \"table_config.yaml and publish.sql already exists at { self . table_folder } \" ) # if config files don't exist, create them self . _make_template ( columns , partition_columns , if_table_config_exists ) else : # Raise: without a path to data sample, should not replace config files with empty template self . _make_template ( columns , partition_columns , if_table_config_exists ) if columns_config_url_or_path is not None : self . update_columns ( columns_config_url_or_path ) return self","title":"init()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.publish","text":"Creates BigQuery table at production dataset. Table should be located at <dataset_id>.<table_id> . It creates a view that uses the query from <metadata_path>/<dataset_id>/<table_id>/publish.sql . Make sure that all columns from the query also exists at <metadata_path>/<dataset_id>/<table_id>/table_config.sql , including the partitions. Parameters: Name Type Description Default if_exists str Optional. What to do if table exists. 'raise' : Raises Conflict exception 'replace' : Replace table 'pass' : Do nothing 'raise' Todo: * Check if all required fields are filled Source code in basedosdados/upload/table.py def publish ( self , if_exists = \"raise\" ): \"\"\"Creates BigQuery table at production dataset. Table should be located at `<dataset_id>.<table_id>`. It creates a view that uses the query from `<metadata_path>/<dataset_id>/<table_id>/publish.sql`. Make sure that all columns from the query also exists at `<metadata_path>/<dataset_id>/<table_id>/table_config.sql`, including the partitions. Args: if_exists (str): Optional. What to do if table exists. * 'raise' : Raises Conflict exception * 'replace' : Replace table * 'pass' : Do nothing Todo: * Check if all required fields are filled \"\"\" if if_exists == \"replace\" : self . delete ( mode = \"prod\" ) self . client [ \"bigquery_prod\" ] . query ( ( self . table_folder / \"publish.sql\" ) . open ( \"r\" , encoding = \"utf-8\" ) . read () ) . result () self . update () logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"published\" , )","title":"publish()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.table_exists","text":"Check if table exists in BigQuery. Parameters: Name Type Description Default mode str Which dataset to check [prod|staging]. required Source code in basedosdados/upload/table.py def table_exists ( self , mode ): \"\"\"Check if table exists in BigQuery. Args: mode (str): Which dataset to check [prod|staging]. \"\"\" try : ref = self . _get_table_obj ( mode = mode ) except google . api_core . exceptions . NotFound : ref = None return bool ( ref )","title":"table_exists()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.update","text":"Updates BigQuery schema and description. Parameters: Name Type Description Default mode str Optional. Table of which table to update [prod|staging|all] 'all' not_found_ok bool Optional. What to do if table is not found required Source code in basedosdados/upload/table.py def update ( self , mode = \"all\" ): \"\"\"Updates BigQuery schema and description. Args: mode (str): Optional. Table of which table to update [prod|staging|all] not_found_ok (bool): Optional. What to do if table is not found \"\"\" self . _check_mode ( mode ) mode = [ \"prod\" , \"staging\" ] if mode == \"all\" else [ mode ] for m in mode : try : table = self . _get_table_obj ( m ) except google . api_core . exceptions . NotFound : continue # if m == \"staging\": table . description = self . _render_template ( Path ( \"table/table_description.txt\" ), self . table_config ) # save table description with open ( self . metadata_path / self . dataset_id / self . table_id / \"table_description.txt\" , \"w\" , encoding = \"utf-8\" , ) as f : f . write ( table . description ) # when mode is staging the table schema already exists table . schema = self . _load_schema ( m ) fields = [ \"description\" , \"schema\" ] if m == \"prod\" else [ \"description\" ] self . client [ f \"bigquery_ { m } \" ] . update_table ( table , fields = fields ) logger . success ( \" {object} {object_id} was {action} !\" , object_id = self . table_id , object = \"Table\" , action = \"updated\" , )","title":"update()"},{"location":"api_reference_python/#basedosdados.upload.table.Table.update_columns","text":"Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format . : - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Parameters: Name Type Description Default columns_config_url_or_path str Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/ /edit#gid= . None Source code in basedosdados/upload/table.py def update_columns ( self , columns_config_url_or_path = None ): \"\"\" Fills columns in table_config.yaml automatically using a public google sheets URL or a local file. Also regenerate publish.sql and autofill type using bigquery_type. The sheet must contain the columns: - name: column name - description: column description - bigquery_type: column bigquery type - measurement_unit: column mesurement unit - covered_by_dictionary: column related dictionary - directory_column: column related directory in the format <dataset_id>.<table_id>:<column_name> - temporal_coverage: column temporal coverage - has_sensitive_data: the column has sensitive data - observations: column observations Args: columns_config_url_or_path (str): Path to the local architeture file or a public google sheets URL. Path only suports csv, xls, xlsx, xlsm, xlsb, odf, ods, odt formats. Google sheets URL must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>. \"\"\" ruamel = ryaml . YAML () ruamel . preserve_quotes = True ruamel . indent ( mapping = 4 , sequence = 6 , offset = 4 ) table_config_yaml = ruamel . load ( ( self . table_folder / \"table_config.yaml\" ) . open ( encoding = \"utf-8\" ) ) if \"https://docs.google.com/spreadsheets/d/\" in columns_config_url_or_path : if ( \"edit#gid=\" not in columns_config_url_or_path or \"https://docs.google.com/spreadsheets/d/\" not in columns_config_url_or_path or not columns_config_url_or_path . split ( \"=\" )[ 1 ] . isdigit () ): raise BaseDosDadosException ( \"The Google sheet url not in correct format.\" \"The url must be in the format https://docs.google.com/spreadsheets/d/<table_key>/edit#gid=<table_gid>\" ) df = self . _sheet_to_df ( columns_config_url_or_path ) else : file_type = columns_config_url_or_path . split ( \".\" )[ - 1 ] if file_type == \"csv\" : df = pd . read_csv ( columns_config_url_or_path , encoding = \"utf-8\" ) elif file_type in [ \"xls\" , \"xlsx\" , \"xlsm\" , \"xlsb\" , \"odf\" , \"ods\" , \"odt\" ]: df = pd . read_excel ( columns_config_url_or_path ) else : raise BaseDosDadosException ( \"File not suported. Only csv, xls, xlsx, xlsm, xlsb, odf, ods, odt are supported.\" ) df = df . fillna ( \"NULL\" ) required_columns = [ \"name\" , \"bigquery_type\" , \"description\" , \"temporal_coverage\" , \"covered_by_dictionary\" , \"directory_column\" , \"measurement_unit\" , \"has_sensitive_data\" , \"observations\" , ] not_found_columns = required_columns . copy () for sheet_column in df . columns . tolist (): for required_column in required_columns : if sheet_column == required_column : not_found_columns . remove ( required_column ) if not_found_columns : raise BaseDosDadosException ( f \"The following required columns are not found: { ', ' . join ( not_found_columns ) } .\" ) columns_parameters = zip ( * [ df [ required_column ] . tolist () for required_column in required_columns ] ) for ( name , bigquery_type , description , temporal_coverage , covered_by_dictionary , directory_column , measurement_unit , has_sensitive_data , observations , ) in columns_parameters : for col in table_config_yaml [ \"columns\" ]: if col [ \"name\" ] == name : col [ \"bigquery_type\" ] = ( col [ \"bigquery_type\" ] if bigquery_type == \"NULL\" else bigquery_type . lower () ) col [ \"description\" ] = ( col [ \"description\" ] if description == \"NULL\" else description ) col [ \"temporal_coverage\" ] = ( col [ \"temporal_coverage\" ] if temporal_coverage == \"NULL\" else [ temporal_coverage ] ) col [ \"covered_by_dictionary\" ] = ( \"no\" if covered_by_dictionary == \"NULL\" else covered_by_dictionary ) dataset = directory_column . split ( \".\" )[ 0 ] col [ \"directory_column\" ][ \"dataset_id\" ] = ( col [ \"directory_column\" ][ \"dataset_id\" ] if dataset == \"NULL\" else dataset ) table = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ 0 ] col [ \"directory_column\" ][ \"table_id\" ] = ( col [ \"directory_column\" ][ \"table_id\" ] if table == \"NULL\" else table ) column = directory_column . split ( \".\" )[ - 1 ] . split ( \":\" )[ - 1 ] col [ \"directory_column\" ][ \"column_name\" ] = ( col [ \"directory_column\" ][ \"column_name\" ] if column == \"NULL\" else column ) col [ \"measurement_unit\" ] = ( col [ \"measurement_unit\" ] if measurement_unit == \"NULL\" else measurement_unit ) col [ \"has_sensitive_data\" ] = ( \"no\" if has_sensitive_data == \"NULL\" else has_sensitive_data ) col [ \"observations\" ] = ( col [ \"observations\" ] if observations == \"NULL\" else observations ) with open ( self . table_folder / \"table_config.yaml\" , \"w\" , encoding = \"utf-8\" ) as f : ruamel . dump ( table_config_yaml , f ) # regenerate publish.sql self . _make_publish_sql ()","title":"update_columns()"},{"location":"api_reference_r/","text":"R Esta API \u00e9 composta somente de m\u00f3dulos para requisi\u00e7\u00e3o de dados , ou seja, download e/ou carregamento de dados do projeto no seu ambiente de an\u00e1lise). Para fazer gerenciamento de dados no Google Cloud, busque as fun\u00e7\u00f5es na API de linha de comando ou em Python . A documenta\u00e7\u00e3o completa encontra-se na p\u00e1gina do CRAN do projeto, e segue baixo. Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas This browser does not support PDFs. Please download the PDF to view it: Download PDF . Ih rapaz, deu erro! E agora? Os principais erros encontrados do pacote da Base dos Dados no Rstudio s\u00e3o dois: * Autentica\u00e7\u00e3o * Bibliotecas atualizadas que crasham com a utiliza\u00e7\u00e3o do pacote. Portanto, se alguns destes erros aparecer para voc\u00ea, por favor, siga o passo a passo listado aqui embaixo. Autentica\u00e7\u00e3o Voc\u00ea tentou rodar um c\u00f3digo como esse aqui: query <- bdplyr(\"DATASET_ID_TABLE_ID\") df <- bd_collect(query) E chegou no seguinte erro: ! The table basedosdados.br_imprensa_nacional_dou.secao_1 doesn't have a valid name or was not found at basedosdados. Esse \u00e9 um erro de autentica\u00e7\u00e3o. Esse erro \u00e9 rotineiro devido a instala\u00e7\u00e3o do pacote basedosdados na sua m\u00e1quina e autoriza\u00e7\u00e3o do Token da Google. \u00c9 necess\u00e1rio estar atento se voc\u00ea marcou todas as caixinhas de sele\u00e7\u00e3o quando o Rstudio, no console, disponibiliza essa tela no navegador: \u00c9 necess\u00e1rio, portanto, tickar todas as caixinhas disponibilizadas. Feito isso, aparecer\u00e1 uma outra tela com um Token gerado para que o pacote Tidyverse acesse seu bigquery. Copie e cole o c\u00f3digo Token direto no console do Rstudio. Com o c\u00f3digo Token a ser inserido no console do Rstudio, \u00e9 v\u00e1lido posteriormente reiniciar a sess\u00e3o do Rstudio. Assim, ap\u00f3s reiniciar a sess\u00e3o, aparecer\u00e1 a op\u00e7\u00e3o de selecionar um e-mail quando o comando basedosdados::bdplyr(\u201cquery\u201d) for utilizado. Veja: Feito todos esses procedimentos, \u00e9 bem prov\u00e1vel que o problema de autentica\u00e7\u00e3o n\u00e3o ocorra mais. O Downgrade do bdplyr Nosso pacote em R foi constru\u00eddo utilizando outros pacotes para acesso ao Bigquery. Isso significa que existem depend\u00eancias e atualiza\u00e7\u00f5es destes pacotes que podem crashar o c\u00f3digo e dar um erro j\u00e1 conhecido pela comunidade. O erro em quest\u00e3o \u00e9 sobre a vers\u00e3o do bdplyr: Diante disso, \u00e9 recomend\u00e1vel que o usu\u00e1rio utilize o comando install.packages(\"~/Downloads/dbplyr_2.1.1.tar\", repos = NULL, type = \"source\") para um downgrade do pacote bdplyr. Feito isso, diante de todas as nossas documenta\u00e7\u00f5es, \u00e9 prov\u00e1vel que o erro seja solucionado. \u00c9 v\u00e1lido tamb\u00e9m visitar as issues que est\u00e3o atribu\u00eddas com o a etiqueta R em nosso github. Veja aqui","title":"R"},{"location":"api_reference_r/#r","text":"Esta API \u00e9 composta somente de m\u00f3dulos para requisi\u00e7\u00e3o de dados , ou seja, download e/ou carregamento de dados do projeto no seu ambiente de an\u00e1lise). Para fazer gerenciamento de dados no Google Cloud, busque as fun\u00e7\u00f5es na API de linha de comando ou em Python . A documenta\u00e7\u00e3o completa encontra-se na p\u00e1gina do CRAN do projeto, e segue baixo. Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas This browser does not support PDFs. Please download the PDF to view it: Download PDF .","title":"R"},{"location":"api_reference_r/#ih-rapaz-deu-erro-e-agora","text":"Os principais erros encontrados do pacote da Base dos Dados no Rstudio s\u00e3o dois: * Autentica\u00e7\u00e3o * Bibliotecas atualizadas que crasham com a utiliza\u00e7\u00e3o do pacote. Portanto, se alguns destes erros aparecer para voc\u00ea, por favor, siga o passo a passo listado aqui embaixo.","title":"Ih rapaz, deu erro! E agora?"},{"location":"api_reference_r/#autenticacao","text":"Voc\u00ea tentou rodar um c\u00f3digo como esse aqui: query <- bdplyr(\"DATASET_ID_TABLE_ID\") df <- bd_collect(query) E chegou no seguinte erro: ! The table basedosdados.br_imprensa_nacional_dou.secao_1 doesn't have a valid name or was not found at basedosdados. Esse \u00e9 um erro de autentica\u00e7\u00e3o. Esse erro \u00e9 rotineiro devido a instala\u00e7\u00e3o do pacote basedosdados na sua m\u00e1quina e autoriza\u00e7\u00e3o do Token da Google. \u00c9 necess\u00e1rio estar atento se voc\u00ea marcou todas as caixinhas de sele\u00e7\u00e3o quando o Rstudio, no console, disponibiliza essa tela no navegador: \u00c9 necess\u00e1rio, portanto, tickar todas as caixinhas disponibilizadas. Feito isso, aparecer\u00e1 uma outra tela com um Token gerado para que o pacote Tidyverse acesse seu bigquery. Copie e cole o c\u00f3digo Token direto no console do Rstudio. Com o c\u00f3digo Token a ser inserido no console do Rstudio, \u00e9 v\u00e1lido posteriormente reiniciar a sess\u00e3o do Rstudio. Assim, ap\u00f3s reiniciar a sess\u00e3o, aparecer\u00e1 a op\u00e7\u00e3o de selecionar um e-mail quando o comando basedosdados::bdplyr(\u201cquery\u201d) for utilizado. Veja: Feito todos esses procedimentos, \u00e9 bem prov\u00e1vel que o problema de autentica\u00e7\u00e3o n\u00e3o ocorra mais.","title":"Autentica\u00e7\u00e3o"},{"location":"api_reference_r/#o-downgrade-do-bdplyr","text":"Nosso pacote em R foi constru\u00eddo utilizando outros pacotes para acesso ao Bigquery. Isso significa que existem depend\u00eancias e atualiza\u00e7\u00f5es destes pacotes que podem crashar o c\u00f3digo e dar um erro j\u00e1 conhecido pela comunidade. O erro em quest\u00e3o \u00e9 sobre a vers\u00e3o do bdplyr: Diante disso, \u00e9 recomend\u00e1vel que o usu\u00e1rio utilize o comando install.packages(\"~/Downloads/dbplyr_2.1.1.tar\", repos = NULL, type = \"source\") para um downgrade do pacote bdplyr. Feito isso, diante de todas as nossas documenta\u00e7\u00f5es, \u00e9 prov\u00e1vel que o erro seja solucionado. \u00c9 v\u00e1lido tamb\u00e9m visitar as issues que est\u00e3o atribu\u00eddas com o a etiqueta R em nosso github. Veja aqui","title":"O Downgrade do bdplyr"},{"location":"api_reference_stata/","text":"Stata Esta API \u00e9 composta por m\u00f3dulos para requisi\u00e7\u00e3o de dados : para aquele(as) que desejam somente consultar os dados e metadados do nosso projeto (ou qualquer outro projeto no Google Cloud). Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas M\u00f3dulos (Requisi\u00e7\u00e3o de dados) Se \u00e9 a sua primeira vez utilizando o pacote, digite db basedosdados e confirme novamente se as etapas acima foram conclu\u00eddas com sucesso. O pacote cont\u00e9m 7 comandos, conforme suas funcionalidades descritas abaixo: Comando Descri\u00e7\u00e3o bd_download baixa dados da Base dos Dados (BD+). bd_read_sql baixa tabelas da BD+ usando consultas espec\u00edficas. bd_read_table baixa tabelas da BD+ usando dataset_id e table_id . bd_list_datasets lista o dataset_id dos conjuntos de dados dispon\u00edveis em query_project_id . bd_list_dataset_tables lista table_id para tabelas dispon\u00edveis no dataset_id especificado. bd_get_table_description mostra a descri\u00e7\u00e3o completa da tabela BD+. bd_get_table_columns mostra os nomes, tipos e descri\u00e7\u00f5es das colunas na tabela especificada. Cada comando tem um help file de apoio, bastando abrir o help e seguir as instru\u00e7\u00f5es: help [comando]","title":"Stata"},{"location":"api_reference_stata/#stata","text":"Esta API \u00e9 composta por m\u00f3dulos para requisi\u00e7\u00e3o de dados : para aquele(as) que desejam somente consultar os dados e metadados do nosso projeto (ou qualquer outro projeto no Google Cloud). Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas","title":"Stata"},{"location":"api_reference_stata/#modulos-requisicao-de-dados","text":"Se \u00e9 a sua primeira vez utilizando o pacote, digite db basedosdados e confirme novamente se as etapas acima foram conclu\u00eddas com sucesso. O pacote cont\u00e9m 7 comandos, conforme suas funcionalidades descritas abaixo: Comando Descri\u00e7\u00e3o bd_download baixa dados da Base dos Dados (BD+). bd_read_sql baixa tabelas da BD+ usando consultas espec\u00edficas. bd_read_table baixa tabelas da BD+ usando dataset_id e table_id . bd_list_datasets lista o dataset_id dos conjuntos de dados dispon\u00edveis em query_project_id . bd_list_dataset_tables lista table_id para tabelas dispon\u00edveis no dataset_id especificado. bd_get_table_description mostra a descri\u00e7\u00e3o completa da tabela BD+. bd_get_table_columns mostra os nomes, tipos e descri\u00e7\u00f5es das colunas na tabela especificada. Cada comando tem um help file de apoio, bastando abrir o help e seguir as instru\u00e7\u00f5es: help [comando]","title":"M\u00f3dulos (Requisi\u00e7\u00e3o de dados)"},{"location":"colab_checks/","text":"Colaborando com testes na BD+ Para manter a qualidade dos bases de dados presentes na BD+, n\u00f3s contamos com um conjunto de checagens autom\u00e1ticas que s\u00e3o realizadas durante a inser\u00e7\u00e3o e atualiza\u00e7\u00e3o de cada base. Essas checagens s\u00e3o necess\u00e1rias, mas n\u00e3o suficientes para garantir a qualidade dos dados. Elas realizam consultas basicas, como se a tabela existe ou se tem colunas totalmente nulas. Voc\u00ea pode colaborar com a BD aumentando a cobertura dos testes, diminuindo assim o trabalho de revis\u00e3o dos dados. Para isso basta criar consultas que testem a qualidade dos dados em SQL, como as seguintes: Verificar se colunas com propor\u00e7\u00e3o possuem valores entre 0 e 100 Verificar se colunas com datas seguem o padr\u00e3o YYYY-MM-DD HH:MM:SS Qual o procedimento? Incluir testes de dados deve seguir o fluxo de trabalho: Colaborando com testes na BD+ Qual o procedimento? 1. Informe seu interesse 2. Escreva sua consulta 3. Submeta sua consulta Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :) 1. Informe seu interesse Converse conosco no bate-papo da infra ou reuni\u00f5es \u00e0s 19h da segunda-feira, ambos no Discord. Caso n\u00e3o tenha uma sugest\u00e3o de melhoria podemos procurar alguma consulta que ainda n\u00e3o foi escrita. 2. Escreva sua consulta Fa\u00e7a um fork do reposit\u00f3rio da Base dos Dados+ . Em seguida adicione novas consultas e suas respectivas fun\u00e7\u00f5es de execu\u00e7\u00e3o nos arquivos checks.yaml e test_data.py . As consultas s\u00e3o escritas em um arquivo YAML com Jinja e SQL, da forma: test_select_all_works : name : Check if select query in {{ table_id }} works query : | SELECT NOT EXISTS ( SELECT * FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}` ) AS failure E executadas como testes do pacote pytest : def test_select_all_works ( configs ): result = fetch_data ( \"test_select_all_works\" , configs ) assert result . failure . values == False N\u00e3o se assuste caso n\u00e3o conhe\u00e7a algo da sintaxe acima, podemos lhe ajudar durante o processo. Note que os valores entre chaves s\u00e3o vari\u00e1veis contidas em arquivos table_config.yaml , que cont\u00e9m metadados das tabelas. Logo a escrita de consulta \u00e9 limitada pelos metadados existentes. Recomendamos consultar estes arquivos no diret\u00f3rio das bases . 3. Submeta sua consulta Por fim realize um pull request para o reposit\u00f3rio principal para que seja realizada uma revis\u00e3o da consulta.","title":"Colaborando com testes na BD+"},{"location":"colab_checks/#colaborando-com-testes-na-bd","text":"Para manter a qualidade dos bases de dados presentes na BD+, n\u00f3s contamos com um conjunto de checagens autom\u00e1ticas que s\u00e3o realizadas durante a inser\u00e7\u00e3o e atualiza\u00e7\u00e3o de cada base. Essas checagens s\u00e3o necess\u00e1rias, mas n\u00e3o suficientes para garantir a qualidade dos dados. Elas realizam consultas basicas, como se a tabela existe ou se tem colunas totalmente nulas. Voc\u00ea pode colaborar com a BD aumentando a cobertura dos testes, diminuindo assim o trabalho de revis\u00e3o dos dados. Para isso basta criar consultas que testem a qualidade dos dados em SQL, como as seguintes: Verificar se colunas com propor\u00e7\u00e3o possuem valores entre 0 e 100 Verificar se colunas com datas seguem o padr\u00e3o YYYY-MM-DD HH:MM:SS","title":"Colaborando com testes na BD+"},{"location":"colab_checks/#qual-o-procedimento","text":"Incluir testes de dados deve seguir o fluxo de trabalho: Colaborando com testes na BD+ Qual o procedimento? 1. Informe seu interesse 2. Escreva sua consulta 3. Submeta sua consulta Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)","title":"Qual o procedimento?"},{"location":"colab_checks/#1-informe-seu-interesse","text":"Converse conosco no bate-papo da infra ou reuni\u00f5es \u00e0s 19h da segunda-feira, ambos no Discord. Caso n\u00e3o tenha uma sugest\u00e3o de melhoria podemos procurar alguma consulta que ainda n\u00e3o foi escrita.","title":"1. Informe seu interesse"},{"location":"colab_checks/#2-escreva-sua-consulta","text":"Fa\u00e7a um fork do reposit\u00f3rio da Base dos Dados+ . Em seguida adicione novas consultas e suas respectivas fun\u00e7\u00f5es de execu\u00e7\u00e3o nos arquivos checks.yaml e test_data.py . As consultas s\u00e3o escritas em um arquivo YAML com Jinja e SQL, da forma: test_select_all_works : name : Check if select query in {{ table_id }} works query : | SELECT NOT EXISTS ( SELECT * FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}` ) AS failure E executadas como testes do pacote pytest : def test_select_all_works ( configs ): result = fetch_data ( \"test_select_all_works\" , configs ) assert result . failure . values == False N\u00e3o se assuste caso n\u00e3o conhe\u00e7a algo da sintaxe acima, podemos lhe ajudar durante o processo. Note que os valores entre chaves s\u00e3o vari\u00e1veis contidas em arquivos table_config.yaml , que cont\u00e9m metadados das tabelas. Logo a escrita de consulta \u00e9 limitada pelos metadados existentes. Recomendamos consultar estes arquivos no diret\u00f3rio das bases .","title":"2. Escreva sua consulta"},{"location":"colab_checks/#3-submeta-sua-consulta","text":"Por fim realize um pull request para o reposit\u00f3rio principal para que seja realizada uma revis\u00e3o da consulta.","title":"3. Submeta sua consulta"},{"location":"colab_data/","text":"Suba dados na BD+ Por que minha organiza\u00e7\u00e3o deve subir dados na BD+? Capacidade de cruzar suas bases com dados de diferentes organiza\u00e7\u00f5es de forma simples e f\u00e1cil. J\u00e1 s\u00e3o centenas de conjuntos de dados p\u00fablicos das maiores organiza\u00e7\u00f5es do Brasil e do mundo presentes no nosso datalake . Compromisso com a transpar\u00eancia, qualidade dos dados e desenvolvimento de melhores pesquisas, an\u00e1lises e solu\u00e7\u00f5es para a sociedade. N\u00e3o s\u00f3 democratizamos o acesso a dados abertos, mas tamb\u00e9m dados de qualidade. Temos um time especializado que revisa e garante a qualidade dos dados adicionados ao datalake . Participa\u00e7\u00e3o de uma comunidade que cresce cada vez mais : milhares de jornalistas, pesquisadores(as), desenvolvedores(as), j\u00e1 utilizam e acompanham a Base dos Dados. Passo a passo para subir dados Quer subir dados na BD+ e nos ajudar a construir esse reposit\u00f3rio? Maravilha! Organizamos tudo o que voc\u00ea precisa no manual abaixo, em 10 passos. Para facilitar a explica\u00e7\u00e3o, vamos seguir um exemplo j\u00e1 pronto com dados da RAIS . Voc\u00ea pode navegar pelas etapas no menu \u00e0 esquerda. Sugerimos fortemente que entre em nosso canal no Discord para tirar d\u00favidas e interagir com a equipe e outros(as) colaboradores(as)! \ud83d\ude09 Antes de come\u00e7ar Alguns conhecimentos s\u00e3o necess\u00e1rias para realizar esse processo: Python, R, SQL e/ou Stata : para criar os c\u00f3digos de captura e limpeza dos dados. Linha de comando : para configurar seu ambiente local e conex\u00e3o com o Google Cloud. Github : para subir seu c\u00f3digo para revis\u00e3o da nossa equipe. N\u00e3o tem alguma dessas habilidades, mas quer colaborar? Temos um time de dados que pode te ajudar, basta entrar no nosso Discord e mandar uma mensagem em #quero-contribuir. 1. Informar seu interesse para a gente Mantemos a lista de conjuntos que ainda n\u00e3o est\u00e3o na BD+ no nosso Github . Para come\u00e7ar a subir uma base do seu interesse, basta abrir uma nova issue de dados e preencher as informa\u00e7\u00f5es indicadas por l\u00e1. Caso sua base (conjunto) j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github como assignee . 2. Baixar nossa pasta template para dados Baixe aqui a pasta template e renomeie para o nome do seu conjunto de dados, <dataset_id> ( veja aqui como nomear seu conjunto ). Ela facilita todos os passos daqui pra frente. Sua estrutura \u00e9 a seguinte: <dataset_id>/ code/ : C\u00f3digos necess\u00e1rios para captura e limpeza dos dados ( veja no passo 5 ). input/ : Cont\u00e9m todos os arquivos com dados originais, exatamente como baixados da fonte prim\u00e1ria. Esses arquivos n\u00e3o devem ser modificados. output/ : Arquivos finais, j\u00e1 no formato pronto para subir na BD+. tmp/ : Quaisquer arquivos tempor\u00e1rios criados pelo c\u00f3digo em /code no processo de limpeza e tratamento. extra/ architecture/ : Tabelas de arquitetura ( veja no passo 4 ). auxiliary_files/ : Arquivos auxiliares aos dados ( veja no passo 6 ). dicionario.csv : Tabela dicion\u00e1rio de todo o conjunto de dados ( veja no passo 7 ). As pastas input , output e tmp n\u00e3o ser\u00e3o commitadas para o seu projeto e existir\u00e3o apenas localmente. 3. Preencher as tabelas de arquitetura As tabelas de arquitetura determinam qual a estrutura de cada tabela do seu conjunto de dados . Elas definem, por exemplo, o nome, ordem e metadados das colunas, e como uma coluna deve ser tratada quando h\u00e1 mudan\u00e7as em vers\u00f5es (por exemplo, se uma coluna muda de nome de um ano para o outro). Cada tabela do conjunto de dados deve ter sua pr\u00f3pria tabela de arquitetura (planilha), que pode ser preenchida no Google Drive ou localmente (Excel, editor de texto). Exemplo: RAIS - Tabelas de arquitetura As tabelas de arquitetura preenchidas podem ser consultadas aqui . Seguindo nosso manual de estilo , n\u00f3s renomeamos, definimos os tipos, preenchemos descri\u00e7\u00f5es, indicamos se h\u00e1 dicion\u00e1rio ou diret\u00f3rio, preenchemos campos (e.g. cobertura temporal e unidade de medida) e fizemos a compatibiliza\u00e7\u00e3o entre anos para todas as vari\u00e1veis (colunas). name : nome da coluna. bigquery_type : tipo de dado do BigQuery (veja quais s\u00e3o no nosso manual de estilo ). description : descri\u00e7\u00e3o dos dados que est\u00e3o nesta coluna. temporal_coverage : cobertura temporal da vari\u00e1vel nessa tabela (veja como preencher no nosso manual de estilo ). covered_by_dictionary : indicar se a vari\u00e1vel \u00e9 coberta por dicion\u00e1rio. Op\u00e7\u00f5es de respostas s\u00e3o: yes , no . directory_column : se a coluna for coberta por um dicion\u00e1rio da BD, usar o formato [DATASET_ID].[TABLE]:[COLUNA] (e.g. br_bd_diretorios_data_tempo.ano:ano ). Caso contr\u00e1rio, deixe em branco measurement_unit : qual unidade de medida da coluna (veja as unidades de medida dispon\u00edveis para preenchimento no nosso Github . has_sensitive_data : indicar se a coluna possui dados sens\u00edveis (e.g. CPF identificado, dados de conta banc\u00e1ria, etc). Op\u00e7\u00f5es de preenchimento s\u00e3o: yes, no . observations : observa\u00e7\u00f5es de tratamento que precisam ser evidenciados. Indicar, por exemplo, porque determinada coluna foi criada ou modificada. original_name : indicar o nome original de cada coluna para cada ano, no formato original_name_YYYY . No exemplo da RAIS, existiam colunas que deixaram de existir em determinados anos. Por isso, criamos colunas \u00e0 direita em ordem descendente (e.g. 2020, 2019, 2018, ...). Para as que foram exclu\u00eddas da vers\u00e3o em produ\u00e7\u00e3o, deixamos seu nome como (deletado) e n\u00e3o preenchemos nenhum metadado. Por exemplo, a coluna Municipio da tabela microdados_vinculos n\u00e3o foi adicionada pois por padr\u00e3o usamos somente o c\u00f3digo IBGE para identificar munic\u00edpios (seu nome fica numa tabela de Diret\u00f3rios ). Logo, ela aparece com o nome (deletado) na respectiva tabela de arquitetura (pen\u00faltima linha). Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados ou nossa comunidade para validar tudo. \u00c9 importante ter certeza que est\u00e1 fazendo sentido antes de come\u00e7ar a escrever c\u00f3digo. 4. Escrever c\u00f3digo de captura e limpeza de dados Ap\u00f3s validadas as tabelas de arquitetura, podemos escrever os c\u00f3digos de captura e limpeza dos dados. Captura : C\u00f3digo que baixa automaticamente todos os dados originais e os salva em /input . Esses dados podem estar dispon\u00edveis em portais ou links FTP, podem ser raspados de sites, entre outros. Limpeza : C\u00f3digo que transforma os dados originais salvos em /input em dados limpos, salva na pasta /output , para, posteriormente, serem subidos na BD+. Cada tabela limpa para produ\u00e7\u00e3o pode ser salva como um arquivo .csv \u00fanico ou, caso seja muito grande (e.g. acima de 100-200 mb), ser particionada no formato Hive em v\u00e1rios sub-arquivos .csv . Nossa recomenda\u00e7\u00e3o \u00e9 particionar tabelas por ano , mes , sigla_uf ou no m\u00e1ximo por id_municipio . A tabela microdados_vinculos da RAIS, por exemplo, \u00e9 uma tabela muito grande (+250GB) por isso n\u00f3s particionamos por ano e sigla_uf . O particionamento foi feito usando a estrutura de pastas /microdados_vinculos/ano=YYYY/sigla_uf=XX . No pacote basedosdados voc\u00ea encontra fun\u00e7\u00f5es \u00fateis para limpeza dos dados Definimos fun\u00e7\u00f5es para particionar tabelas de forma autom\u00e1tica, criar vari\u00e1veis comuns (e.g. sigla_uf a partir de id_uf ) e outras. Veja em Python e R Padr\u00f5es necess\u00e1rios no c\u00f3digo Devem ser escritos em Python , R ou Stata - para que a revis\u00e3o possa ser realizada pela equipe. Pode estar em script ( .py , .R , ...) ou notebooks (Google Colab, Jupyter, Rmarkdown, etc). Os caminhos de arquivos devem ser atalhos relativos \u00e0 pasta ra\u00edz ( <dataset_id> ), ou seja, n\u00e3o devem depender dos caminhos do seu computador. A limpeza deve seguir nosso manual de estilo e as melhores pr\u00e1ticas de programa\u00e7\u00e3o . Exemplo: PNAD Cont\u00ednua - C\u00f3digo de limpeza O c\u00f3digo de limpeza foi constru\u00eddo em R e pode ser consultado aqui . 5. (Caso necess\u00e1rio) Organizar arquivos auxiliares \u00c9 comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t\u00e9cnicas, descri\u00e7\u00f5es de coleta e amostragem, etc. Para ajudar usu\u00e1rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em /extra/auxiliary_files . Fique \u00e0 vontade para estruturar sub-pastas como quiser l\u00e1 dentro. O que importa \u00e9 que fique claro o que s\u00e3o esses arquivos. 6. (Caso necess\u00e1rio) Criar tabela dicion\u00e1rio Muitas vezes, especialmente com bases antigas, h\u00e1 m\u00faltiplos dicion\u00e1rios em formatos Excel ou outros. Na Base dos Dados n\u00f3s unificamos tudo em um \u00fanico arquivo em formato .csv - um \u00fanico dicion\u00e1rio para todas as colunas de todas as tabelas do seu conjunto. Detalhes importantes de como construir seu dicion\u00e1rio est\u00e3o no nosso manual de estilo . Exemplo: RAIS - Dicion\u00e1rio O dicion\u00e1rio completo pode ser consultado aqui . Ele j\u00e1 possui a estrutura padr\u00e3o que utilizamos para dicion\u00e1rios. 7. Configure suas credenciais localmente e prepare o ambiente No seu terminal: Instale nosso cliente: pip install basedosdados . Rode basedosdados config init e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Caso seu ambiente de produ\u00e7\u00e3o n\u00e3o permita o uso interativo do nosso cliente ou apresente alguma outra dificuldade relativa a esse modo de configura\u00e7\u00e3o, voc\u00ea pode configurar o basedosdados a partir de vari\u00e1veis de ambiente da seguinte forma: export BASEDOSDADOS_CONFIG = $( cat ~/.basedosdados/config.toml | base64 ) export BASEDOSDADOS_CREDENTIALS_PROD = $( cat ~/.basedosdados/credentials/prod.json | base64 ) export BASEDOSDADOS_CREDENTIALS_STAGING = $( cat ~/.basedosdados/credentials/staging.json | base64 ) Clone um fork do nosso reposit\u00f3rio localmente. D\u00ea um cd para a pasta local do reposit\u00f3rio e abra uma nova branch com git checkout -b [BRANCH_ID] . Todas as adi\u00e7\u00f5es e modifica\u00e7\u00f5es ser\u00e3o inclu\u00eddas nessa branch . 8. Subir tudo no Google Cloud Tudo pronto! Agora s\u00f3 falta subir para o Google Cloud e enviar para revis\u00e3o. Para isso, vamos usar o cliente basedosdados (dispon\u00edvel em linha de comando e Python) que facilita as configura\u00e7\u00f5es e etapas do processo. Configure seu projeto no Google Cloud e um bucket no Google Storage Os dados v\u00e3o passar ao todo por 3 lugares no Google Cloud: Storage : local onde ser\u00e3o armazenados o arquivos \"frios\" (arquiteturas, dados, arquivos auxiliares). BigQuery : super banco de dados do Google, dividido em 2 projetos/tipos de tabela: Staging : banco para teste e tratamento final do conjunto de dados. Produ\u00e7\u00e3o : banco oficial de publica\u00e7\u00e3o dos dados (nosso projeto basedosdados ou o seu mesmo caso queira reproduzir o ambiente) \u00c9 necess\u00e1rio ter uma conta Google e um projeto (gratuito) no Google Cloud. Para criar seu projeto basta: Acessar o link e aceitar o Termo de Servi\u00e7os do Google Cloud. Clicar em Create Project/Criar Projeto - escolha um nome bacana para o seu projeto, ele ter\u00e1 tamb\u00e9m um Project ID que ser\u00e1 utilizado para configura\u00e7\u00e3o local. Depois de criado o projeto, v\u00e1 at\u00e9 a funcionalidade de Storage e crie uma pasta ( bucket ) para subir os dados (pode ter o mesmo nome do projeto). Suba e configure uma tabela no seu bucket Aqui s\u00e3o dois passos: primeiro publicamos uma base (conjunto) e depois publicamos tabelas. Este processo est\u00e1 automatizado na cria\u00e7\u00e3o das tabelas, conforme abaixo. Para publicar o dataset e a(s) tabela(s) : Crie a tabela no bucket , indicando o caminho do arquivo no seu local, rodando o seguinte comando no seu terminal: basedosdados table create [ DATASET_ID ] [ TABLE_ID ] --path <caminho_para_os_dados> --force_dataset False --if_table_exists raise --if_storage_data_exists raise --if_table_config_exists raise --columns_config_url <url_da_planilha_google> Se preferir, voc\u00ea pode usar a API do Python, da seguinte forma: import basedosdados as bd tb = bd . Table ( dataset_id =< dataset_id > , table_id =< table_id > ) tb . create ( path = 'caminho_para_os_dados' , force_dataset = False , if_table_exists = 'raise' , if_storage_data_exists = 'raise' , if_table_config_exists = 'raise' , ) Os seguintes par\u00e2metros podem ser usados (no terminal ou no Python): -- path (obrigat\u00f3rio): o caminho completo do arquivo no seu computador, como: /Users/<seu_usuario>/projetos/basedosdados/mais/bases/[DATASET_ID]/output/microdados.csv . Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est\u00e3o as parti\u00e7\u00f5es. No contr\u00e1rio, deve apontar para um arquivo .csv (por exemplo, microdados.csv). --force_dataset [True|False]: comando que cria os arquivos de configura\u00e7\u00e3o do dataset locais e no BigQuery. True : os arquivos de configura\u00e7\u00e3o do dataset ser\u00e3o criados no seu projeto e, caso ele n\u00e3o exista no BigQuery, ser\u00e1 criado automaticamente. Se voc\u00ea j\u00e1 tiver criado e configurado o dataset. Se voc\u00ea j\u00e1 tiver criado e configurado o dataset, n\u00e3o use esta op\u00e7\u00e3o, pois ir\u00e1 sobrescrever arquivos . False : o dataset n\u00e3o ser\u00e1 recriado e, se n\u00e3o existir, ser\u00e1 criado automaticamente. --if_table_exists [raise|replace|pass]: comando utilizado quando a tabela j\u00e1 existe : raise : retorna mensagem de erroo. replace : substitui a tabela. pass : n\u00e3o faz nada. --if_storage_data_exists [raise|replace|pass]: comando utilizado quando os dados j\u00e1 existem no Google Cloud Storage. raise : retorna mensagem de erro replace : substitui os dados existentes. pass : n\u00e3o faz nada. --if_table_config_exists [raise|replace|pass]: comando utilizado quando as configura\u00e7\u00f5es da tabela j\u00e1 existem . raise : retorna mensagem de erro replace : substitui as configura\u00e7\u00f5es da tabela existentes pass : n\u00e3o faz nada --columns_config_url [google sheets url]: comando para preencher os metadados via arquitetura constru\u00edda do Google Sheets. A url precisa estar no formato https://docs.google.com/spreadsheets/d/ edit#gid= . Certifique-se de que a planilha est\u00e1 compartilhada com a op\u00e7\u00e3o \"qualquer pessoa com o link pode ver\" Se o projeto n\u00e3o existir no BigQuery, ele ser\u00e1 autmaticamente criado, junto com os arquivos README.md e dataset_config.yaml , que dever\u00e3o ser preenchidos, segundo o modelo j\u00e1 criado Preencha os arquivos de configura\u00e7\u00e3o da tabela: /[TABLE_ID]/table_config.yaml : informa\u00e7\u00f5es espec\u00edficas da tabela. /[TABLE_ID]/publish.sql : aqui voc\u00ea pode indicar tratamentos finais na tabela staging em SQL para publica\u00e7\u00e3o (e.g. modificar a query para dar um JOIN em outra tabela da BD+ e selecionar vari\u00e1veis). Publique a tabela em produ\u00e7\u00e3o: basedosdados table publish [ DATASET_ID ] [ TABLE_ID ] Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo. Abra o console do BigQuery e rode algumas queries para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom\u00e1ticos para facilitar esse processo no futuro. 9. Validar os metadados para publica\u00e7\u00e3o Para validar os metadados preenchidos nos arquivos dataset_config.yaml e table_config.yaml , o processo \u00e9 simples. Valide os metadados do conjunto atrav\u00e9s do comando basedosdados metadata validate [DATASET_ID] . Para validar metadados de tabelas, basta rodar basedosdados metadata validate [DATASET_ID] [TABLE_ID] . Ap\u00f3s a revis\u00e3o da nossa equipe, os dados ser\u00e3o publicados no CKAN. Atualizando metadados de bases ou tabelas j\u00e1 existentes Atrav\u00e9s do m\u00f3dulo metadata \u00e9 poss\u00edvel tamb\u00e9m trabalhar com bases e tabelas j\u00e1 existentes na plataforma. Elas podem ser atualizadas a partir do procedimento que descrevemos a seguir. Rode basedosdados metadata create [DATASET_ID] para puxar os metadados do conjunto dispon\u00edveis no site para o arquivo dataset_config.yaml . Para puxar metadados de tabelas, use basedosdados metadata create [DATASET_ID] [TABLE_ID] , que ir\u00e1 atualizo arquivo table_config.yaml Preencha os novos valores de metadados nos respectivos arquivos. Rode basedosdados metadata validate [DATASET_ID] 10. Enviar tudo para revis\u00e3o Ufa, \u00e9 isso! Agora s\u00f3 resta enviar tudo para revis\u00e3o no reposit\u00f3rio da Base dos Dados. Crie os commits necess\u00e1rios e rode git push origin [BRANCH_ID] . Depois \u00e9 s\u00f3 abrir um pull request (PR) no nosso reposit\u00f3rio. E agora? Nossa equipe ir\u00e1 revisar os dados e metadados submetidos via Github. Podemos entrar em contato para tirar d\u00favidas ou solicitar mudan\u00e7as no c\u00f3digo. Quando tudo estiver OK, fazemos um merge do seu pull request e os dados s\u00e3o automaticamente publicados na nossa plataforma!","title":"Dados"},{"location":"colab_data/#suba-dados-na-bd","text":"","title":"Suba dados na BD+"},{"location":"colab_data/#por-que-minha-organizacao-deve-subir-dados-na-bd","text":"Capacidade de cruzar suas bases com dados de diferentes organiza\u00e7\u00f5es de forma simples e f\u00e1cil. J\u00e1 s\u00e3o centenas de conjuntos de dados p\u00fablicos das maiores organiza\u00e7\u00f5es do Brasil e do mundo presentes no nosso datalake . Compromisso com a transpar\u00eancia, qualidade dos dados e desenvolvimento de melhores pesquisas, an\u00e1lises e solu\u00e7\u00f5es para a sociedade. N\u00e3o s\u00f3 democratizamos o acesso a dados abertos, mas tamb\u00e9m dados de qualidade. Temos um time especializado que revisa e garante a qualidade dos dados adicionados ao datalake . Participa\u00e7\u00e3o de uma comunidade que cresce cada vez mais : milhares de jornalistas, pesquisadores(as), desenvolvedores(as), j\u00e1 utilizam e acompanham a Base dos Dados.","title":"Por que minha organiza\u00e7\u00e3o deve subir dados na BD+?"},{"location":"colab_data/#passo-a-passo-para-subir-dados","text":"Quer subir dados na BD+ e nos ajudar a construir esse reposit\u00f3rio? Maravilha! Organizamos tudo o que voc\u00ea precisa no manual abaixo, em 10 passos. Para facilitar a explica\u00e7\u00e3o, vamos seguir um exemplo j\u00e1 pronto com dados da RAIS . Voc\u00ea pode navegar pelas etapas no menu \u00e0 esquerda. Sugerimos fortemente que entre em nosso canal no Discord para tirar d\u00favidas e interagir com a equipe e outros(as) colaboradores(as)! \ud83d\ude09","title":"Passo a passo para subir dados"},{"location":"colab_data/#antes-de-comecar","text":"Alguns conhecimentos s\u00e3o necess\u00e1rias para realizar esse processo: Python, R, SQL e/ou Stata : para criar os c\u00f3digos de captura e limpeza dos dados. Linha de comando : para configurar seu ambiente local e conex\u00e3o com o Google Cloud. Github : para subir seu c\u00f3digo para revis\u00e3o da nossa equipe. N\u00e3o tem alguma dessas habilidades, mas quer colaborar? Temos um time de dados que pode te ajudar, basta entrar no nosso Discord e mandar uma mensagem em #quero-contribuir.","title":"Antes de come\u00e7ar"},{"location":"colab_data/#1-informar-seu-interesse-para-a-gente","text":"Mantemos a lista de conjuntos que ainda n\u00e3o est\u00e3o na BD+ no nosso Github . Para come\u00e7ar a subir uma base do seu interesse, basta abrir uma nova issue de dados e preencher as informa\u00e7\u00f5es indicadas por l\u00e1. Caso sua base (conjunto) j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github como assignee .","title":"1. Informar seu interesse para a gente"},{"location":"colab_data/#2-baixar-nossa-pasta-template-para-dados","text":"Baixe aqui a pasta template e renomeie para o nome do seu conjunto de dados, <dataset_id> ( veja aqui como nomear seu conjunto ). Ela facilita todos os passos daqui pra frente. Sua estrutura \u00e9 a seguinte: <dataset_id>/ code/ : C\u00f3digos necess\u00e1rios para captura e limpeza dos dados ( veja no passo 5 ). input/ : Cont\u00e9m todos os arquivos com dados originais, exatamente como baixados da fonte prim\u00e1ria. Esses arquivos n\u00e3o devem ser modificados. output/ : Arquivos finais, j\u00e1 no formato pronto para subir na BD+. tmp/ : Quaisquer arquivos tempor\u00e1rios criados pelo c\u00f3digo em /code no processo de limpeza e tratamento. extra/ architecture/ : Tabelas de arquitetura ( veja no passo 4 ). auxiliary_files/ : Arquivos auxiliares aos dados ( veja no passo 6 ). dicionario.csv : Tabela dicion\u00e1rio de todo o conjunto de dados ( veja no passo 7 ). As pastas input , output e tmp n\u00e3o ser\u00e3o commitadas para o seu projeto e existir\u00e3o apenas localmente.","title":"2. Baixar nossa pasta template para dados"},{"location":"colab_data/#3-preencher-as-tabelas-de-arquitetura","text":"As tabelas de arquitetura determinam qual a estrutura de cada tabela do seu conjunto de dados . Elas definem, por exemplo, o nome, ordem e metadados das colunas, e como uma coluna deve ser tratada quando h\u00e1 mudan\u00e7as em vers\u00f5es (por exemplo, se uma coluna muda de nome de um ano para o outro). Cada tabela do conjunto de dados deve ter sua pr\u00f3pria tabela de arquitetura (planilha), que pode ser preenchida no Google Drive ou localmente (Excel, editor de texto).","title":"3. Preencher as tabelas de arquitetura"},{"location":"colab_data/#exemplo-rais-tabelas-de-arquitetura","text":"As tabelas de arquitetura preenchidas podem ser consultadas aqui . Seguindo nosso manual de estilo , n\u00f3s renomeamos, definimos os tipos, preenchemos descri\u00e7\u00f5es, indicamos se h\u00e1 dicion\u00e1rio ou diret\u00f3rio, preenchemos campos (e.g. cobertura temporal e unidade de medida) e fizemos a compatibiliza\u00e7\u00e3o entre anos para todas as vari\u00e1veis (colunas). name : nome da coluna. bigquery_type : tipo de dado do BigQuery (veja quais s\u00e3o no nosso manual de estilo ). description : descri\u00e7\u00e3o dos dados que est\u00e3o nesta coluna. temporal_coverage : cobertura temporal da vari\u00e1vel nessa tabela (veja como preencher no nosso manual de estilo ). covered_by_dictionary : indicar se a vari\u00e1vel \u00e9 coberta por dicion\u00e1rio. Op\u00e7\u00f5es de respostas s\u00e3o: yes , no . directory_column : se a coluna for coberta por um dicion\u00e1rio da BD, usar o formato [DATASET_ID].[TABLE]:[COLUNA] (e.g. br_bd_diretorios_data_tempo.ano:ano ). Caso contr\u00e1rio, deixe em branco measurement_unit : qual unidade de medida da coluna (veja as unidades de medida dispon\u00edveis para preenchimento no nosso Github . has_sensitive_data : indicar se a coluna possui dados sens\u00edveis (e.g. CPF identificado, dados de conta banc\u00e1ria, etc). Op\u00e7\u00f5es de preenchimento s\u00e3o: yes, no . observations : observa\u00e7\u00f5es de tratamento que precisam ser evidenciados. Indicar, por exemplo, porque determinada coluna foi criada ou modificada. original_name : indicar o nome original de cada coluna para cada ano, no formato original_name_YYYY . No exemplo da RAIS, existiam colunas que deixaram de existir em determinados anos. Por isso, criamos colunas \u00e0 direita em ordem descendente (e.g. 2020, 2019, 2018, ...). Para as que foram exclu\u00eddas da vers\u00e3o em produ\u00e7\u00e3o, deixamos seu nome como (deletado) e n\u00e3o preenchemos nenhum metadado. Por exemplo, a coluna Municipio da tabela microdados_vinculos n\u00e3o foi adicionada pois por padr\u00e3o usamos somente o c\u00f3digo IBGE para identificar munic\u00edpios (seu nome fica numa tabela de Diret\u00f3rios ). Logo, ela aparece com o nome (deletado) na respectiva tabela de arquitetura (pen\u00faltima linha). Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados ou nossa comunidade para validar tudo. \u00c9 importante ter certeza que est\u00e1 fazendo sentido antes de come\u00e7ar a escrever c\u00f3digo.","title":"Exemplo: RAIS - Tabelas de arquitetura"},{"location":"colab_data/#4-escrever-codigo-de-captura-e-limpeza-de-dados","text":"Ap\u00f3s validadas as tabelas de arquitetura, podemos escrever os c\u00f3digos de captura e limpeza dos dados. Captura : C\u00f3digo que baixa automaticamente todos os dados originais e os salva em /input . Esses dados podem estar dispon\u00edveis em portais ou links FTP, podem ser raspados de sites, entre outros. Limpeza : C\u00f3digo que transforma os dados originais salvos em /input em dados limpos, salva na pasta /output , para, posteriormente, serem subidos na BD+. Cada tabela limpa para produ\u00e7\u00e3o pode ser salva como um arquivo .csv \u00fanico ou, caso seja muito grande (e.g. acima de 100-200 mb), ser particionada no formato Hive em v\u00e1rios sub-arquivos .csv . Nossa recomenda\u00e7\u00e3o \u00e9 particionar tabelas por ano , mes , sigla_uf ou no m\u00e1ximo por id_municipio . A tabela microdados_vinculos da RAIS, por exemplo, \u00e9 uma tabela muito grande (+250GB) por isso n\u00f3s particionamos por ano e sigla_uf . O particionamento foi feito usando a estrutura de pastas /microdados_vinculos/ano=YYYY/sigla_uf=XX . No pacote basedosdados voc\u00ea encontra fun\u00e7\u00f5es \u00fateis para limpeza dos dados Definimos fun\u00e7\u00f5es para particionar tabelas de forma autom\u00e1tica, criar vari\u00e1veis comuns (e.g. sigla_uf a partir de id_uf ) e outras. Veja em Python e R","title":"4. Escrever c\u00f3digo de captura e limpeza de dados"},{"location":"colab_data/#padroes-necessarios-no-codigo","text":"Devem ser escritos em Python , R ou Stata - para que a revis\u00e3o possa ser realizada pela equipe. Pode estar em script ( .py , .R , ...) ou notebooks (Google Colab, Jupyter, Rmarkdown, etc). Os caminhos de arquivos devem ser atalhos relativos \u00e0 pasta ra\u00edz ( <dataset_id> ), ou seja, n\u00e3o devem depender dos caminhos do seu computador. A limpeza deve seguir nosso manual de estilo e as melhores pr\u00e1ticas de programa\u00e7\u00e3o .","title":"Padr\u00f5es necess\u00e1rios no c\u00f3digo"},{"location":"colab_data/#exemplo-pnad-continua-codigo-de-limpeza","text":"O c\u00f3digo de limpeza foi constru\u00eddo em R e pode ser consultado aqui .","title":"Exemplo: PNAD Cont\u00ednua - C\u00f3digo de limpeza"},{"location":"colab_data/#5-caso-necessario-organizar-arquivos-auxiliares","text":"\u00c9 comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t\u00e9cnicas, descri\u00e7\u00f5es de coleta e amostragem, etc. Para ajudar usu\u00e1rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em /extra/auxiliary_files . Fique \u00e0 vontade para estruturar sub-pastas como quiser l\u00e1 dentro. O que importa \u00e9 que fique claro o que s\u00e3o esses arquivos.","title":"5. (Caso necess\u00e1rio) Organizar arquivos auxiliares"},{"location":"colab_data/#6-caso-necessario-criar-tabela-dicionario","text":"Muitas vezes, especialmente com bases antigas, h\u00e1 m\u00faltiplos dicion\u00e1rios em formatos Excel ou outros. Na Base dos Dados n\u00f3s unificamos tudo em um \u00fanico arquivo em formato .csv - um \u00fanico dicion\u00e1rio para todas as colunas de todas as tabelas do seu conjunto. Detalhes importantes de como construir seu dicion\u00e1rio est\u00e3o no nosso manual de estilo .","title":"6. (Caso necess\u00e1rio) Criar tabela dicion\u00e1rio"},{"location":"colab_data/#exemplo-rais-dicionario","text":"O dicion\u00e1rio completo pode ser consultado aqui . Ele j\u00e1 possui a estrutura padr\u00e3o que utilizamos para dicion\u00e1rios.","title":"Exemplo: RAIS - Dicion\u00e1rio"},{"location":"colab_data/#7-configure-suas-credenciais-localmente-e-prepare-o-ambiente","text":"No seu terminal: Instale nosso cliente: pip install basedosdados . Rode basedosdados config init e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Caso seu ambiente de produ\u00e7\u00e3o n\u00e3o permita o uso interativo do nosso cliente ou apresente alguma outra dificuldade relativa a esse modo de configura\u00e7\u00e3o, voc\u00ea pode configurar o basedosdados a partir de vari\u00e1veis de ambiente da seguinte forma: export BASEDOSDADOS_CONFIG = $( cat ~/.basedosdados/config.toml | base64 ) export BASEDOSDADOS_CREDENTIALS_PROD = $( cat ~/.basedosdados/credentials/prod.json | base64 ) export BASEDOSDADOS_CREDENTIALS_STAGING = $( cat ~/.basedosdados/credentials/staging.json | base64 ) Clone um fork do nosso reposit\u00f3rio localmente. D\u00ea um cd para a pasta local do reposit\u00f3rio e abra uma nova branch com git checkout -b [BRANCH_ID] . Todas as adi\u00e7\u00f5es e modifica\u00e7\u00f5es ser\u00e3o inclu\u00eddas nessa branch .","title":"7. Configure suas credenciais localmente e prepare o ambiente"},{"location":"colab_data/#8-subir-tudo-no-google-cloud","text":"Tudo pronto! Agora s\u00f3 falta subir para o Google Cloud e enviar para revis\u00e3o. Para isso, vamos usar o cliente basedosdados (dispon\u00edvel em linha de comando e Python) que facilita as configura\u00e7\u00f5es e etapas do processo.","title":"8. Subir tudo no Google Cloud"},{"location":"colab_data/#configure-seu-projeto-no-google-cloud-e-um-bucket-no-google-storage","text":"Os dados v\u00e3o passar ao todo por 3 lugares no Google Cloud: Storage : local onde ser\u00e3o armazenados o arquivos \"frios\" (arquiteturas, dados, arquivos auxiliares). BigQuery : super banco de dados do Google, dividido em 2 projetos/tipos de tabela: Staging : banco para teste e tratamento final do conjunto de dados. Produ\u00e7\u00e3o : banco oficial de publica\u00e7\u00e3o dos dados (nosso projeto basedosdados ou o seu mesmo caso queira reproduzir o ambiente) \u00c9 necess\u00e1rio ter uma conta Google e um projeto (gratuito) no Google Cloud. Para criar seu projeto basta: Acessar o link e aceitar o Termo de Servi\u00e7os do Google Cloud. Clicar em Create Project/Criar Projeto - escolha um nome bacana para o seu projeto, ele ter\u00e1 tamb\u00e9m um Project ID que ser\u00e1 utilizado para configura\u00e7\u00e3o local. Depois de criado o projeto, v\u00e1 at\u00e9 a funcionalidade de Storage e crie uma pasta ( bucket ) para subir os dados (pode ter o mesmo nome do projeto).","title":"Configure seu projeto no Google Cloud e um bucket no Google Storage"},{"location":"colab_data/#suba-e-configure-uma-tabela-no-seu-bucket","text":"Aqui s\u00e3o dois passos: primeiro publicamos uma base (conjunto) e depois publicamos tabelas. Este processo est\u00e1 automatizado na cria\u00e7\u00e3o das tabelas, conforme abaixo. Para publicar o dataset e a(s) tabela(s) : Crie a tabela no bucket , indicando o caminho do arquivo no seu local, rodando o seguinte comando no seu terminal: basedosdados table create [ DATASET_ID ] [ TABLE_ID ] --path <caminho_para_os_dados> --force_dataset False --if_table_exists raise --if_storage_data_exists raise --if_table_config_exists raise --columns_config_url <url_da_planilha_google> Se preferir, voc\u00ea pode usar a API do Python, da seguinte forma: import basedosdados as bd tb = bd . Table ( dataset_id =< dataset_id > , table_id =< table_id > ) tb . create ( path = 'caminho_para_os_dados' , force_dataset = False , if_table_exists = 'raise' , if_storage_data_exists = 'raise' , if_table_config_exists = 'raise' , ) Os seguintes par\u00e2metros podem ser usados (no terminal ou no Python): -- path (obrigat\u00f3rio): o caminho completo do arquivo no seu computador, como: /Users/<seu_usuario>/projetos/basedosdados/mais/bases/[DATASET_ID]/output/microdados.csv . Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est\u00e3o as parti\u00e7\u00f5es. No contr\u00e1rio, deve apontar para um arquivo .csv (por exemplo, microdados.csv). --force_dataset [True|False]: comando que cria os arquivos de configura\u00e7\u00e3o do dataset locais e no BigQuery. True : os arquivos de configura\u00e7\u00e3o do dataset ser\u00e3o criados no seu projeto e, caso ele n\u00e3o exista no BigQuery, ser\u00e1 criado automaticamente. Se voc\u00ea j\u00e1 tiver criado e configurado o dataset. Se voc\u00ea j\u00e1 tiver criado e configurado o dataset, n\u00e3o use esta op\u00e7\u00e3o, pois ir\u00e1 sobrescrever arquivos . False : o dataset n\u00e3o ser\u00e1 recriado e, se n\u00e3o existir, ser\u00e1 criado automaticamente. --if_table_exists [raise|replace|pass]: comando utilizado quando a tabela j\u00e1 existe : raise : retorna mensagem de erroo. replace : substitui a tabela. pass : n\u00e3o faz nada. --if_storage_data_exists [raise|replace|pass]: comando utilizado quando os dados j\u00e1 existem no Google Cloud Storage. raise : retorna mensagem de erro replace : substitui os dados existentes. pass : n\u00e3o faz nada. --if_table_config_exists [raise|replace|pass]: comando utilizado quando as configura\u00e7\u00f5es da tabela j\u00e1 existem . raise : retorna mensagem de erro replace : substitui as configura\u00e7\u00f5es da tabela existentes pass : n\u00e3o faz nada --columns_config_url [google sheets url]: comando para preencher os metadados via arquitetura constru\u00edda do Google Sheets. A url precisa estar no formato https://docs.google.com/spreadsheets/d/ edit#gid= . Certifique-se de que a planilha est\u00e1 compartilhada com a op\u00e7\u00e3o \"qualquer pessoa com o link pode ver\" Se o projeto n\u00e3o existir no BigQuery, ele ser\u00e1 autmaticamente criado, junto com os arquivos README.md e dataset_config.yaml , que dever\u00e3o ser preenchidos, segundo o modelo j\u00e1 criado Preencha os arquivos de configura\u00e7\u00e3o da tabela: /[TABLE_ID]/table_config.yaml : informa\u00e7\u00f5es espec\u00edficas da tabela. /[TABLE_ID]/publish.sql : aqui voc\u00ea pode indicar tratamentos finais na tabela staging em SQL para publica\u00e7\u00e3o (e.g. modificar a query para dar um JOIN em outra tabela da BD+ e selecionar vari\u00e1veis). Publique a tabela em produ\u00e7\u00e3o: basedosdados table publish [ DATASET_ID ] [ TABLE_ID ] Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo. Abra o console do BigQuery e rode algumas queries para testar se foi tudo publicado corretamente. Estamos desenvolvendo testes autom\u00e1ticos para facilitar esse processo no futuro.","title":"Suba e configure uma tabela no seu bucket"},{"location":"colab_data/#9-validar-os-metadados-para-publicacao","text":"Para validar os metadados preenchidos nos arquivos dataset_config.yaml e table_config.yaml , o processo \u00e9 simples. Valide os metadados do conjunto atrav\u00e9s do comando basedosdados metadata validate [DATASET_ID] . Para validar metadados de tabelas, basta rodar basedosdados metadata validate [DATASET_ID] [TABLE_ID] . Ap\u00f3s a revis\u00e3o da nossa equipe, os dados ser\u00e3o publicados no CKAN.","title":"9. Validar os metadados para publica\u00e7\u00e3o"},{"location":"colab_data/#atualizando-metadados-de-bases-ou-tabelas-ja-existentes","text":"Atrav\u00e9s do m\u00f3dulo metadata \u00e9 poss\u00edvel tamb\u00e9m trabalhar com bases e tabelas j\u00e1 existentes na plataforma. Elas podem ser atualizadas a partir do procedimento que descrevemos a seguir. Rode basedosdados metadata create [DATASET_ID] para puxar os metadados do conjunto dispon\u00edveis no site para o arquivo dataset_config.yaml . Para puxar metadados de tabelas, use basedosdados metadata create [DATASET_ID] [TABLE_ID] , que ir\u00e1 atualizo arquivo table_config.yaml Preencha os novos valores de metadados nos respectivos arquivos. Rode basedosdados metadata validate [DATASET_ID]","title":"Atualizando metadados de bases ou tabelas j\u00e1 existentes"},{"location":"colab_data/#10-enviar-tudo-para-revisao","text":"Ufa, \u00e9 isso! Agora s\u00f3 resta enviar tudo para revis\u00e3o no reposit\u00f3rio da Base dos Dados. Crie os commits necess\u00e1rios e rode git push origin [BRANCH_ID] . Depois \u00e9 s\u00f3 abrir um pull request (PR) no nosso reposit\u00f3rio. E agora? Nossa equipe ir\u00e1 revisar os dados e metadados submetidos via Github. Podemos entrar em contato para tirar d\u00favidas ou solicitar mudan\u00e7as no c\u00f3digo. Quando tudo estiver OK, fazemos um merge do seu pull request e os dados s\u00e3o automaticamente publicados na nossa plataforma!","title":"10. Enviar tudo para revis\u00e3o"},{"location":"colab_infrastructure/","text":"Infra da BD Nosso time de infraestrutura cuida para que todos os pacotes e pipelines estejam funcionando da melhor forma para o p\u00fablico. Utilizamos o Github para gerir todo o c\u00f3digo e mant\u00ea-lo organizado, onde voc\u00ea pode encontrar issues de novas features, bugs e melhorias que estamos trabalhando. Como funciona nossa infraestrutura Nossa infraestrutura \u00e9 composta de 3 frentes principais: Sistema de ingest\u00e3o de dados : desde o upload at\u00e9 a disponibiliza\u00e7\u00e3o em produ\u00e7\u00e3o; Pacotes de acesso Website : Front-end, Back-end e APIs. Atualmente \u00e9 poss\u00edvel colaborar em todas as frentes, com destaque ao desenvolvimento dos pesos e contrapesos e atualiza\u00e7\u00e3o do site. Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :) Sistema de ingest\u00e3o de dados O sistema possui ambientes de desenvolvimento ( basedosdados-dev ), homologa\u00e7\u00e3o ( basedosdados-staging ) e produ\u00e7\u00e3o ( basedosdados ) no BigQuery. Os processos para subida de dados s\u00e3o detalhados na imagem abaixo, sendo alguns deles automatizados via Github Actions. Explicamos com mais detalhes do funcionamento desse sistema no blog . Como contribuir? Melhorando a documenta\u00e7\u00e3o do sistema aqui :) Criando checagens autom\u00e1ticas de qualidade de dados e metadados (em Python) Criando novos issues e sugest\u00f5es de melhorias Pacotes de acesso Os pacotes de acesso ao datalake est\u00e3o em constante melhoria e voc\u00ea pode colaborar com a gente com novas features, conserto de bugs e muito mais. Como contribuir? Explore os issues do pacote Python Explore os issues do pacote R Ajude a desenvolver o pacote em Stata Website Nosso website \u00e9 desenvolvido em Next.js e consome uma API de metadados do CKAN. O c\u00f3digo do site est\u00e1 tamb\u00e9m no nosso Github . Como contribuir? Melhore o UX do site (Next, CSS, HTML) Ajudando em issues abertas de BE, FE ou API Criando novos issues e sugest\u00f5es de melhorias","title":"Infraestrutura"},{"location":"colab_infrastructure/#infra-da-bd","text":"Nosso time de infraestrutura cuida para que todos os pacotes e pipelines estejam funcionando da melhor forma para o p\u00fablico. Utilizamos o Github para gerir todo o c\u00f3digo e mant\u00ea-lo organizado, onde voc\u00ea pode encontrar issues de novas features, bugs e melhorias que estamos trabalhando.","title":"Infra da BD"},{"location":"colab_infrastructure/#como-funciona-nossa-infraestrutura","text":"Nossa infraestrutura \u00e9 composta de 3 frentes principais: Sistema de ingest\u00e3o de dados : desde o upload at\u00e9 a disponibiliza\u00e7\u00e3o em produ\u00e7\u00e3o; Pacotes de acesso Website : Front-end, Back-end e APIs. Atualmente \u00e9 poss\u00edvel colaborar em todas as frentes, com destaque ao desenvolvimento dos pesos e contrapesos e atualiza\u00e7\u00e3o do site. Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)","title":"Como funciona nossa infraestrutura"},{"location":"colab_infrastructure/#sistema-de-ingestao-de-dados","text":"O sistema possui ambientes de desenvolvimento ( basedosdados-dev ), homologa\u00e7\u00e3o ( basedosdados-staging ) e produ\u00e7\u00e3o ( basedosdados ) no BigQuery. Os processos para subida de dados s\u00e3o detalhados na imagem abaixo, sendo alguns deles automatizados via Github Actions. Explicamos com mais detalhes do funcionamento desse sistema no blog .","title":"Sistema de ingest\u00e3o de dados"},{"location":"colab_infrastructure/#como-contribuir","text":"Melhorando a documenta\u00e7\u00e3o do sistema aqui :) Criando checagens autom\u00e1ticas de qualidade de dados e metadados (em Python) Criando novos issues e sugest\u00f5es de melhorias","title":"Como contribuir?"},{"location":"colab_infrastructure/#pacotes-de-acesso","text":"Os pacotes de acesso ao datalake est\u00e3o em constante melhoria e voc\u00ea pode colaborar com a gente com novas features, conserto de bugs e muito mais.","title":"Pacotes de acesso"},{"location":"colab_infrastructure/#como-contribuir_1","text":"Explore os issues do pacote Python Explore os issues do pacote R Ajude a desenvolver o pacote em Stata","title":"Como contribuir?"},{"location":"colab_infrastructure/#website","text":"Nosso website \u00e9 desenvolvido em Next.js e consome uma API de metadados do CKAN. O c\u00f3digo do site est\u00e1 tamb\u00e9m no nosso Github .","title":"Website"},{"location":"colab_infrastructure/#como-contribuir_2","text":"Melhore o UX do site (Next, CSS, HTML) Ajudando em issues abertas de BE, FE ou API Criando novos issues e sugest\u00f5es de melhorias","title":"Como contribuir?"},{"location":"style_data/","text":"Manual de estilo Nessa se\u00e7\u00e3o listamos todos os padr\u00f5es do nosso manual de estilo e diretrizes de dados que usamos na Base dos Dados. Eles nos ajudam a manter os dados e metadados que publicamos com qualidade alta. Voc\u00ea pode usar o menu esquerdo para navegar pelos diferentes t\u00f3picos dessa p\u00e1gina. Nomea\u00e7\u00e3o de bases e tabelas Conjuntos de dados ( dataset_id ) Nomeamos conjuntos no formato <organization_id\\>_<descri\u00e7\u00e3o\\> , onde organization_id segue por padr\u00e3o a abrang\u00eancia geogr\u00e1fica da organiza\u00e7\u00e3o que publica o conjunto: organization_id Mundial mundo_<organizacao> Federal <sigla_pais>_<organizacao> Estadual <sigla_pais>_<sigla_uf>_<organizacao> Municipal <sigla_pais>_<sigla_uf>_<cidade>_<organizacao> sigla_pais e sigla_uf s\u00e3o sempre 2 letras min\u00fasculas; organizacao \u00e9 o nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que publicou os dados orginais (ex: ibge , tse , inep ). descricao \u00e9 uma breve descri\u00e7\u00e3o do conjunto de dados, que pode ser Por exemplo, o conjunto de dados do PIB do IBGE tem como dataset_id : br_ibge_pib N\u00e3o sabe como nomear a organiza\u00e7\u00e3o? Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria br_rj_detran ) Tabelas Nomear tabelas \u00e9 algo menos estruturado e, por isso, requer bom senso. Mas temos algumas regras: Se houver tabelas para diferentes entidades, incluir a entidade no come\u00e7o do nome. Exemplo: municipio_valor , uf_valor . N\u00e3o incluir a unidade temporal no nome. Exemplo: nomear municipio , e n\u00e3o municipio_ano . Deixar nomes no singular. Exemplo: escola , e n\u00e3o escolas . Nomear de microdados as tabelas mais desagregadas. Em geral essas tem dados a n\u00edvel de pessoa ou transa\u00e7\u00e3o. Exemplos de dataset_id.table_id Mundial mundo_waze.alertas Dados de alertas do Waze de diferentes cidades. Federal br_tse_eleicoes.candidatos Dados de candidatos a cargos pol\u00edticos do TSE. Federal br_ibge_pnad.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal br_ibge_pnadc.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual br_sp_see_docentes.carga_horaria Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal br_rj_riodejaneiro_cmrj_legislativo.votacoes Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ). Formatos de tabelas Tabelas devem, na medida do poss\u00edvel, estar no formato long , ao inv\u00e9s de wide . Nomea\u00e7\u00e3o de vari\u00e1veis Nomes de vari\u00e1veis devem respeitar algumas regras: Usar ao m\u00e1ximo nomes j\u00e1 presentes no reposit\u00f3rio. Exemplos: ano , mes , id_municipio , sigla_uf , idade , cargo , resultado , votos , receita , despesa , preco , etc. Respeitar padr\u00f5es das tabelas de diret\u00f3rios. Ser o mais intuitivo, claro e extenso poss\u00edvel. Ter todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por _ . N\u00e3o incluir conectores como de , da , dos , e , a , em , etc. S\u00f3 ter o prefixo id_ quando a vari\u00e1vel representar chaves prim\u00e1rias de entidades (que eventualmente teriam uma tabela de diret\u00f3rio). Exemplos que tem: id_municipio , id_uf , id_escola , id_pessoa . Exemplos que n\u00e3o tem: rede , localizacao . S\u00f3 ter sufixos de entidade quando a entidade da coluna for diferente da entidade da tabela. Exemplos que tem: numa tabela com entidade pessoa , uma coluna sobre PIB municipal se chamaria pib_municipio . Exemplos que n\u00e3o tem: numa tabela com entidade pessoa , caracter\u00edsticas da pessoa se chamariam nome , idade , sexo , etc. Lista de prefixos permitidos nome_ , data_ , numero_ , quantidade_ , proporcao_ (vari\u00e1veis de porcentagem 0-100%), taxa_ , razao_ , indice_ , indicador_ , tipo_ , sigla_ , sequencial_ . Lista de sufixos comuns _pc (per capita) Ordenamento de vari\u00e1veis A ordem de vari\u00e1veis em tabelas \u00e9 padronizada para manter uma consist\u00eancia no reposit\u00f3rio. Nossas regras s\u00e3o: Chaves prim\u00e1rias \u00e0 esquerda, em ordem descendente de abrang\u00eancia. Exemplo de ordem: ano , sigla_uf , id_municipio , id_escola , nota_ideb . Agrupar e ordenar vari\u00e1veis por import\u00e2ncia ou temas. Tipos de vari\u00e1veis N\u00f3s utilizamos algumas das op\u00e7\u00f5es de tipos do BigQuery : STRING , INT64 , FLOAT64 , DATE , TIME , GEOGRAPHY . Quando escolher: STRING : Vari\u00e1veis de texto Chaves de vari\u00e1veis categ\u00f3ricas com dicion\u00e1rio ou diret\u00f3rio INT64 : Vari\u00e1veis de n\u00fameros inteiros com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o) FLOAT64 : Vari\u00e1veis de n\u00fameros com casas decimais com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o) DATE : Vari\u00e1veis de data no formato YYYY-MM-DD TIME : Vari\u00e1veis de tempo no formato HH:MM:SS GEOGRAPHY : Vari\u00e1veis de geografia Unidades de medida A regra \u00e9 manter vari\u00e1veis com suas unidades de medida originais listadas nesse c\u00f3digo , com a exce\u00e7\u00e3o de vari\u00e1veis financeiras onde convertermos moedas antigas para as atuais (e.g. Cruzeiro para Real). Catalogamos unidades de medida em formato padr\u00e3o na tabela de arquitetura. Lista completa aqui Exemplos: m , km/h , BRL . Para colunas financeiras deflacionadas, listamos a moeda com o ano base. Exemplo: uma coluna medida em reais de 2010 tem unidade BRL_2010 . Vari\u00e1veis devem ter sempre unidades de medida com base 1. Ou seja, ter BRL ao inv\u00e9s de 1000 BRL , ou pessoa ao inv\u00e9s de 1000 pessoas . Essa informa\u00e7\u00e3o, como outros metadados de colunas, s\u00e3o registradas na tabela de arquitetura da tabela. Quais vari\u00e1veis manter, quais adicionar e quais remover Mantemos nossas tabelas parcialmente normalizadas , e temos regras para quais vari\u00e1veis incluirmos em produ\u00e7\u00e3o. Elas s\u00e3o: Remover vari\u00e1veis de nomes de entidades que j\u00e1 est\u00e3o em diret\u00f3rios. Exemplo: retirar municipio da tabela que j\u00e1 inclui id_municipio . Remover vari\u00e1veis servindo de parti\u00e7\u00e3o. Exemplo: remover ano e sigla_uf se a tabela \u00e9 particionada nessas duas dimens\u00f5es. Adicionar chaves prim\u00e1rias principais para cada entidade j\u00e1 existente. Exemplo: adicionar id_municipio a tabelas que s\u00f3 incluem id_municipio_tse . Manter todas as chaves prim\u00e1rias que j\u00e1 vem com a tabela, mas (1) adicionar chaves relevantes (e.g. sigla_uf , id_municipio ) e (2) retirar chaves irrelevantes (e.g. regiao ). Cobertura temporal Preencher a coluna cobertura_temporal nos metadados de tabela, coluna e chave (em dicion\u00e1rios) segue o seguinte padr\u00e3o. Formato geral: data_inicial(unidade_temporal)data_final data_inicial e data_final est\u00e3o na correspondente unidade temporal. Exemplo: tabela com unidade ano tem cobertura 2005(1)2018 . Exemplo: tabela com unidade mes tem cobertura 2005-08(1)2018-12 . Exemplo: tabela com unidade dia tem cobertura 2005-08-01(1)2018-12-31 . Regras para preenchimento Metadados de tabela Preencher no formato geral. Metadados de coluna Preencher no formato geral, exceto quando a data_inicial ou data_final sejam iguais aos da tabela. Nesse caso deixe vazio. Exemplo: suponha que a cobertura da tabela seja 2005(1)2018 . Se uma coluna aparece s\u00f3 em 2012 e existe at\u00e9 2018, preenchemos sua cobertura como 2012(1) . Se uma coluna desaparece em 2013, preenchemos sua cobertura como (1)2013 . Se uma coluna existe na mesma cobertura temporal da tabela, preenchemos sua cobertura como (1) . Metadados de chave Preencher no mesmo padr\u00e3o de colunas, mas a refer\u00eancia sendo a coluna correspondente, e n\u00e3o a tabela. Limpando STRINGs Vari\u00e1veis categ\u00f3ricas: inicial mai\u00fascula e resto min\u00fasculo, com acentos. STRINGs n\u00e3o-estruturadas: manter igual aos dados originais. Formatos de valores Decimal: formato americano, i.e. sempre . (ponto) ao inv\u00e9s de , (v\u00edrgula). Data: YYYY-MM-DD Hor\u00e1rio (24h): HH:MM:SS Datetime ( ISO-8601 ): YYYY-MM-DDTHH:MM:SS.sssZ Valor nulo: \"\" (csv), NULL (Python), NA (R), . ou \"\" (Stata) Propor\u00e7\u00e3o/porcentagem: entre 0-100 Particionamento de tabelas Uma tabela particionada \u00e9 uma tabela especial dividida em segmentos, chamados de parti\u00e7\u00f5es, que facilitam o gerenciamento e a consulta de seus dados. Ao dividir uma grande tabela em parti\u00e7\u00f5es menores, voc\u00ea pode melhorar o desempenho da consulta e pode controlar os custos reduzindo o n\u00famero de bytes lidos por uma consulta. Por isso, sempre recomendamos que tabelas grandes sejam particionadas. Leia mais a respeito na documenta\u00e7\u00e3o da Google Cloud . Note que ao particionar uma tabela \u00e9 preciso excluir a coluna correspondente. Exemplo: \u00e9 preciso excluir a coluna ano ao particionar por ano . Colunas comuns para usar como parti\u00e7\u00e3o: ano , mes , sigla_uf , id_municipio . N\u00famero de bases por pull request Pull requests no Github devem incluir no m\u00e1ximo uma base. Ou seja, podem envolver uma ou mais tabela intra-base. Dicion\u00e1rios Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas). Para cada tabela, coluna, e cobertura temporal, cada chave mapeia unicamente um valor. Chaves n\u00e3o podem ter valores nulos. Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais. Chaves s\u00f3 podem possuir zeros \u00e0 esquerda quando o n\u00famero de d\u00edgitos da vari\u00e1vel tiver significado. Quando a vari\u00e1vel for enum padr\u00e3o, n\u00f3s excluimos os zeros \u00e0 esquerda. Exemplo: mantemos o zero \u00e0 esquerda da vari\u00e1vel br_bd_diretorios_brasil.cbo_2002:cbo_2002 , que tem seis d\u00edgitos, pois o primeiro d\u00edgito 0 significa a categoria ser do grande grupo = \"Membros das for\u00e7as armadas, policiais e bombeiros militares\" . Para outros casos, como por exemplo br_inep_censo_escolar.turma:etapa_ensino , n\u00f3s excluimos os zeros \u00e0 esquerda. Ou seja, mudamos 01 para 1 . Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc. Diret\u00f3rios Diret\u00f3rios s\u00e3o as pedras fundamentais da estrutura do nosso datalake . Nossas regras para gerenciar diret\u00f3rios s\u00e3o: Diret\u00f3rios representam entidades do reposit\u00f3rio que tenham chaves prim\u00e1rias (e.g. uf , munic\u00edpio , escola ) e unidades de data-tempo (e.g. data , tempo , dia , mes , ano ). Cada tabela de diret\u00f3rio tem ao menos uma chave prim\u00e1ria com valores \u00fanicos e sem nulos. Exemplos: municipio:id_municipio , uf:sigla_uf . Nomes de vari\u00e1veis com prefixo id_ s\u00e3o reservadas para chaves prim\u00e1rias de entidades. Veja todas as tabelas j\u00e1 dispon\u00edveis aqui. Pensou em melhorias para os padr\u00f5es definidos? Abra um issue no nosso Github ou mande uma mensagem no Discord para conversarmos :)","title":"Manual de estilo"},{"location":"style_data/#manual-de-estilo","text":"Nessa se\u00e7\u00e3o listamos todos os padr\u00f5es do nosso manual de estilo e diretrizes de dados que usamos na Base dos Dados. Eles nos ajudam a manter os dados e metadados que publicamos com qualidade alta. Voc\u00ea pode usar o menu esquerdo para navegar pelos diferentes t\u00f3picos dessa p\u00e1gina.","title":"Manual de estilo"},{"location":"style_data/#nomeacao-de-bases-e-tabelas","text":"","title":"Nomea\u00e7\u00e3o de bases e tabelas"},{"location":"style_data/#conjuntos-de-dados-dataset_id","text":"Nomeamos conjuntos no formato <organization_id\\>_<descri\u00e7\u00e3o\\> , onde organization_id segue por padr\u00e3o a abrang\u00eancia geogr\u00e1fica da organiza\u00e7\u00e3o que publica o conjunto: organization_id Mundial mundo_<organizacao> Federal <sigla_pais>_<organizacao> Estadual <sigla_pais>_<sigla_uf>_<organizacao> Municipal <sigla_pais>_<sigla_uf>_<cidade>_<organizacao> sigla_pais e sigla_uf s\u00e3o sempre 2 letras min\u00fasculas; organizacao \u00e9 o nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que publicou os dados orginais (ex: ibge , tse , inep ). descricao \u00e9 uma breve descri\u00e7\u00e3o do conjunto de dados, que pode ser Por exemplo, o conjunto de dados do PIB do IBGE tem como dataset_id : br_ibge_pib N\u00e3o sabe como nomear a organiza\u00e7\u00e3o? Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria br_rj_detran )","title":"Conjuntos de dados (dataset_id)"},{"location":"style_data/#tabelas","text":"Nomear tabelas \u00e9 algo menos estruturado e, por isso, requer bom senso. Mas temos algumas regras: Se houver tabelas para diferentes entidades, incluir a entidade no come\u00e7o do nome. Exemplo: municipio_valor , uf_valor . N\u00e3o incluir a unidade temporal no nome. Exemplo: nomear municipio , e n\u00e3o municipio_ano . Deixar nomes no singular. Exemplo: escola , e n\u00e3o escolas . Nomear de microdados as tabelas mais desagregadas. Em geral essas tem dados a n\u00edvel de pessoa ou transa\u00e7\u00e3o.","title":"Tabelas"},{"location":"style_data/#exemplos-de-dataset_idtable_id","text":"Mundial mundo_waze.alertas Dados de alertas do Waze de diferentes cidades. Federal br_tse_eleicoes.candidatos Dados de candidatos a cargos pol\u00edticos do TSE. Federal br_ibge_pnad.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal br_ibge_pnadc.microdados Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual br_sp_see_docentes.carga_horaria Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal br_rj_riodejaneiro_cmrj_legislativo.votacoes Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ).","title":"Exemplos de dataset_id.table_id"},{"location":"style_data/#formatos-de-tabelas","text":"Tabelas devem, na medida do poss\u00edvel, estar no formato long , ao inv\u00e9s de wide .","title":"Formatos de tabelas"},{"location":"style_data/#nomeacao-de-variaveis","text":"Nomes de vari\u00e1veis devem respeitar algumas regras: Usar ao m\u00e1ximo nomes j\u00e1 presentes no reposit\u00f3rio. Exemplos: ano , mes , id_municipio , sigla_uf , idade , cargo , resultado , votos , receita , despesa , preco , etc. Respeitar padr\u00f5es das tabelas de diret\u00f3rios. Ser o mais intuitivo, claro e extenso poss\u00edvel. Ter todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por _ . N\u00e3o incluir conectores como de , da , dos , e , a , em , etc. S\u00f3 ter o prefixo id_ quando a vari\u00e1vel representar chaves prim\u00e1rias de entidades (que eventualmente teriam uma tabela de diret\u00f3rio). Exemplos que tem: id_municipio , id_uf , id_escola , id_pessoa . Exemplos que n\u00e3o tem: rede , localizacao . S\u00f3 ter sufixos de entidade quando a entidade da coluna for diferente da entidade da tabela. Exemplos que tem: numa tabela com entidade pessoa , uma coluna sobre PIB municipal se chamaria pib_municipio . Exemplos que n\u00e3o tem: numa tabela com entidade pessoa , caracter\u00edsticas da pessoa se chamariam nome , idade , sexo , etc. Lista de prefixos permitidos nome_ , data_ , numero_ , quantidade_ , proporcao_ (vari\u00e1veis de porcentagem 0-100%), taxa_ , razao_ , indice_ , indicador_ , tipo_ , sigla_ , sequencial_ . Lista de sufixos comuns _pc (per capita)","title":"Nomea\u00e7\u00e3o de vari\u00e1veis"},{"location":"style_data/#ordenamento-de-variaveis","text":"A ordem de vari\u00e1veis em tabelas \u00e9 padronizada para manter uma consist\u00eancia no reposit\u00f3rio. Nossas regras s\u00e3o: Chaves prim\u00e1rias \u00e0 esquerda, em ordem descendente de abrang\u00eancia. Exemplo de ordem: ano , sigla_uf , id_municipio , id_escola , nota_ideb . Agrupar e ordenar vari\u00e1veis por import\u00e2ncia ou temas.","title":"Ordenamento de vari\u00e1veis"},{"location":"style_data/#tipos-de-variaveis","text":"N\u00f3s utilizamos algumas das op\u00e7\u00f5es de tipos do BigQuery : STRING , INT64 , FLOAT64 , DATE , TIME , GEOGRAPHY . Quando escolher: STRING : Vari\u00e1veis de texto Chaves de vari\u00e1veis categ\u00f3ricas com dicion\u00e1rio ou diret\u00f3rio INT64 : Vari\u00e1veis de n\u00fameros inteiros com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o) FLOAT64 : Vari\u00e1veis de n\u00fameros com casas decimais com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o) DATE : Vari\u00e1veis de data no formato YYYY-MM-DD TIME : Vari\u00e1veis de tempo no formato HH:MM:SS GEOGRAPHY : Vari\u00e1veis de geografia","title":"Tipos de vari\u00e1veis"},{"location":"style_data/#unidades-de-medida","text":"A regra \u00e9 manter vari\u00e1veis com suas unidades de medida originais listadas nesse c\u00f3digo , com a exce\u00e7\u00e3o de vari\u00e1veis financeiras onde convertermos moedas antigas para as atuais (e.g. Cruzeiro para Real). Catalogamos unidades de medida em formato padr\u00e3o na tabela de arquitetura. Lista completa aqui Exemplos: m , km/h , BRL . Para colunas financeiras deflacionadas, listamos a moeda com o ano base. Exemplo: uma coluna medida em reais de 2010 tem unidade BRL_2010 . Vari\u00e1veis devem ter sempre unidades de medida com base 1. Ou seja, ter BRL ao inv\u00e9s de 1000 BRL , ou pessoa ao inv\u00e9s de 1000 pessoas . Essa informa\u00e7\u00e3o, como outros metadados de colunas, s\u00e3o registradas na tabela de arquitetura da tabela.","title":"Unidades de medida"},{"location":"style_data/#quais-variaveis-manter-quais-adicionar-e-quais-remover","text":"Mantemos nossas tabelas parcialmente normalizadas , e temos regras para quais vari\u00e1veis incluirmos em produ\u00e7\u00e3o. Elas s\u00e3o: Remover vari\u00e1veis de nomes de entidades que j\u00e1 est\u00e3o em diret\u00f3rios. Exemplo: retirar municipio da tabela que j\u00e1 inclui id_municipio . Remover vari\u00e1veis servindo de parti\u00e7\u00e3o. Exemplo: remover ano e sigla_uf se a tabela \u00e9 particionada nessas duas dimens\u00f5es. Adicionar chaves prim\u00e1rias principais para cada entidade j\u00e1 existente. Exemplo: adicionar id_municipio a tabelas que s\u00f3 incluem id_municipio_tse . Manter todas as chaves prim\u00e1rias que j\u00e1 vem com a tabela, mas (1) adicionar chaves relevantes (e.g. sigla_uf , id_municipio ) e (2) retirar chaves irrelevantes (e.g. regiao ).","title":"Quais vari\u00e1veis manter, quais adicionar e quais remover"},{"location":"style_data/#cobertura-temporal","text":"Preencher a coluna cobertura_temporal nos metadados de tabela, coluna e chave (em dicion\u00e1rios) segue o seguinte padr\u00e3o. Formato geral: data_inicial(unidade_temporal)data_final data_inicial e data_final est\u00e3o na correspondente unidade temporal. Exemplo: tabela com unidade ano tem cobertura 2005(1)2018 . Exemplo: tabela com unidade mes tem cobertura 2005-08(1)2018-12 . Exemplo: tabela com unidade dia tem cobertura 2005-08-01(1)2018-12-31 . Regras para preenchimento Metadados de tabela Preencher no formato geral. Metadados de coluna Preencher no formato geral, exceto quando a data_inicial ou data_final sejam iguais aos da tabela. Nesse caso deixe vazio. Exemplo: suponha que a cobertura da tabela seja 2005(1)2018 . Se uma coluna aparece s\u00f3 em 2012 e existe at\u00e9 2018, preenchemos sua cobertura como 2012(1) . Se uma coluna desaparece em 2013, preenchemos sua cobertura como (1)2013 . Se uma coluna existe na mesma cobertura temporal da tabela, preenchemos sua cobertura como (1) . Metadados de chave Preencher no mesmo padr\u00e3o de colunas, mas a refer\u00eancia sendo a coluna correspondente, e n\u00e3o a tabela.","title":"Cobertura temporal"},{"location":"style_data/#limpando-strings","text":"Vari\u00e1veis categ\u00f3ricas: inicial mai\u00fascula e resto min\u00fasculo, com acentos. STRINGs n\u00e3o-estruturadas: manter igual aos dados originais.","title":"Limpando STRINGs"},{"location":"style_data/#formatos-de-valores","text":"Decimal: formato americano, i.e. sempre . (ponto) ao inv\u00e9s de , (v\u00edrgula). Data: YYYY-MM-DD Hor\u00e1rio (24h): HH:MM:SS Datetime ( ISO-8601 ): YYYY-MM-DDTHH:MM:SS.sssZ Valor nulo: \"\" (csv), NULL (Python), NA (R), . ou \"\" (Stata) Propor\u00e7\u00e3o/porcentagem: entre 0-100","title":"Formatos de valores"},{"location":"style_data/#particionamento-de-tabelas","text":"Uma tabela particionada \u00e9 uma tabela especial dividida em segmentos, chamados de parti\u00e7\u00f5es, que facilitam o gerenciamento e a consulta de seus dados. Ao dividir uma grande tabela em parti\u00e7\u00f5es menores, voc\u00ea pode melhorar o desempenho da consulta e pode controlar os custos reduzindo o n\u00famero de bytes lidos por uma consulta. Por isso, sempre recomendamos que tabelas grandes sejam particionadas. Leia mais a respeito na documenta\u00e7\u00e3o da Google Cloud . Note que ao particionar uma tabela \u00e9 preciso excluir a coluna correspondente. Exemplo: \u00e9 preciso excluir a coluna ano ao particionar por ano . Colunas comuns para usar como parti\u00e7\u00e3o: ano , mes , sigla_uf , id_municipio .","title":"Particionamento de tabelas"},{"location":"style_data/#numero-de-bases-por-pull-request","text":"Pull requests no Github devem incluir no m\u00e1ximo uma base. Ou seja, podem envolver uma ou mais tabela intra-base.","title":"N\u00famero de bases por pull request"},{"location":"style_data/#dicionarios","text":"Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas). Para cada tabela, coluna, e cobertura temporal, cada chave mapeia unicamente um valor. Chaves n\u00e3o podem ter valores nulos. Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais. Chaves s\u00f3 podem possuir zeros \u00e0 esquerda quando o n\u00famero de d\u00edgitos da vari\u00e1vel tiver significado. Quando a vari\u00e1vel for enum padr\u00e3o, n\u00f3s excluimos os zeros \u00e0 esquerda. Exemplo: mantemos o zero \u00e0 esquerda da vari\u00e1vel br_bd_diretorios_brasil.cbo_2002:cbo_2002 , que tem seis d\u00edgitos, pois o primeiro d\u00edgito 0 significa a categoria ser do grande grupo = \"Membros das for\u00e7as armadas, policiais e bombeiros militares\" . Para outros casos, como por exemplo br_inep_censo_escolar.turma:etapa_ensino , n\u00f3s excluimos os zeros \u00e0 esquerda. Ou seja, mudamos 01 para 1 . Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc.","title":"Dicion\u00e1rios"},{"location":"style_data/#diretorios","text":"Diret\u00f3rios s\u00e3o as pedras fundamentais da estrutura do nosso datalake . Nossas regras para gerenciar diret\u00f3rios s\u00e3o: Diret\u00f3rios representam entidades do reposit\u00f3rio que tenham chaves prim\u00e1rias (e.g. uf , munic\u00edpio , escola ) e unidades de data-tempo (e.g. data , tempo , dia , mes , ano ). Cada tabela de diret\u00f3rio tem ao menos uma chave prim\u00e1ria com valores \u00fanicos e sem nulos. Exemplos: municipio:id_municipio , uf:sigla_uf . Nomes de vari\u00e1veis com prefixo id_ s\u00e3o reservadas para chaves prim\u00e1rias de entidades. Veja todas as tabelas j\u00e1 dispon\u00edveis aqui.","title":"Diret\u00f3rios"},{"location":"style_data/#pensou-em-melhorias-para-os-padroes-definidos","text":"Abra um issue no nosso Github ou mande uma mensagem no Discord para conversarmos :)","title":"Pensou em melhorias para os padr\u00f5es definidos?"},{"location":"tutorial_cross_tables/","text":"Como cruzar tabelas no datalake Organizamos os dados de forma que o cruzamento de tabelas de diferentes intitui\u00e7\u00f5es e temas seja t\u00e3o simples quanto qualquer outra consulta. Para isso, definimos uma metodologia padr\u00e3o para tratamento dos dados, nomea\u00e7\u00e3o de colunas, tabelas e conjuntos. Como funciona a metolodiga BD+? Alguma frase sobre .... Para saber mais, leia a documenta\u00e7\u00e3o sobre tratamento e arquitetura de dados. Informa\u00e7\u00f5es de diferentes tabelas podem ser agregadas por meiode chaves identificadora . Uma chave identificadora \u00e9 uma coluna cujo nome \u00e9 \u00fanico em todas as tabelas do datalake e \u00e9 utilizada para identificar uma entidade. Exemplo de chave identificadora A coluna ano tem esse mesmo nome em todas as tabelas do datalake - ela sempre se refere a vari\u00e1vel que tem como valor quaisquer anos do nosso calend\u00e1rio. Quando vamos trabalhar com dados de popula\u00e7\u00e3o do IBGE, a coluna ano , junto com a coluna municipio , identificam unicamente cada linha da tabela: N\u00e3o existe mais de uma linha com o mesmo ano e munic\u00edpio; N\u00e3o existe linha com valor nulo de ano ou municipio na tabela; Teste voc\u00ea mesmo(a): as queries abaixo devem retornar vazio! R library ( \"basedosdados\" ) # Busca alguma linha que possui ano e munic\u00edpio repetido query <- \"SELECT ano, municipio, count(*) as total FROM `basedosdados.br_ibge_populacao.municipios` GROUP BY ano, municipio WHERE total > 1\" read_sql ( query = query ) # Busca linhas com ano ou municipio nulos query <- \"SELECT * FROM `basedosdados.br_ibge_populacao.municipios` WHERE ano IS NULL OR municipio IS NULL\" read_sql ( query = query ) Python import basedadosdados as bd # Busca alguma linha que possui ano e munic\u00edpio repetido query = \"\"\"SELECT ano, municipio, count(*) as total FROM `basedosdados.br_ibge_populacao.municipios` GROUP BY ano, municipio WHERE total > 1\"\"\" bd . read_sql ( query = query ) # Busca linhas com ano ou municipio nulos query = \"\"\"SELECT * FROM `basedosdados.br_ibge_populacao.municipios` WHERE ano IS NULL OR municipio IS NULL\"\"\" bd . read_sql ( query = query ) CLI ... Cruzando tabelas com chaves identificadoras A indica\u00e7\u00e3o de um conjunto de colunas como chave identificadora \u00e9 feita direto nos metadados da tabela. Assim, voc\u00ea pode saber quais tabelas podem ser cruzadas comparando o conjunto de chaves identificadoras de cada uma. Abaixo vamos fazer um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros. Nas tabelas de popula\u00e7\u00e3o e PIB, a coluna ano e municipio s\u00e3o chaves identificadoras. Logo usaremos essas colunas na nossa fun\u00e7\u00e3o JOIN para determinar como cruzar as tabelas. R library ( \"basedosdados\" ) set_billing_id ( \"<YOUR_PROJECT_ID>\" ) query <- \"SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" # Voc\u00ea pode fazer o download no seu computador dir <- tempdir () data <- download ( query , file.path ( dir , \"pib_per_capita.csv\" )) # Ou carregar o resultado da query no seu ambiente de an\u00e1lise data <- read_sql ( query ) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Lista de chaves identificadoras Chaves geogr\u00e1ficas Setor censit\u00e1rio: id_setor_censitario Munic\u00edpio: id_municipio (padr\u00e3o), id_municipio_6 , id_municipio_tse , id_municipio_rf , id_municipio_bcb \u00c1rea M\u00ednima Compar\u00e1vel: id_AMC Regi\u00e3o imediata: id_regiao_imediata Regi\u00e3o intermedi\u00e1ria: id_regiao_intermediaria Microrregi\u00e3o: id_microrregiao Mesorregi\u00e3o: id_mesorregiao Unidade da federa\u00e7\u00e3o (UF): sigla_uf (padr\u00e3o), id_uf , uf Regi\u00e3o: regiao Chaves temporais ano , semestre , mes , semana , dia , hora Chaves de pessoas f\u00edsicas cpf , pis , nis Chaves de pessoas jur\u00eddicas Empresa: cnpj Escola: id_escola Chaves em pol\u00edtica Candidato(a): id_candidato_bd Partido: sigla_partido , partido","title":"Como cruzar tabelas no **datalake**"},{"location":"tutorial_cross_tables/#como-cruzar-tabelas-no-datalake","text":"Organizamos os dados de forma que o cruzamento de tabelas de diferentes intitui\u00e7\u00f5es e temas seja t\u00e3o simples quanto qualquer outra consulta. Para isso, definimos uma metodologia padr\u00e3o para tratamento dos dados, nomea\u00e7\u00e3o de colunas, tabelas e conjuntos. Como funciona a metolodiga BD+? Alguma frase sobre .... Para saber mais, leia a documenta\u00e7\u00e3o sobre tratamento e arquitetura de dados. Informa\u00e7\u00f5es de diferentes tabelas podem ser agregadas por meiode chaves identificadora . Uma chave identificadora \u00e9 uma coluna cujo nome \u00e9 \u00fanico em todas as tabelas do datalake e \u00e9 utilizada para identificar uma entidade.","title":"Como cruzar tabelas no datalake"},{"location":"tutorial_cross_tables/#exemplo-de-chave-identificadora","text":"A coluna ano tem esse mesmo nome em todas as tabelas do datalake - ela sempre se refere a vari\u00e1vel que tem como valor quaisquer anos do nosso calend\u00e1rio. Quando vamos trabalhar com dados de popula\u00e7\u00e3o do IBGE, a coluna ano , junto com a coluna municipio , identificam unicamente cada linha da tabela: N\u00e3o existe mais de uma linha com o mesmo ano e munic\u00edpio; N\u00e3o existe linha com valor nulo de ano ou municipio na tabela; Teste voc\u00ea mesmo(a): as queries abaixo devem retornar vazio! R library ( \"basedosdados\" ) # Busca alguma linha que possui ano e munic\u00edpio repetido query <- \"SELECT ano, municipio, count(*) as total FROM `basedosdados.br_ibge_populacao.municipios` GROUP BY ano, municipio WHERE total > 1\" read_sql ( query = query ) # Busca linhas com ano ou municipio nulos query <- \"SELECT * FROM `basedosdados.br_ibge_populacao.municipios` WHERE ano IS NULL OR municipio IS NULL\" read_sql ( query = query ) Python import basedadosdados as bd # Busca alguma linha que possui ano e munic\u00edpio repetido query = \"\"\"SELECT ano, municipio, count(*) as total FROM `basedosdados.br_ibge_populacao.municipios` GROUP BY ano, municipio WHERE total > 1\"\"\" bd . read_sql ( query = query ) # Busca linhas com ano ou municipio nulos query = \"\"\"SELECT * FROM `basedosdados.br_ibge_populacao.municipios` WHERE ano IS NULL OR municipio IS NULL\"\"\" bd . read_sql ( query = query ) CLI ...","title":"Exemplo de chave identificadora"},{"location":"tutorial_cross_tables/#cruzando-tabelas-com-chaves-identificadoras","text":"A indica\u00e7\u00e3o de um conjunto de colunas como chave identificadora \u00e9 feita direto nos metadados da tabela. Assim, voc\u00ea pode saber quais tabelas podem ser cruzadas comparando o conjunto de chaves identificadoras de cada uma. Abaixo vamos fazer um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros. Nas tabelas de popula\u00e7\u00e3o e PIB, a coluna ano e municipio s\u00e3o chaves identificadoras. Logo usaremos essas colunas na nossa fun\u00e7\u00e3o JOIN para determinar como cruzar as tabelas. R library ( \"basedosdados\" ) set_billing_id ( \"<YOUR_PROJECT_ID>\" ) query <- \"SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" # Voc\u00ea pode fazer o download no seu computador dir <- tempdir () data <- download ( query , file.path ( dir , \"pib_per_capita.csv\" )) # Ou carregar o resultado da query no seu ambiente de an\u00e1lise data <- read_sql ( query ) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;'","title":"Cruzando tabelas com chaves identificadoras"},{"location":"tutorial_cross_tables/#lista-de-chaves-identificadoras","text":"","title":"Lista de chaves identificadoras"},{"location":"tutorial_cross_tables/#chaves-geograficas","text":"Setor censit\u00e1rio: id_setor_censitario Munic\u00edpio: id_municipio (padr\u00e3o), id_municipio_6 , id_municipio_tse , id_municipio_rf , id_municipio_bcb \u00c1rea M\u00ednima Compar\u00e1vel: id_AMC Regi\u00e3o imediata: id_regiao_imediata Regi\u00e3o intermedi\u00e1ria: id_regiao_intermediaria Microrregi\u00e3o: id_microrregiao Mesorregi\u00e3o: id_mesorregiao Unidade da federa\u00e7\u00e3o (UF): sigla_uf (padr\u00e3o), id_uf , uf Regi\u00e3o: regiao","title":"Chaves geogr\u00e1ficas"},{"location":"tutorial_cross_tables/#chaves-temporais","text":"ano , semestre , mes , semana , dia , hora","title":"Chaves temporais"},{"location":"tutorial_cross_tables/#chaves-de-pessoas-fisicas","text":"cpf , pis , nis","title":"Chaves de pessoas f\u00edsicas"},{"location":"tutorial_cross_tables/#chaves-de-pessoas-juridicas","text":"Empresa: cnpj Escola: id_escola","title":"Chaves de pessoas jur\u00eddicas"},{"location":"tutorial_cross_tables/#chaves-em-politica","text":"Candidato(a): id_candidato_bd Partido: sigla_partido , partido","title":"Chaves em pol\u00edtica"},{"location":"archive/access_data_local/","text":"Como acessar os dados localmente $ pip install basedosdados Em apenas 3 passos voc\u00ea consegue obter dados estruturados para baixar e analisar: Instalar a aplica\u00e7\u00e3o Criar um projeto no Google Cloud Realizar sua query para explorar os dados Instalando a aplica\u00e7\u00e3o CLI pip install basedosdados Python pip install basedosdados R install.packages ( \"basedosdados\" ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)! Criando um projeto no Google Cloud Caso j\u00e1 tenha um projeto pr\u00f3prio, v\u00e1 direto para a pr\u00f3xima etapa! Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso reposit\u00f3rio p\u00fablico. Basta seguir o passo-a-passo: Acesse o link: https://console.cloud.google.com/projectselector2/home/dashboard Aceite o Termo de Servi\u00e7os do Google Cloud Clique em Create Project/Criar Projeto Escolha um nome bacana para o seu projeto :) Clique em Create/Criar Veja que seu projeto tem um Nome e um Project ID - este segundo \u00e9 a informa\u00e7\u00e3o que voc\u00ea ir\u00e1 utilizar em <YOUR_PROJECT_ID> para fazer queries no nosso reposit\u00f3rio p\u00fablico. Fazendo queries Utilize todo o poder do BigQuery onde quiser. Para obter, filtrar ou cruzar bases basta escrever a query e carregar em sua linguagem favorita. Abaixo voc\u00ea pode seguir um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros em todos os anos dispon\u00edveis . CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar uma tabela inteira no pandas -- por padr\u00e3o, `query_project_id` # \u00e9 o basedosdados, voc\u00ea pode usar esse par\u00e2metro para escolher outro projeto df = bd . read_table ( dataset_id = 'br_ibge_populacao' , table_id = 'municipio' , billing_project_id =< YOUR_PROJECT_ID > , limit = 100 ) Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) R if ( ! require ( \"basedosdados\" )) install.packages ( \"basedosdados\" ) library ( \"basedosdados\" ) set_billing_id ( \"<YOUR_PROJECT_ID>\" ) query <- \"SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" # Voc\u00ea pode fazer o download no seu computador dir <- tempdir () data <- download ( query , file.path ( dir , \"pib_per_capita.csv\" )) # Ou carregar o resultado da query no seu ambiente de an\u00e1lise data <- read_sql ( query ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Como acessar os dados localmente"},{"location":"archive/access_data_local/#como-acessar-os-dados-localmente","text":"$ pip install basedosdados Em apenas 3 passos voc\u00ea consegue obter dados estruturados para baixar e analisar: Instalar a aplica\u00e7\u00e3o Criar um projeto no Google Cloud Realizar sua query para explorar os dados","title":"Como acessar os dados localmente"},{"location":"archive/access_data_local/#instalando-a-aplicacao","text":"CLI pip install basedosdados Python pip install basedosdados R install.packages ( \"basedosdados\" ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Instalando a aplica\u00e7\u00e3o"},{"location":"archive/access_data_local/#criando-um-projeto-no-google-cloud","text":"Caso j\u00e1 tenha um projeto pr\u00f3prio, v\u00e1 direto para a pr\u00f3xima etapa! Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso reposit\u00f3rio p\u00fablico. Basta seguir o passo-a-passo: Acesse o link: https://console.cloud.google.com/projectselector2/home/dashboard Aceite o Termo de Servi\u00e7os do Google Cloud Clique em Create Project/Criar Projeto Escolha um nome bacana para o seu projeto :) Clique em Create/Criar Veja que seu projeto tem um Nome e um Project ID - este segundo \u00e9 a informa\u00e7\u00e3o que voc\u00ea ir\u00e1 utilizar em <YOUR_PROJECT_ID> para fazer queries no nosso reposit\u00f3rio p\u00fablico.","title":"Criando um projeto no Google Cloud"},{"location":"archive/access_data_local/#fazendo-queries","text":"Utilize todo o poder do BigQuery onde quiser. Para obter, filtrar ou cruzar bases basta escrever a query e carregar em sua linguagem favorita. Abaixo voc\u00ea pode seguir um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros em todos os anos dispon\u00edveis . CLI basedosdados download \"where/to/save/file\" \\ --billing_project_id <YOUR_PROJECT_ID> \\ --query ' SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano LIMIT 100;' Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) Python import basedosdados as bd pib_per_capita = \"\"\"SELECT pib.id_municipio , pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano \"\"\" # Voc\u00ea pode fazer o download no seu computador bd . download ( query = pib_per_capita , savepath = \"where/to/save/file\" , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar o resultado da query no pandas df = bd . read_sql ( pib_per_capita , billing_project_id =< YOUR_PROJECT_ID > ) # Ou carregar uma tabela inteira no pandas -- por padr\u00e3o, `query_project_id` # \u00e9 o basedosdados, voc\u00ea pode usar esse par\u00e2metro para escolher outro projeto df = bd . read_table ( dataset_id = 'br_ibge_populacao' , table_id = 'municipio' , billing_project_id =< YOUR_PROJECT_ID > , limit = 100 ) Caso esteja rodando a query pela 1\u00aa vez ser\u00e1 feita somente a configura\u00e7\u00e3o do seu ambiente Siga as instru\u00e7\u00f5es que ir\u00e3o aparecer at\u00e9 o final e rode a query novamente para puxar os dados :) R if ( ! require ( \"basedosdados\" )) install.packages ( \"basedosdados\" ) library ( \"basedosdados\" ) set_billing_id ( \"<YOUR_PROJECT_ID>\" ) query <- \"SELECT pib.id_municipio, pop.ano, pib.PIB / pop.populacao as pib_per_capita FROM `basedosdados.br_ibge_pib.municipio` as pib JOIN `basedosdados.br_ibge_populacao.municipio` as pop ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\" # Voc\u00ea pode fazer o download no seu computador dir <- tempdir () data <- download ( query , file.path ( dir , \"pib_per_capita.csv\" )) # Ou carregar o resultado da query no seu ambiente de an\u00e1lise data <- read_sql ( query ) Stata # Ainda n\u00e3o temos suporte :( # Seja a primeira pessoa a contribuir (veja Issue #83 no GitHub)!","title":"Fazendo queries"},{"location":"archive/colab/","text":"Como colaborar com o projeto Boa parte do projeto at\u00e9 hoje \u00e9 desenvolvido por pessoas volunt\u00e1rias, como voc\u00ea. Ajude subindo dados Voc\u00ea e sua organiza\u00e7\u00e3o podem ajudar a subir dados para o projeto! Nossa metodologia de tratamento e padroniza\u00e7\u00e3o \u00e9 aberta e constru\u00edmos um manual completo para subida de dados no datalake . Por que minha organiza\u00e7\u00e3o deve subir dados na BD+? Incentivamos fortemente a participa\u00e7\u00e3o institucional na subida de dados para garantir a manuten\u00e7\u00e3o das bases. Sua organiza\u00e7\u00e3o tem diversas vantagens com isso, incluindo: Capacidade de cruzar suas bases com dados de diferentes organiza\u00e7\u00f5es de forma simples e f\u00e1cil Compromisso com a transpar\u00eancia, qualidade dos dados e desenvolvimento de melhores pesquisas, an\u00e1lises e solu\u00e7\u00f5es para a sociedade Participa\u00e7\u00e3o numa comunidade que cresce cada vez mais : milhares de jornalistas, pesquisadores(as), desenvolvedores(as), j\u00e1 utilizam a Base dos Dados! Saiba mais Ajude na infraestrutura Nosso time de infraestrutura cuida para que todos os pacotes e pipelines estejam funcionando da melhor forma para o p\u00fablico. Utilizamos o Github para gerir todo o c\u00f3digo e mant\u00ea-lo organizado, onde voc\u00ea pode encontrar issues de novas features, bugs e melhorias que estamos trabalhando. Saiba mais","title":"Como colaborar com o projeto"},{"location":"archive/colab/#como-colaborar-com-o-projeto","text":"Boa parte do projeto at\u00e9 hoje \u00e9 desenvolvido por pessoas volunt\u00e1rias, como voc\u00ea.","title":"Como colaborar com o projeto"},{"location":"archive/colab/#ajude-subindo-dados","text":"Voc\u00ea e sua organiza\u00e7\u00e3o podem ajudar a subir dados para o projeto! Nossa metodologia de tratamento e padroniza\u00e7\u00e3o \u00e9 aberta e constru\u00edmos um manual completo para subida de dados no datalake . Por que minha organiza\u00e7\u00e3o deve subir dados na BD+? Incentivamos fortemente a participa\u00e7\u00e3o institucional na subida de dados para garantir a manuten\u00e7\u00e3o das bases. Sua organiza\u00e7\u00e3o tem diversas vantagens com isso, incluindo: Capacidade de cruzar suas bases com dados de diferentes organiza\u00e7\u00f5es de forma simples e f\u00e1cil Compromisso com a transpar\u00eancia, qualidade dos dados e desenvolvimento de melhores pesquisas, an\u00e1lises e solu\u00e7\u00f5es para a sociedade Participa\u00e7\u00e3o numa comunidade que cresce cada vez mais : milhares de jornalistas, pesquisadores(as), desenvolvedores(as), j\u00e1 utilizam a Base dos Dados! Saiba mais","title":"Ajude subindo dados"},{"location":"archive/colab/#ajude-na-infraestrutura","text":"Nosso time de infraestrutura cuida para que todos os pacotes e pipelines estejam funcionando da melhor forma para o p\u00fablico. Utilizamos o Github para gerir todo o c\u00f3digo e mant\u00ea-lo organizado, onde voc\u00ea pode encontrar issues de novas features, bugs e melhorias que estamos trabalhando. Saiba mais","title":"Ajude na infraestrutura"},{"location":"archive/datathon_2021/","text":"Datathon BD 2021 \ud83c\udfb2 Tema: Como dados abertos podem contribuir para o desenvolvimento igualit\u00e1rio no Brasil? A divulga\u00e7\u00e3o de dados p\u00fablicos mostra em n\u00fameros a desigualdade em diferentes aspectos da sociedade e \u00e9 a partir disso que podemos come\u00e7ar a elaborar solu\u00e7\u00f5es e iniciativas para um desenvolvimento mais democr\u00e1tico. Por isso, inspirados(as) no tema do Open Data Day 2021, resolvemos abrir espa\u00e7o para programadores, jornalistas, pesquisadores e entusiastas de dados pensarem conosco como podemos identificar ou combater desigualdades no Brasil a partir de dados p\u00fablicos . E como come\u00e7ar? N\u00f3s demos o ponto de partida: a partir das mais de 30 bases p\u00fablicas que disponibilizamos tratadas e integradas para uso da sociedade no nosso datalake p\u00fablico (BD+) . Recebemos ao todo mais de 30 inscri\u00e7\u00f5es de diferentes p\u00fablicos, agradecemos a participa\u00e7\u00e3o de todas e todos! \u26a0\ufe0f Nenhuma das an\u00e1lises tem a inten\u00e7\u00e3o de trazer evid\u00eancias rigorosamente testadas sobre os temas abordados, mas sim explorar e abrir poss\u00edveis caminhos para pensarmos os mesmos. E os(as) vencedores(as) s\u00e3o... UFRJ Analytica Equipe: Erica Ferreira, Pedro Boechat, Pedro Borges e Rafael Ribeiro (graduandos e analistas/cientistas de dados) An\u00e1lise: De que forma diferen\u00e7as no acesso a uma educa\u00e7\u00e3o de qualidade se manifestam em diferentes regi\u00f5es do pa\u00eds? Para entender melhor sobre essa e outras perguntas levantadas quanto \u00e0 qualidade de ensino e investimento em educa\u00e7\u00e3o, eles utilizaram ao todo 4 bases disponibilizadas na BD+: Atlas do Desenvolvimento Humano (ADH) , \u00cdndice de Desenvolvimento da Educa\u00e7\u00e3o B\u00e1sica (Ideb) , Finan\u00e7as do Brasil (Finbra) e nossa base de diret\u00f3rios brasileiros , que liga diferentes identifica\u00e7\u00f5es para munic\u00edpios, estados e regi\u00f5es do pa\u00eds. \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado Felipe Macedo Dias (graduando em Economia na USP) An\u00e1lise: A infla\u00e7\u00e3o foi maior em munic\u00edpios mais beneficiados pelo Aux\u00edlio Emergencial? O aux\u00edlio emergencial foi um grande tema socioecon\u00f4mico no contexto de pandemia no pa\u00eds, trazendo o debate tamb\u00e9m para sua perman\u00eancia em momentos posteriores. Felipe buscou uma explorar essa quest\u00e3o incialmente observando as capitais: existe alguma correla\u00e7\u00e3o entre o aumento dos pre\u00e7os, com um maior n\u00edvel de consumo, e a incid\u00eancia do aux\u00edlio nas capitais? Para isso, se utilizou das bases de popula\u00e7\u00e3o estimada do IBGE , Produto Interno Bruto (PIB) e dados do aux\u00edlio emergencial por munic\u00edpio . \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado","title":"Datathon 2021"},{"location":"archive/datathon_2021/#datathon-bd-2021","text":"","title":"Datathon BD 2021 \ud83c\udfb2"},{"location":"archive/datathon_2021/#tema-como-dados-abertos-podem-contribuir-para-o-desenvolvimento-igualitario-no-brasil","text":"A divulga\u00e7\u00e3o de dados p\u00fablicos mostra em n\u00fameros a desigualdade em diferentes aspectos da sociedade e \u00e9 a partir disso que podemos come\u00e7ar a elaborar solu\u00e7\u00f5es e iniciativas para um desenvolvimento mais democr\u00e1tico. Por isso, inspirados(as) no tema do Open Data Day 2021, resolvemos abrir espa\u00e7o para programadores, jornalistas, pesquisadores e entusiastas de dados pensarem conosco como podemos identificar ou combater desigualdades no Brasil a partir de dados p\u00fablicos . E como come\u00e7ar? N\u00f3s demos o ponto de partida: a partir das mais de 30 bases p\u00fablicas que disponibilizamos tratadas e integradas para uso da sociedade no nosso datalake p\u00fablico (BD+) . Recebemos ao todo mais de 30 inscri\u00e7\u00f5es de diferentes p\u00fablicos, agradecemos a participa\u00e7\u00e3o de todas e todos! \u26a0\ufe0f Nenhuma das an\u00e1lises tem a inten\u00e7\u00e3o de trazer evid\u00eancias rigorosamente testadas sobre os temas abordados, mas sim explorar e abrir poss\u00edveis caminhos para pensarmos os mesmos.","title":"Tema: Como dados abertos podem contribuir para o desenvolvimento igualit\u00e1rio no Brasil?"},{"location":"archive/datathon_2021/#e-osas-vencedoresas-sao","text":"UFRJ Analytica Equipe: Erica Ferreira, Pedro Boechat, Pedro Borges e Rafael Ribeiro (graduandos e analistas/cientistas de dados) An\u00e1lise: De que forma diferen\u00e7as no acesso a uma educa\u00e7\u00e3o de qualidade se manifestam em diferentes regi\u00f5es do pa\u00eds? Para entender melhor sobre essa e outras perguntas levantadas quanto \u00e0 qualidade de ensino e investimento em educa\u00e7\u00e3o, eles utilizaram ao todo 4 bases disponibilizadas na BD+: Atlas do Desenvolvimento Humano (ADH) , \u00cdndice de Desenvolvimento da Educa\u00e7\u00e3o B\u00e1sica (Ideb) , Finan\u00e7as do Brasil (Finbra) e nossa base de diret\u00f3rios brasileiros , que liga diferentes identifica\u00e7\u00f5es para munic\u00edpios, estados e regi\u00f5es do pa\u00eds. \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado Felipe Macedo Dias (graduando em Economia na USP) An\u00e1lise: A infla\u00e7\u00e3o foi maior em munic\u00edpios mais beneficiados pelo Aux\u00edlio Emergencial? O aux\u00edlio emergencial foi um grande tema socioecon\u00f4mico no contexto de pandemia no pa\u00eds, trazendo o debate tamb\u00e9m para sua perman\u00eancia em momentos posteriores. Felipe buscou uma explorar essa quest\u00e3o incialmente observando as capitais: existe alguma correla\u00e7\u00e3o entre o aumento dos pre\u00e7os, com um maior n\u00edvel de consumo, e a incid\u00eancia do aux\u00edlio nas capitais? Para isso, se utilizou das bases de popula\u00e7\u00e3o estimada do IBGE , Produto Interno Bruto (PIB) e dados do aux\u00edlio emergencial por munic\u00edpio . \u27a1\ufe0f Confira a an\u00e1lise completa aqui \ud83d\udcbb Confira o c\u00f3digo utilizado","title":"E os(as) vencedores(as) s\u00e3o..."},{"location":"archive/odd_2021/","text":"Open Data Day: Datathon BD Que tal explorar dados p\u00fablicos com a gente neste Dia dos Dados Abertos? Em parceria \u00e0 Escola de Matem\u00e1tica Aplicada da FGV , a Base dos Dados trar\u00e1 um evento apresentando sobre a nossa iniciativa, o novo curso de forma\u00e7\u00e3o em Ci\u00eancia de Dados da FGV EMAp e exemplos de aplica\u00e7\u00f5es com nosso data lake p\u00fablico. Para fechar o evento com chave de ouro, lan\u00e7aremos nosso primeiro de muitos Datathons! Por que participar? Essa \u00e9 uma \u00f3tima oportunidade de aprender mais sobre dados p\u00fablicos e conhecer pessoas interessantes! As melhores an\u00e1lises ser\u00e3o publicadas no nosso site e nas redes. A Base dos Dados tem a miss\u00e3o de universalizar o acesso a dados no Brasil. Coletamos bases que s\u00e3o p\u00fablicas mas s\u00e3o dif\u00edceis de manusear e organizar, e criamos uma forma super f\u00e1cil de us\u00e1-las. Acreditamos que a dist\u00e2ncia entre voc\u00ea e uma an\u00e1lise de dados deveria ser apenas uma boa pergunta. Fazemos tudo isso por meio de um mecanismo de busca, com 900+ bases catalogadas; e nosso data lake p\u00fablico que j\u00e1 tem mais de 30 bases p\u00fablicas tratadas, integradas e prontas para uso. O trabalho \u00e9 volunt\u00e1rio e mantido por nossa comunidade, formada por programadores, pesquisadores e entusiastas de dados. A FGV EMAp atua desde 2011 com os cursos de forma\u00e7\u00e3o para o desenvolvimento de uma matem\u00e1tica contempor\u00e2nea, adaptada aos desafios da era da informa\u00e7\u00e3o e do conhecimento. Em 2020 lan\u00e7aram o mais competo curso de Ci\u00eancia de Dados do pa\u00eds , um curso criado para transformar o horizonte profissional dos estudantes brasileiros interessados em seguir uma carreira inovadora. Como participar? O evento \u00e9 aberto a todos(as) e n\u00e3o \u00e9 necess\u00e1ria inscri\u00e7\u00e3o, basta entrar no nosso canal do Discord . Para participar do Datathon que ser\u00e1 lan\u00e7ado no dia do evento, inscreva-se em: https://forms.gle/k1ux95S5zR6oJ5RL8 Cronograma 14H Abertura: Base dos Dados e FGV EMAp 14:40 Workshop: Analisando dados p\u00fablicos na BD+ 15:40 Lan\u00e7amento do Datathon BD+ Todo o evento ir\u00e1 acontecer no nosso canal do Discord . O que \u00e9 o Dia dos Dados Abertos? O Dia dos Dados Abertos \u00e9 uma celebra\u00e7\u00e3o anual dos dados abertos em todo o mundo. Grupos de todas as partes do mundo criaram eventos locais no dia em que usar\u00e3o dados abertos em suas comunidades. \u00c9 uma oportunidade para mostrar os benef\u00edcios dos dados abertos e encorajar a ado\u00e7\u00e3o de pol\u00edticas de dados abertos no governo, empresas e na sociedade civil. Todos os resultados s\u00e3o abertos para que todos usem e reusem. Veja mais sobre o Dia de Dados Abertos.","title":"Odd 2021"},{"location":"archive/odd_2021/#open-data-day-datathon-bd","text":"Que tal explorar dados p\u00fablicos com a gente neste Dia dos Dados Abertos? Em parceria \u00e0 Escola de Matem\u00e1tica Aplicada da FGV , a Base dos Dados trar\u00e1 um evento apresentando sobre a nossa iniciativa, o novo curso de forma\u00e7\u00e3o em Ci\u00eancia de Dados da FGV EMAp e exemplos de aplica\u00e7\u00f5es com nosso data lake p\u00fablico. Para fechar o evento com chave de ouro, lan\u00e7aremos nosso primeiro de muitos Datathons!","title":"Open Data Day: Datathon BD"},{"location":"archive/odd_2021/#por-que-participar","text":"Essa \u00e9 uma \u00f3tima oportunidade de aprender mais sobre dados p\u00fablicos e conhecer pessoas interessantes! As melhores an\u00e1lises ser\u00e3o publicadas no nosso site e nas redes. A Base dos Dados tem a miss\u00e3o de universalizar o acesso a dados no Brasil. Coletamos bases que s\u00e3o p\u00fablicas mas s\u00e3o dif\u00edceis de manusear e organizar, e criamos uma forma super f\u00e1cil de us\u00e1-las. Acreditamos que a dist\u00e2ncia entre voc\u00ea e uma an\u00e1lise de dados deveria ser apenas uma boa pergunta. Fazemos tudo isso por meio de um mecanismo de busca, com 900+ bases catalogadas; e nosso data lake p\u00fablico que j\u00e1 tem mais de 30 bases p\u00fablicas tratadas, integradas e prontas para uso. O trabalho \u00e9 volunt\u00e1rio e mantido por nossa comunidade, formada por programadores, pesquisadores e entusiastas de dados. A FGV EMAp atua desde 2011 com os cursos de forma\u00e7\u00e3o para o desenvolvimento de uma matem\u00e1tica contempor\u00e2nea, adaptada aos desafios da era da informa\u00e7\u00e3o e do conhecimento. Em 2020 lan\u00e7aram o mais competo curso de Ci\u00eancia de Dados do pa\u00eds , um curso criado para transformar o horizonte profissional dos estudantes brasileiros interessados em seguir uma carreira inovadora.","title":"Por que participar?"},{"location":"archive/odd_2021/#como-participar","text":"O evento \u00e9 aberto a todos(as) e n\u00e3o \u00e9 necess\u00e1ria inscri\u00e7\u00e3o, basta entrar no nosso canal do Discord . Para participar do Datathon que ser\u00e1 lan\u00e7ado no dia do evento, inscreva-se em: https://forms.gle/k1ux95S5zR6oJ5RL8","title":"Como participar?"},{"location":"archive/odd_2021/#cronograma","text":"14H Abertura: Base dos Dados e FGV EMAp 14:40 Workshop: Analisando dados p\u00fablicos na BD+ 15:40 Lan\u00e7amento do Datathon BD+","title":"Cronograma"},{"location":"archive/odd_2021/#todo-o-evento-ira-acontecer-no-nosso-canal-do-discord","text":"","title":"Todo o evento ir\u00e1 acontecer no nosso canal do Discord."},{"location":"archive/odd_2021/#o-que-e-o-dia-dos-dados-abertos","text":"O Dia dos Dados Abertos \u00e9 uma celebra\u00e7\u00e3o anual dos dados abertos em todo o mundo. Grupos de todas as partes do mundo criaram eventos locais no dia em que usar\u00e3o dados abertos em suas comunidades. \u00c9 uma oportunidade para mostrar os benef\u00edcios dos dados abertos e encorajar a ado\u00e7\u00e3o de pol\u00edticas de dados abertos no governo, empresas e na sociedade civil. Todos os resultados s\u00e3o abertos para que todos usem e reusem. Veja mais sobre o Dia de Dados Abertos.","title":"O que \u00e9 o Dia dos Dados Abertos?"},{"location":"archive/support/","text":"Nos apoie Financiamento coletivo \ud83d\udcb8 A Base dos Dados j\u00e1 poupou horas da sua vida? Ou permitiu coisas antes imposs\u00edveis? Nosso trabalho \u00e9 quase todo volunt\u00e1rio, mas temos v\u00e1rios custos de infraestrutura, equipe, e outros. Nos ajude a fazer esse projeto se manter e crescer! Fa\u00e7a desenvolvedores felizes Ajude a manter nosso c\u00f3digo \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Precisamos de ajuda para manter e melhorar nossos clientes Python, R, entre outros. Acesse nossos issues ou abra um novo para come\u00e7ar a desenvolver :)","title":"Nos apoie"},{"location":"archive/support/#nos-apoie","text":"","title":"Nos apoie"},{"location":"archive/support/#financiamento-coletivo","text":"A Base dos Dados j\u00e1 poupou horas da sua vida? Ou permitiu coisas antes imposs\u00edveis? Nosso trabalho \u00e9 quase todo volunt\u00e1rio, mas temos v\u00e1rios custos de infraestrutura, equipe, e outros. Nos ajude a fazer esse projeto se manter e crescer! Fa\u00e7a desenvolvedores felizes","title":"Financiamento coletivo \ud83d\udcb8"},{"location":"archive/support/#ajude-a-manter-nosso-codigo","text":"Precisamos de ajuda para manter e melhorar nossos clientes Python, R, entre outros. Acesse nossos issues ou abra um novo para come\u00e7ar a desenvolver :)","title":"Ajude a manter nosso c\u00f3digo \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb"}]}